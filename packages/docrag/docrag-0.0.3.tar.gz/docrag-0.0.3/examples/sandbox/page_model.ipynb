{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lllang\\miniconda3\\envs\\docsearch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docsearch.figure_extraction import DocumentPageAnalyzer\n",
    "from IPython.display import display\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\n",
      "DATA_DIR: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\n",
      "MODEL_WEIGHTS: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\data\\doclayout_yolo_docstructbench_imgsz1024.pt\n",
      "SAMPLES_DIR: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\n",
      "sample_filepath: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\\sample_1.png\n",
      "sample_filepath: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\\sample_2.png\n",
      "sample_filepath: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\\sample_3.png\n",
      "sample_filepath: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\\sample_4.png\n",
      "sample_filepath: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\\sample_5.png\n",
      "sample_filepath: c:\\Users\\lllang\\Desktop\\Current_Projects\\DocSearch\\examples\\sandbox\\data\\samples\\sample_6.png\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = Path(os.path.abspath(\".\"))\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "MODEL_WEIGHTS = ROOT_DIR.parent.parent / \"data\" / \"doclayout_yolo_docstructbench_imgsz1024.pt\"\n",
    "SAMPLES_DIR = DATA_DIR / \"samples\"\n",
    "sample_filepaths = list(SAMPLES_DIR.glob(\"*.png\"))\n",
    "\n",
    "print(f\"ROOT_DIR: {ROOT_DIR}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"MODEL_WEIGHTS: {MODEL_WEIGHTS}\")\n",
    "print(f\"SAMPLES_DIR: {SAMPLES_DIR}\")\n",
    "for filepath in sample_filepaths:\n",
    "    print(f\"sample_filepath: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x800 3 titles, 21 plain texts, 2 isolate_formulas, 3 formula_captions, 1108.2ms\n",
      "Speed: 7.0ms preprocess, 1108.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 800)\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=827x50 at 0x210C9498A60>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=495x48 at 0x210C9498AC0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=353x53 at 0x210C9498B20>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1000x416 at 0x210C94C8400>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=999x412 at 0x210C94C80A0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1002x508 at 0x210C94CA6E0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1004x414 at 0x210C9499660>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1003x276 at 0x210C9498BE0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1001x161 at 0x210C9498B80>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1002x132 at 0x210C949BA00>\n",
      "Making API call (Model: gemini-2.0-flash)...Processing async: <PIL.Image.Image image mode=RGB size=998x132 at 0x210C949A6E0>\n",
      "\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1002x140 at 0x210C949ABC0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1000x133 at 0x210C949A620>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=1000x137 at 0x210C949A680>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=998x89 at 0x210C9498DC0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=997x90 at 0x210C9498CA0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=999x89 at 0x210C9498A00>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=998x91 at 0x210C94989A0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=989x381 at 0x210C94988E0>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=791x47 at 0x210C9498700>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=991x1039 at 0x210C94986A0>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=985x412 at 0x210C9498670>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=988x370 at 0x210C9498640>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=983x232 at 0x210C94985E0>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=479x93 at 0x210C949A800>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=46x41 at 0x210C9498E80>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=850x156 at 0x210C9498C40>\n",
      "Processing async: <PIL.Image.Image image mode=RGB size=46x41 at 0x210C9498940>\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "Making API call (Model: gemini-2.0-flash)...\n",
      "# Page\n",
      "\n",
      "## Text\n",
      "\n",
      "Fine-tuning Tasks and Datasets. We use 8 benchmark\n",
      "datasets from the MoleculeNet (Wu et al. 2018a) to per-\n",
      "form the experiments, which cover a wide range of molec-\n",
      "ular tasks such as quantum mechanics, physical chemistry,\n",
      "biophysics, and physiology. For each dataset, as suggested\n",
      "by (Wu et al. 2018a), we apply three independent runs\n",
      "on three random-seeded random splitting or scaffold split-\n",
      "ting with a ratio for train/validation/test as 8:1:1. Details of\n",
      "datasets and dataset splitting are deferred to Appendix B.1\n",
      "\n",
      "Performance under Fine-tune Protocol. We first exam-\n",
      "ine whether the proposed KCL performs better than SOTA\n",
      "methods. Table 2 displays the complete results of supervised\n",
      "learning baselines and pre-trained methods, where the un-\n",
      "derlined cells indicate the previous SOTAs, and the cells\n",
      "with bold show the best result achieved by KCL. The Tox21,\n",
      "SIDER, and ClinTox are all multiple-task learning tasks,\n",
      "including totally 42 classification tasks. We also imple-\n",
      "mented two versions of our KCL model, the original molec-\n",
      "\n",
      "Implementation details. We use the Adam optimizer with\n",
      "an initial learning rate of 0.0001 and batch size of 256. For\n",
      "pre-training models, the running epoch is fixed to 20. The\n",
      "temperature $\\tau$ is set as 0.1. For downstream tasks, we use\n",
      "early stopping on the validation set. We apply the random\n",
      "search to obtain the best hyper-parameters based on the vali-\n",
      "dation set. Our model is implemented with PyTorch (Paszke\n",
      "et al. 2019) and Deep Graph Library (Wang et al. 2019). We\n",
      "develop all codes on a Ubuntu Server with 4 GPUs (NVIDIA\n",
      "GeForce 1080Ti). More experimental details are available in\n",
      "Appendix C and D.\n",
      "\n",
      "Contrastive Loss. We augmented a minibatch of $N$ simi-\n",
      "lar molecular graphs with knowledge-guided graph augmen-\n",
      "tation, resulting in a total of $2N$ graphs. Following (You\n",
      "et al. 2020; Chen et al. 2020), given a positive pair, we treat\n",
      "the other $2(N-1)$ graphs within the same minibatch as hard\n",
      "negative samples. We utilize NT-Xent as our objective func-\n",
      "tion like in (Hjelm et al. 2019; Chen et al. 2020; You et al.\n",
      "2020; Carse, Carey, and McKenna 2021). The training ob-\n",
      "jective for ($G_i$, $G'_i$) is defined as\n",
      "\n",
      "where $e_1$, $e_2$ denotes the fingerprints, $N_1$, $N_2$ denotes the\n",
      "number of 1s in $e_1$, $e_2$ respectively, and $N_{12}$ denotes the\n",
      "number of 1s in the intersection of $e_1$, $e_2$. In order to en-\n",
      "sure all molecules have negative samples, instead of setting a\n",
      "fixed threshold, we sorted samples by similarity and selected\n",
      "a batch of most similar molecules as the negative samples.\n",
      "\n",
      "where 7 denotes the temperature parameter and sim(z1, z2)\n",
      "\n",
      "$\\frac{z_1^T z_2}{||z_1|| \\cdot ||z_2||}$. The final loss is computed\n",
      "across all positive pairs in the minibatch.\n",
      "\n",
      "fixed length binary vector. Then we calculate the molecular\n",
      "similarity through their Tanimoto coefficient (Bajusz, Rácz,\n",
      "and Héberger 2015):\n",
      "\n",
      "In this section, we conduct extensive experiments to exam-\n",
      "ine the proposed method by answering the following ques-\n",
      "tions:\n",
      "\n",
      "Pre-training Data Collection. We collect 250K unlabeled\n",
      "molecules sampled from the ZINC15 datasets (Sterling and\n",
      "Irwin 2015) to pre-train KCL.\n",
      "\n",
      "Q2: Does the knowledge-guided graph augmentation in\n",
      "Module 1 learns better representations than general augmentations?\n",
      "\n",
      "**Evaluation Protocol.** The evaluation process follows two steps. We first pre-train the model and then evaluate the learned model on downstream tasks under two protocols.\n",
      "\n",
      "Q1: How does KCL perform compared with state-of-the-\n",
      "art methods for molecular property prediction?\n",
      "\n",
      "Q5: How can we interpret KCL(KMPNN) from a domain-\n",
      "specific perspective?\n",
      "\n",
      "Q3: How do knowledge feature initialization and graph\n",
      "encoders in Module 2 affect KCL?\n",
      "\n",
      "Q4: How useful are the self-supervised contrastive learn-\n",
      "ing and hard negative strategy in Module 3?\n",
      "\n",
      "-   *Fine-tune protocol:* To achieve the full potential of our model, given graph embeddings output by the KCL encoder, we use an additional MLP to predict the property of the molecule. Fine-tune parameters in the encoders and the MLP.\n",
      "-   *Linear Protocol:* For comparison of our model and contrastive learning baselines, we fix the graph embeddings from the pre-trained model, and train a linear classifier.\n",
      "\n",
      "**Baselines.** We adopt three types of baselines:\n",
      "\n",
      "- Supervised learning methods: GCN (Kipf and Welling\n",
      "[2017]) and Weave (Kearnes et al. [2016]) are two types\n",
      "of graph convolutional methods. MPNN (Gilmer et al.\n",
      "[2017]) and its variants DMPNN (Yang et al. [2019]),\n",
      "CMPNN (Song et al. [2020]), CoMPT (Chen et al. [2021])\n",
      "consider the edge features and strengthen the message\n",
      "interactions between bonds and atoms during message\n",
      "passing.\n",
      "- Pre-trained methods: N-GRAM (Liu et al. [2019]) con-\n",
      "ducts node embeddings by predicting the node attributes.\n",
      "Hu et al. (Hu et al. [2020]) and GROVER (Rong et al.\n",
      "[2020]) are pre-trained models incorporating both node-\n",
      "level and graph-level pretext tasks.\n",
      "- Graph contrastive learning baselines: InfoGraph (Sun\n",
      "et al. [2020]) maximizes the mutual information between\n",
      "nodes and graphs. MICRO-Graph (Subramonian [2021]) is\n",
      "a motif-based contrastive method. GraphCL (You et al.\n",
      "[2020]) constructs contrastive views of graph data via\n",
      "hand-picking ad-hoc augmentations. JOAO (You et al.\n",
      "[2021]) automates the augmentation selection. MoCL (Sun\n",
      "et al. [2021]) utilizes domain knowledge at two levels to\n",
      "assist representation learning.\n",
      "\n",
      "- Graph contrastive learning baselines: InfoGraph (Sun\n",
      "et al. 2020) maximizes the mutual information between\n",
      "nodes and graphs. MICRO-Graph (Subramonian 2021) is\n",
      "a motif-based contrastive method. GraphCL (You et al.\n",
      "2020) constructs contrastive views of graph data via\n",
      "hand-picking ad-hoc augmentations. JOAO (You et al.\n",
      "2021) automates the augmentation selection. MoCL (Sun\n",
      "et al. 2021) utilizes domain knowledge at two levels to\n",
      "assist representation learning.\n",
      "\n",
      "- Supervised learning methods: GCN (Kipf and Welling\n",
      "2017) and Weave (Kearnes et al. 2016) are two types\n",
      "of graph convolutional methods. MPNN (Gilmer et al.\n",
      "2017) and its variants DMPNN (Yang et al. 2019),\n",
      "CMPNN (Song et al. 2020), CoMPT (Chen et al. 2021)\n",
      "consider the edge features and strengthen the message\n",
      "interactions between bonds and atoms during message\n",
      "passing.\n",
      "\n",
      "- Pre-trained methods: N-GRAM (Liu et al. 2019) con-\n",
      "ducts node embeddings by predicting the node attributes.\n",
      "Hu et al. (Hu et al. 2020) and GROVER (Rong et al.\n",
      "2020) are pre-trained models incorporating both node-\n",
      "level and graph-level pretext tasks.\n",
      "\n",
      "## Title\n",
      "\n",
      "# 4.2 Performance Comparison (Q1 & Q2)\n",
      "\n",
      "# 4.1 Experimental Setup\n",
      "\n",
      "# 4 Experiments\n",
      "\n",
      "## Formulas\n",
      "\n",
      "s(\\boldsymbol{e}_1, \\boldsymbol{e}_2) = \\frac{N_{12}}{N_1 + N_2 - N_{12}},\n",
      "\n",
      "$\\ell_i = -\\log \\frac{e^{\\text{sim}(z_i, z'_i) / \\tau}}{\\sum_{j=1}^N \\left( e^{\\text{sim}(z_i, z'_j) / \\tau} + e^{\\text{sim}(z'_i, z_j) / \\tau} \\right)},$\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                      <span style=\"font-weight: bold\">Page</span>                                                       ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\n",
       "                                                       <span style=\"font-weight: bold; text-decoration: underline\">Text</span>                                                        \n",
       "\n",
       "Fine-tuning Tasks and Datasets. We use 8 benchmark datasets from the MoleculeNet (Wu et al. 2018a) to per- form the\n",
       "experiments, which cover a wide range of molec- ular tasks such as quantum mechanics, physical chemistry,          \n",
       "biophysics, and physiology. For each dataset, as suggested by (Wu et al. 2018a), we apply three independent runs on\n",
       "three random-seeded random splitting or scaffold split- ting with a ratio for train/validation/test as 8:1:1.      \n",
       "Details of datasets and dataset splitting are deferred to Appendix B.1                                             \n",
       "\n",
       "Performance under Fine-tune Protocol. We first exam- ine whether the proposed KCL performs better than SOTA        \n",
       "methods. Table 2 displays the complete results of supervised learning baselines and pre-trained methods, where the \n",
       "un- derlined cells indicate the previous SOTAs, and the cells with bold show the best result achieved by KCL. The  \n",
       "Tox21, SIDER, and ClinTox are all multiple-task learning tasks, including totally 42 classification tasks. We also \n",
       "imple- mented two versions of our KCL model, the original molec-                                                   \n",
       "\n",
       "Implementation details. We use the Adam optimizer with an initial learning rate of 0.0001 and batch size of 256.   \n",
       "For pre-training models, the running epoch is fixed to 20. The temperature $\\tau$ is set as 0.1. For downstream    \n",
       "tasks, we use early stopping on the validation set. We apply the random search to obtain the best hyper-parameters \n",
       "based on the vali- dation set. Our model is implemented with PyTorch (Paszke et al. 2019) and Deep Graph Library   \n",
       "(Wang et al. 2019). We develop all codes on a Ubuntu Server with 4 GPUs (NVIDIA GeForce 1080Ti). More experimental \n",
       "details are available in Appendix C and D.                                                                         \n",
       "\n",
       "Contrastive Loss. We augmented a minibatch of $N$ simi- lar molecular graphs with knowledge-guided graph augmen-   \n",
       "tation, resulting in a total of $2N$ graphs. Following (You et al. 2020; Chen et al. 2020), given a positive pair, \n",
       "we treat the other $2(N-1)$ graphs within the same minibatch as hard negative samples. We utilize NT-Xent as our   \n",
       "objective func- tion like in (Hjelm et al. 2019; Chen et al. 2020; You et al. 2020; Carse, Carey, and McKenna      \n",
       "2021). The training ob- jective for ($G_i$, $G'_i$) is defined as                                                  \n",
       "\n",
       "where $e_1$, $e_2$ denotes the fingerprints, $N_1$, $N_2$ denotes the number of 1s in $e_1$, $e_2$ respectively,   \n",
       "and $N_{12}$ denotes the number of 1s in the intersection of $e_1$, $e_2$. In order to en- sure all molecules have \n",
       "negative samples, instead of setting a fixed threshold, we sorted samples by similarity and selected a batch of    \n",
       "most similar molecules as the negative samples.                                                                    \n",
       "\n",
       "where 7 denotes the temperature parameter and sim(z1, z2)                                                          \n",
       "\n",
       "$\\frac{z_1^T z_2}{||z_1|| \\cdot ||z_2||}$. The final loss is computed across all positive pairs in the minibatch.  \n",
       "\n",
       "fixed length binary vector. Then we calculate the molecular similarity through their Tanimoto coefficient (Bajusz, \n",
       "Rácz, and Héberger 2015):                                                                                          \n",
       "\n",
       "In this section, we conduct extensive experiments to exam- ine the proposed method by answering the following ques-\n",
       "tions:                                                                                                             \n",
       "\n",
       "Pre-training Data Collection. We collect 250K unlabeled molecules sampled from the ZINC15 datasets (Sterling and   \n",
       "Irwin 2015) to pre-train KCL.                                                                                      \n",
       "\n",
       "Q2: Does the knowledge-guided graph augmentation in Module 1 learns better representations than general            \n",
       "augmentations?                                                                                                     \n",
       "\n",
       "<span style=\"font-weight: bold\">Evaluation Protocol.</span> The evaluation process follows two steps. We first pre-train the model and then evaluate the  \n",
       "learned model on downstream tasks under two protocols.                                                             \n",
       "\n",
       "Q1: How does KCL perform compared with state-of-the- art methods for molecular property prediction?                \n",
       "\n",
       "Q5: How can we interpret KCL(KMPNN) from a domain- specific perspective?                                           \n",
       "\n",
       "Q3: How do knowledge feature initialization and graph encoders in Module 2 affect KCL?                             \n",
       "\n",
       "Q4: How useful are the self-supervised contrastive learn- ing and hard negative strategy in Module 3?              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-style: italic\">Fine-tune protocol:</span> To achieve the full potential of our model, given graph embeddings output by the KCL        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>encoder, we use an additional MLP to predict the property of the molecule. Fine-tune parameters in the encoders \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>and the MLP.                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-style: italic\">Linear Protocol:</span> For comparison of our model and contrastive learning baselines, we fix the graph embeddings    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>from the pre-trained model, and train a linear classifier.                                                      \n",
       "\n",
       "<span style=\"font-weight: bold\">Baselines.</span> We adopt three types of baselines:                                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Supervised learning methods: GCN (Kipf and Welling [2017]) and Weave (Kearnes et al. [2016]) are two types of   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>graph convolutional methods. MPNN (Gilmer et al. [2017]) and its variants DMPNN (Yang et al. [2019]), CMPNN     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>(Song et al. [2020]), CoMPT (Chen et al. [2021]) consider the edge features and strengthen the message          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>interactions between bonds and atoms during message passing.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Pre-trained methods: N-GRAM (Liu et al. [2019]) con- ducts node embeddings by predicting the node attributes. Hu\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>et al. (Hu et al. [2020]) and GROVER (Rong et al. [2020]) are pre-trained models incorporating both node- level \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>and graph-level pretext tasks.                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Graph contrastive learning baselines: InfoGraph (Sun et al. [2020]) maximizes the mutual information between    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>nodes and graphs. MICRO-Graph (Subramonian [2021]) is a motif-based contrastive method. GraphCL (You et al.     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>[2020]) constructs contrastive views of graph data via hand-picking ad-hoc augmentations. JOAO (You et al.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>[2021]) automates the augmentation selection. MoCL (Sun et al. [2021]) utilizes domain knowledge at two levels  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>to assist representation learning.                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Graph contrastive learning baselines: InfoGraph (Sun et al. 2020) maximizes the mutual information between nodes\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>and graphs. MICRO-Graph (Subramonian 2021) is a motif-based contrastive method. GraphCL (You et al.             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2020 </span>constructs contrastive views of graph data via hand-picking ad-hoc augmentations. JOAO (You et al.           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2021 </span>automates the augmentation selection. MoCL (Sun et al. 2021) utilizes domain knowledge at two levels to      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>assist representation learning.                                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Supervised learning methods: GCN (Kipf and Welling                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2017 </span>and Weave (Kearnes et al. 2016) are two types of graph convolutional methods. MPNN (Gilmer et al.            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2018 </span>and its variants DMPNN (Yang et al. 2019), CMPNN (Song et al. 2020), CoMPT (Chen et al. 2021) consider the   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>edge features and strengthen the message interactions between bonds and atoms during message passing.        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Pre-trained methods: N-GRAM (Liu et al. 2019) con- ducts node embeddings by predicting the node attributes. Hu  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>et al. (Hu et al. 2020) and GROVER (Rong et al.                                                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2020 </span>are pre-trained models incorporating both node- level and graph-level pretext tasks.                         \n",
       "\n",
       "\n",
       "                                                       <span style=\"font-weight: bold; text-decoration: underline\">Title</span>                                                       \n",
       "\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                      <span style=\"font-weight: bold\">4.2 Performance Comparison (Q1 &amp; Q2)</span>                                       ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                             <span style=\"font-weight: bold\">4.1 Experimental Setup</span>                                              ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                  <span style=\"font-weight: bold\">4 Experiments</span>                                                  ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">Formulas</span>                                                      \n",
       "\n",
       "s(\\boldsymbol{e}<span style=\"font-style: italic\">1, \\boldsymbol{e}2) = \\frac{N{12}}{N_1 + N_2 - N</span>{12}},                                             \n",
       "\n",
       "$\\ell_i = -\\log \\frac{e^{\\text{sim}(z_i, z'<span style=\"font-style: italic\">i) / \\tau}}{\\sum</span>{j=1}^N \\left( e^{\\text{sim}(z_i, z'_j) / \\tau} +       \n",
       "e^{\\text{sim}(z'_i, z_j) / \\tau} \\right)},$                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                      \u001b[1mPage\u001b[0m                                                       ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\n",
       "                                                       \u001b[1;4mText\u001b[0m                                                        \n",
       "\n",
       "Fine-tuning Tasks and Datasets. We use 8 benchmark datasets from the MoleculeNet (Wu et al. 2018a) to per- form the\n",
       "experiments, which cover a wide range of molec- ular tasks such as quantum mechanics, physical chemistry,          \n",
       "biophysics, and physiology. For each dataset, as suggested by (Wu et al. 2018a), we apply three independent runs on\n",
       "three random-seeded random splitting or scaffold split- ting with a ratio for train/validation/test as 8:1:1.      \n",
       "Details of datasets and dataset splitting are deferred to Appendix B.1                                             \n",
       "\n",
       "Performance under Fine-tune Protocol. We first exam- ine whether the proposed KCL performs better than SOTA        \n",
       "methods. Table 2 displays the complete results of supervised learning baselines and pre-trained methods, where the \n",
       "un- derlined cells indicate the previous SOTAs, and the cells with bold show the best result achieved by KCL. The  \n",
       "Tox21, SIDER, and ClinTox are all multiple-task learning tasks, including totally 42 classification tasks. We also \n",
       "imple- mented two versions of our KCL model, the original molec-                                                   \n",
       "\n",
       "Implementation details. We use the Adam optimizer with an initial learning rate of 0.0001 and batch size of 256.   \n",
       "For pre-training models, the running epoch is fixed to 20. The temperature $\\tau$ is set as 0.1. For downstream    \n",
       "tasks, we use early stopping on the validation set. We apply the random search to obtain the best hyper-parameters \n",
       "based on the vali- dation set. Our model is implemented with PyTorch (Paszke et al. 2019) and Deep Graph Library   \n",
       "(Wang et al. 2019). We develop all codes on a Ubuntu Server with 4 GPUs (NVIDIA GeForce 1080Ti). More experimental \n",
       "details are available in Appendix C and D.                                                                         \n",
       "\n",
       "Contrastive Loss. We augmented a minibatch of $N$ simi- lar molecular graphs with knowledge-guided graph augmen-   \n",
       "tation, resulting in a total of $2N$ graphs. Following (You et al. 2020; Chen et al. 2020), given a positive pair, \n",
       "we treat the other $2(N-1)$ graphs within the same minibatch as hard negative samples. We utilize NT-Xent as our   \n",
       "objective func- tion like in (Hjelm et al. 2019; Chen et al. 2020; You et al. 2020; Carse, Carey, and McKenna      \n",
       "2021). The training ob- jective for ($G_i$, $G'_i$) is defined as                                                  \n",
       "\n",
       "where $e_1$, $e_2$ denotes the fingerprints, $N_1$, $N_2$ denotes the number of 1s in $e_1$, $e_2$ respectively,   \n",
       "and $N_{12}$ denotes the number of 1s in the intersection of $e_1$, $e_2$. In order to en- sure all molecules have \n",
       "negative samples, instead of setting a fixed threshold, we sorted samples by similarity and selected a batch of    \n",
       "most similar molecules as the negative samples.                                                                    \n",
       "\n",
       "where 7 denotes the temperature parameter and sim(z1, z2)                                                          \n",
       "\n",
       "$\\frac{z_1^T z_2}{||z_1|| \\cdot ||z_2||}$. The final loss is computed across all positive pairs in the minibatch.  \n",
       "\n",
       "fixed length binary vector. Then we calculate the molecular similarity through their Tanimoto coefficient (Bajusz, \n",
       "Rácz, and Héberger 2015):                                                                                          \n",
       "\n",
       "In this section, we conduct extensive experiments to exam- ine the proposed method by answering the following ques-\n",
       "tions:                                                                                                             \n",
       "\n",
       "Pre-training Data Collection. We collect 250K unlabeled molecules sampled from the ZINC15 datasets (Sterling and   \n",
       "Irwin 2015) to pre-train KCL.                                                                                      \n",
       "\n",
       "Q2: Does the knowledge-guided graph augmentation in Module 1 learns better representations than general            \n",
       "augmentations?                                                                                                     \n",
       "\n",
       "\u001b[1mEvaluation Protocol.\u001b[0m The evaluation process follows two steps. We first pre-train the model and then evaluate the  \n",
       "learned model on downstream tasks under two protocols.                                                             \n",
       "\n",
       "Q1: How does KCL perform compared with state-of-the- art methods for molecular property prediction?                \n",
       "\n",
       "Q5: How can we interpret KCL(KMPNN) from a domain- specific perspective?                                           \n",
       "\n",
       "Q3: How do knowledge feature initialization and graph encoders in Module 2 affect KCL?                             \n",
       "\n",
       "Q4: How useful are the self-supervised contrastive learn- ing and hard negative strategy in Module 3?              \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[3mFine-tune protocol:\u001b[0m To achieve the full potential of our model, given graph embeddings output by the KCL        \n",
       "\u001b[1;33m   \u001b[0mencoder, we use an additional MLP to predict the property of the molecule. Fine-tune parameters in the encoders \n",
       "\u001b[1;33m   \u001b[0mand the MLP.                                                                                                    \n",
       "\u001b[1;33m • \u001b[0m\u001b[3mLinear Protocol:\u001b[0m For comparison of our model and contrastive learning baselines, we fix the graph embeddings    \n",
       "\u001b[1;33m   \u001b[0mfrom the pre-trained model, and train a linear classifier.                                                      \n",
       "\n",
       "\u001b[1mBaselines.\u001b[0m We adopt three types of baselines:                                                                      \n",
       "\n",
       "\u001b[1;33m • \u001b[0mSupervised learning methods: GCN (Kipf and Welling [2017]) and Weave (Kearnes et al. [2016]) are two types of   \n",
       "\u001b[1;33m   \u001b[0mgraph convolutional methods. MPNN (Gilmer et al. [2017]) and its variants DMPNN (Yang et al. [2019]), CMPNN     \n",
       "\u001b[1;33m   \u001b[0m(Song et al. [2020]), CoMPT (Chen et al. [2021]) consider the edge features and strengthen the message          \n",
       "\u001b[1;33m   \u001b[0minteractions between bonds and atoms during message passing.                                                    \n",
       "\u001b[1;33m • \u001b[0mPre-trained methods: N-GRAM (Liu et al. [2019]) con- ducts node embeddings by predicting the node attributes. Hu\n",
       "\u001b[1;33m   \u001b[0met al. (Hu et al. [2020]) and GROVER (Rong et al. [2020]) are pre-trained models incorporating both node- level \n",
       "\u001b[1;33m   \u001b[0mand graph-level pretext tasks.                                                                                  \n",
       "\u001b[1;33m • \u001b[0mGraph contrastive learning baselines: InfoGraph (Sun et al. [2020]) maximizes the mutual information between    \n",
       "\u001b[1;33m   \u001b[0mnodes and graphs. MICRO-Graph (Subramonian [2021]) is a motif-based contrastive method. GraphCL (You et al.     \n",
       "\u001b[1;33m   \u001b[0m[2020]) constructs contrastive views of graph data via hand-picking ad-hoc augmentations. JOAO (You et al.      \n",
       "\u001b[1;33m   \u001b[0m[2021]) automates the augmentation selection. MoCL (Sun et al. [2021]) utilizes domain knowledge at two levels  \n",
       "\u001b[1;33m   \u001b[0mto assist representation learning.                                                                              \n",
       "\u001b[1;33m • \u001b[0mGraph contrastive learning baselines: InfoGraph (Sun et al. 2020) maximizes the mutual information between nodes\n",
       "\u001b[1;33m   \u001b[0mand graphs. MICRO-Graph (Subramonian 2021) is a motif-based contrastive method. GraphCL (You et al.             \n",
       "\n",
       "\u001b[1;33m 2020 \u001b[0mconstructs contrastive views of graph data via hand-picking ad-hoc augmentations. JOAO (You et al.           \n",
       "\u001b[1;33m 2021 \u001b[0mautomates the augmentation selection. MoCL (Sun et al. 2021) utilizes domain knowledge at two levels to      \n",
       "\u001b[1;33m      \u001b[0massist representation learning.                                                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0mSupervised learning methods: GCN (Kipf and Welling                                                              \n",
       "\n",
       "\u001b[1;33m 2017 \u001b[0mand Weave (Kearnes et al. 2016) are two types of graph convolutional methods. MPNN (Gilmer et al.            \n",
       "\u001b[1;33m 2018 \u001b[0mand its variants DMPNN (Yang et al. 2019), CMPNN (Song et al. 2020), CoMPT (Chen et al. 2021) consider the   \n",
       "\u001b[1;33m      \u001b[0medge features and strengthen the message interactions between bonds and atoms during message passing.        \n",
       "\n",
       "\u001b[1;33m • \u001b[0mPre-trained methods: N-GRAM (Liu et al. 2019) con- ducts node embeddings by predicting the node attributes. Hu  \n",
       "\u001b[1;33m   \u001b[0met al. (Hu et al. 2020) and GROVER (Rong et al.                                                                 \n",
       "\n",
       "\u001b[1;33m 2020 \u001b[0mare pre-trained models incorporating both node- level and graph-level pretext tasks.                         \n",
       "\n",
       "\n",
       "                                                       \u001b[1;4mTitle\u001b[0m                                                       \n",
       "\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                      \u001b[1m4.2 Performance Comparison (Q1 & Q2)\u001b[0m                                       ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                             \u001b[1m4.1 Experimental Setup\u001b[0m                                              ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                  \u001b[1m4 Experiments\u001b[0m                                                  ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\n",
       "                                                     \u001b[1;4mFormulas\u001b[0m                                                      \n",
       "\n",
       "s(\\boldsymbol{e}\u001b[3m1, \\boldsymbol{e}\u001b[0m\u001b[3m2) = \\frac{N\u001b[0m\u001b[3m{12}}{N_1 + N_2 - N\u001b[0m{12}},                                             \n",
       "\n",
       "$\\ell_i = -\\log \\frac{e^{\\text{sim}(z_i, z'\u001b[3mi) / \\tau}}{\\sum\u001b[0m{j=1}^N \\left( e^{\\text{sim}(z_i, z'_j) / \\tau} +       \n",
       "e^{\\text{sim}(z'_i, z_j) / \\tau} \\right)},$                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from docsearch.core import Page\n",
    "\n",
    "page = Page.from_image(sample_filepaths[5], model_weights=MODEL_WEIGHTS)\n",
    "print(page)\n",
    "md = Markdown(page.md)\n",
    "console.print(md, crop=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = sample_filepaths[5].parent / sample_filepaths[5].stem\n",
    "page.full_save(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

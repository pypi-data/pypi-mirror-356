Product Requirements Document â€” Metaâ€¯Agent

â¸»

1â€¯Â·â€¯Overview

Metaâ€¯Agent is a â€œdeveloper of developersâ€: a Python application that, given a naturalâ€‘language specification, automatically produces a fullyâ€‘functional OpenAIâ€¯Agentsâ€¯SDK agent (codeâ€¯+â€¯testsâ€¯+â€¯guardrails). It eliminates handâ€‘coding boilerplate, accelerates prototyping, and enforces best practices from day one.
	â€¢	Problem solvedâ€‚Developers spend significant time translating English requirements into agent scaffolds, wiring tools, and writing safety checks.
	â€¢	Target usersâ€‚AI engineers, solutions architects, and power users who need bespoke agents quickly but canâ€™t invest days in setup.
	â€¢	Value propositionâ€‚Generate productionâ€‘ready agents in minutes, with builtâ€‘in validation, sandboxing, and documentation.

â¸»

2â€¯Â·â€¯Coreâ€¯Features

Feature	What it does	Why it matters	How it works (highâ€‘level)
Naturalâ€‘language spec ingestion	Accepts structured or freeâ€‘form specs describing goals, I/O contracts, tools, and constraints.	Lowers the barrier to entry; no DSL to learn.	Uses o3 to parse spec into an internal schema and identify work chunks.
Agent planner (Metaâ€¯Agent)	Decomposes the spec, orchestrates subâ€‘agents, assembles final artifact.	Central logic hub; enforces consistency & versioning.	Runs on o3 with minimal tools (webâ€¯search, template library, python_interpreter for compilation).
Toolâ€¯Designer subâ€‘agent	Generates runnable Python code for each required tool and its unit tests.	Eliminates stubs; ensures tools are functional dayâ€‘one.	o4â€‘miniâ€‘high + sandboxed interpreter; may call webâ€¯search for API discovery.
Guardrailâ€¯Designer subâ€‘agent	Creates validation logic (Pydantic, regex, policy checks) + guardrail tests.	Embeds safety & compliance early; prevents bad outputs.	Uses gptâ€‘4o; attaches to Agentsâ€‘SDK guardrail hooks.
Automated evaluation harness	Compiles generated code, executes unit tests, and surfaces results.	Guarantees â€œit actually runs.â€	Pytest inside sandbox container; results returned in summary.
Artifact bundle & dependency lock	Outputs agent.py, tests/â€¯, requirements.txt, and optional diagram.	Oneâ€‘command install & run; reproducible builds.	Metaâ€¯Agent writes files to target directory or pushes to Git repo.
Cost & trace telemetry	Logs token usage, latency, and spend per generation.	Keeps surprises off the cloud bill; aids optimization.	Leverages Agentsâ€‘SDK tracing and OpenAI usage APIs.



â¸»

3â€¯Â·â€¯Userâ€¯Experience

Personas
	1.	Rapid Prototyper (â€œAliceâ€) â€” AI startup engineer needing a demoâ€‘ready agent by tomorrow.
	2.	Enterprise Solutions Architect (â€œBobâ€) â€” integrates bespoke agents into client workflows; cares about compliance and audit trails.
	3.	Curious Hobbyist (â€œCharlieâ€) â€” explores AI agents but lacks deep coding expertise.

Key User Flows

Flow	Steps	Success signal
Create agent from scratch	(1) Run CLI â†’ paste spec or select template. (2) Metaâ€¯Agent generates files. (3) User python my_agent.py --demo.	Agent runs without error & passes autogenerated tests.
Iterate on existing agent	(1) Provide updated spec. (2) Meta diffâ€‘parses changes, regenerates only affected parts.	Modified agent works; unchanged tests still pass.
Inspect guardrails	(1) Run --audit. (2) Tool lists guardrail coverage and edge cases.	Developer signs off or tweaks validators.

UI/UX Considerations
	â€¢	CLI first, VSâ€¯Code extension later (syntaxâ€‘highlighted diff view).
	â€¢	Colored status banner (âœ”â€¯build passed, âœ–â€¯tests failed, ðŸ›¡ï¸ guardrail triggered).
	â€¢	Optional Markdown/mermaid diagram autoâ€‘rendered for README.

â¸»

4â€¯Â·â€¯Technicalâ€¯Architecture

Component	Responsibility	Tech / Integration
Metaâ€¯Agent Orchestrator	Planning, subâ€‘agent delegation, artifact assembly.	openai-agents-python, modelâ€¯=o3
Toolâ€¯Designer Agent	Code + unit tests for tools.	modelâ€¯=o4â€‘miniâ€‘high, sandboxed python_interpreter
Guardrailâ€¯Designer Agent	Validation logic & tests.	modelâ€¯=gptâ€‘4o, Agentsâ€‘SDK guardrails
Evaluation Harness	Compile, execute, report.	Pytest in Docker (no outbound net)
Template/Pattern Library	Store proven agent blueprints.	Local JSON/YAML files; search indexed by embeddings
Artifact Repository	Save generated bundles, metadata.	Git backend or S3 bucket
Telemetry & Tracing	Cost, latency, usage stats.	Agentsâ€‘SDK tracing hooks, OpenAI usage API

Data Models
	â€¢	SpecSchema â€” parsed user requirements (goal, I/O, tools[], guardrails[], constraints).
	â€¢	ToolArtifact â€” {code:str, tests:str, metadata}.
	â€¢	AgentBundle â€” {agent_code, tests, requirements, diagram, trace}.

Infrastructure
	â€¢	Dockerized sandbox with resource limits.
	â€¢	Optional Kubernetes job runner for concurrent generations.
	â€¢	Vector store (Chroma) for template retrieval & caching webâ€¯search hits.

â¸»

5â€¯Â·â€¯Developmentâ€¯Roadmap (scopeâ€‘only)

Phase	Deliverables (no timelines)
0 Â· Foundations	CLI skeleton, SpecSchema, Metaâ€¯Agent orchestrator stub, Docker sandbox.
1 Â· MVP	â€¢ Naturalâ€‘language ingestion â†’ working agent.py â€¢ Single Toolâ€¯Designer/Guardrail flow â€¢ Basic pytest harness â€¢ Requirements lockfile generation.
2 Â· Validation & Observability	â€¢ Guardrail test coverage report â€¢ Cost/latency telemetry dashboard â€¢ Compileâ€‘time linting (ruff/pyright).
3 Â· Templates & UX polish	â€¢ Library of common agent archetypes â€¢ Mermaid diagram autoâ€‘generation â€¢ Colored CLI feedback.
4 Â· Extensibility	â€¢ VSâ€¯Code extension â€¢ Plugin system for custom subâ€‘agents â€¢ API endpoints for SaaS version.
5 Â· Enterprise Hardening	â€¢ RBAC, audit logs, SSO â€¢ Policy engine integration â€¢ Multiâ€‘tenant artifact store.



â¸»

6â€¯Â·â€¯Logical Dependency Chain
	1.	SpecSchema & CLI skeleton (foundation).
	2.	Sandbox container â€” needed before executing any generated code.
	3.	Metaâ€¯Agent orchestration â€” operates spec â†’ tasks.
	4.	Toolâ€¯Designer + Guardrailâ€¯Designer â€” unlock functional code generation.
	5.	Evaluation harness â€” proves MVP works endâ€‘toâ€‘end.
	6.	Artifact bundling & dependency lock â€” delivers usable output.
	7.	Telemetry & diagrams â€” polish after core loop is stable.
	8.	Extended UX layers (VSâ€¯Code, SaaS API) â€” once stable backend exists.

Goal is earliest possible â€œhelloâ€‘world agentâ€ that passes tests, then iterate outward.

â¸»

7â€¯Â·Risks & Mitigations

Risk	Impact	Mitigation
LLM hallucination â†’ nonâ€‘runnable code	Build fails	Automated compilation + retry loops; constrain generation via templates.
Sandbox escape / malicious code	Security breach	Docker seccomp/apparmor, readâ€‘only FS, no outbound net unless allowâ€‘listed.
OpenAI API cost spikes	Budget blowâ€‘out	Telemetry guard; cost caps per generation; use cheaper models when possible.
Spec ambiguity	Rework, user frustration	Metaâ€¯Agent autoâ€‘asks clarifying questions; provide spec template wizard.
External API drift	Generated tools break over time	Healthâ€‘check tests; version pin APIs; regenerate tools on failure.
Scope creep delaying MVP	Missed delivery	Strict phase gating; prioritize â€œagent builds, runs, testedâ€ above all.



â¸»

8â€¯Â·â€¯Appendix
	â€¢	Research links
	â€¢	OpenAI Agents SDK docs (v0.0.7)
	â€¢	Bestâ€‘practice guardrail frameworks (Guardian, Rebuff)
	â€¢	Pytest sandboxing strategies
	â€¢	Example Spec Template (YAML)

goal: "Summarize Slack threads into daily digest"
io_contract:
  input: "Channel history (JSON)"
  output: "Markdown summary"
tools:
  - name: slack_api
    requirement: "Fetch channel messages"
guardrails:
  - "No PII leakage"
constraints:
  max_latency: 30s
  max_cost: $0.50


	â€¢	Mermaid Diagram Sample

graph TD
  A[User Spec] --> B[Meta Agent]
  B --> C[Tool Designer]
  B --> D[Guardrail Designer]
  C --> E[Code + Tests]
  D --> E
  E --> F[Evaluation Harness]
  F --> G[Agent Bundle]
"""Data augmentation library for paper data."""

from __future__ import annotations
import importlib.util
from pathlib import Path
from typing import Literal
import polars as pl
import logging

logger = logging.getLogger(__name__)


def merge_datasets(
    left_df: pl.DataFrame,
    right_df: pl.DataFrame,
    on_cols: list[str],
    how: Literal["left", "inner", "outer", "full"] = "left",
) -> pl.DataFrame:
    """
    Merges two Polars DataFrames.

    Args:
        left_df: The left DataFrame.
        right_df: The right DataFrame.
        on_cols: A list of column names to merge on.
        how: The type of join (e.g., "left", "inner", "outer", "full").

    Returns:
        The merged Polars DataFrame.
    """
    logger.info(f"Merging datasets on columns: {on_cols} with how='{how}'")
    merged_df = left_df.join(right_df, on=on_cols, how=how)
    logger.info(f"Merge complete. Resulting shape: {merged_df.shape}")
    return merged_df


def lag_columns(
    df: pl.DataFrame,
    date_col: str,
    id_col: str | None,
    cols_to_lag: list[str],
    periods: int,
    drop_original_cols_after_lag: bool,
    restore_names: bool,
    drop_generated_nans: bool,
) -> pl.DataFrame:
    """
    Lags or leads specified columns in a Polars DataFrame.

    If an `id_col` is provided, the lagging is performed within each group
    defined by `id_col`, ordered by `date_col`. Otherwise, it's a simple
    time-series lag ordered by `date_col`.

    Args:
        df: The input Polars DataFrame.
        date_col: The name of the date column to order by.
        id_col: The name of the identifier column for panel data lagging (e.g., 'permco').
                If None, a simple time-series lag is performed.
        cols_to_lag: A list of column names to apply the lag/lead operation to.
        periods: The number of periods to shift.
                 Positive for lagging (e.g., 1 for previous period's value).
                 Negative for leading (e.g., -1 for next period's value).
        drop_original_cols_after_lag: If True, the original (unlagged/unled) columns are dropped
                                      from the output DataFrame.
        restore_names: If True, the lagged columns will be renamed back to their
                       original names (e.g., 'volume_lag_1' becomes 'volume').
                       This requires `drop_original_cols_after_lag` to be true.
        drop_generated_nans: If True, rows containing NaN values that were
                             generated by the lagging operation will be dropped.

    Returns:
        A new Polars DataFrame with the lagged/led columns.
    """
    if not cols_to_lag:
        logger.info("No columns specified for lagging. Returning original DataFrame.")
        return df

    if date_col not in df.schema.names():
        raise ValueError(f"Date column '{date_col}' not found in DataFrame.")
    if not isinstance(df[date_col].dtype, (pl.Date, pl.Datetime)):
        raise ValueError(
            f"Date column '{date_col}' must be of Date or Datetime type for lagging."
        )

    # Validate that all specified columns exist in the DataFrame
    missing_cols = [c for c in cols_to_lag if c not in df.schema.names()]
    if missing_cols:
        raise ValueError(
            f"Columns specified for lagging not found in DataFrame: {missing_cols}"
        )

    suffix = f"_lag_{periods}" if periods > 0 else f"_lead_{abs(periods)}"
    lagged_col_names = [f"{col}{suffix}" for col in cols_to_lag]

    logger.info(
        f"Applying {'lag' if periods > 0 else 'lead'} of {abs(periods)} periods to columns: {cols_to_lag}"
    )

    # Define expressions for creating lagged columns
    expressions = []
    for i, col in enumerate(cols_to_lag):
        lagged_col_name = lagged_col_names[i]
        if id_col:
            # Panel data lag: group by id_col, sort by date_col within the group
            expr = (
                pl.col(col)
                .sort_by(date_col)
                .shift(periods)
                .over(id_col)
                .alias(lagged_col_name)
            )
        else:
            # Time-series lag: sort the whole frame by date_col first
            expr = pl.col(col).shift(periods).alias(lagged_col_name)
        expressions.append(expr)

    # Sort the dataframe correctly before applying expressions
    # For panel data, .over() handles internal sorting. For time-series, we must sort the frame.
    out_df = df.clone()
    if not id_col:
        logger.info(f"Sorting DataFrame by '{date_col}' for time-series operations.")
        out_df = out_df.sort(date_col)

    # Apply all expressions at once
    out_df = out_df.with_columns(expressions)

    if drop_generated_nans:
        logger.info(
            f"Dropping rows with NaN values generated by lagging in columns: {lagged_col_names}"
        )
        # Combine all null checks with an OR condition
        combined_null_condition = pl.any_horizontal(
            pl.col(c).is_null() for c in lagged_col_names
        )
        initial_rows = out_df.shape[0]
        out_df = out_df.filter(~combined_null_condition)
        dropped_rows = initial_rows - out_df.shape[0]
        logger.info(f"Dropped {dropped_rows} rows due to generated NaNs.")

    if drop_original_cols_after_lag:
        logger.info(f"Dropping original columns after lagging: {cols_to_lag}")
        out_df = out_df.drop(cols_to_lag)

    if restore_names:
        renaming_dict = {
            lagged_name: original_name
            for lagged_name, original_name in zip(lagged_col_names, cols_to_lag)
            if lagged_name in out_df.schema.names()
        }
        if renaming_dict:
            logger.info(
                f"Restoring original column names: {list(renaming_dict.keys())} -> {list(renaming_dict.values())}"
            )
            out_df = out_df.rename(renaming_dict)

    # Final sort for deterministic output
    sort_keys = [id_col, date_col] if id_col else [date_col]
    out_df = out_df.sort(sort_keys)

    logger.info(f"Lag operation complete. Resulting shape: {out_df.shape}")
    return out_df


def run_custom_script(
    df: pl.DataFrame, project_root: Path, script: str, function_name: str
) -> pl.DataFrame:
    """
    Dynamically loads and executes a user-defined function from a Python script.

    Args:
        df: The input Polars DataFrame to be transformed.
        project_root: The absolute path to the root of the user's project.
        script: The filename of the Python script, located in the project's
                'data/scripts/' directory.
        function_name: The name of the function to execute from the script.

    Returns:
        A new Polars DataFrame as returned by the custom function.
    """
    script_full_path = project_root / "data" / "scripts" / script
    if not script_full_path.is_file():
        raise FileNotFoundError(
            f"Custom script not found. Looked for '{script}' inside the 'data/scripts/' directory."
        )

    logger.info(
        f"Executing function '{function_name}' from script '{script_full_path.relative_to(project_root)}'"
    )

    # Dynamically load the module from the specified path
    spec = importlib.util.spec_from_file_location(
        name=script_full_path.stem, location=str(script_full_path)
    )
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not create module spec for {script_full_path}")

    custom_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(custom_module)

    # Get the function from the loaded module
    if not hasattr(custom_module, function_name):
        raise AttributeError(
            f"Function '{function_name}' not found in script '{script}'"
        )

    transform_func = getattr(custom_module, function_name)
    if not callable(transform_func):
        raise TypeError(
            f"The attribute '{function_name}' in '{script}' is not a callable function."
        )

    # Execute the function and validate its output
    output_df = transform_func(df)
    if not isinstance(output_df, pl.DataFrame):
        raise TypeError(
            f"The function '{function_name}' from '{script}' must return a Polars DataFrame, "
            f"but it returned type '{type(output_df).__name__}'."
        )

    return output_df


def create_macro_firm_interactions(
    df: pl.DataFrame,
    macro_columns: list[str],
    firm_columns: list[str],
    drop_macro_columns: bool,
) -> pl.DataFrame:
    """
    Creates interaction columns between macro characteristics and firm characteristics.
    Each interaction column is the product of a macro column and a firm column.

    Args:
        df: The input Polars DataFrame containing both macro and firm characteristics.
        macro_columns: A list of column names representing macro characteristics.
        firm_columns: A list of column names representing firm characteristics.
        drop_macro_columns: If True, the original macro columns will be dropped
                            from the output DataFrame after creating interactions.

    Returns:
        A new Polars DataFrame with the added interaction columns.
    """
    out_df = df.clone()
    interaction_expressions = []
    new_interaction_cols = []

    # Validate that all specified columns exist in the DataFrame
    missing_macro_cols = [c for c in macro_columns if c not in out_df.schema.names()]
    if missing_macro_cols:
        raise ValueError(
            f"Macro columns specified for interaction not found in DataFrame: {missing_macro_cols}"
        )
    missing_firm_cols = [c for c in firm_columns if c not in out_df.schema.names()]
    if missing_firm_cols:
        raise ValueError(
            f"Firm columns specified for interaction not found in DataFrame: {missing_firm_cols}"
        )

    logger.info(
        f"Creating interaction columns between macro: {macro_columns} and firm: {firm_columns}..."
    )

    for firm_col in firm_columns:
        for macro_col in macro_columns:
            interaction_col_name = f"{firm_col}_x_{macro_col}"
            new_interaction_cols.append(interaction_col_name)
            interaction_expressions.append(
                (pl.col(firm_col) * pl.col(macro_col)).alias(interaction_col_name)
            )

    if interaction_expressions:
        out_df = out_df.with_columns(interaction_expressions)
        logger.info(f"Created {len(new_interaction_cols)} new interaction columns.")
    else:
        logger.info("No interaction columns were created.")

    if drop_macro_columns:
        logger.info(f"Dropping original macro columns: {macro_columns}")
        out_df = out_df.drop(macro_columns)

    logger.info(f"Interaction creation complete. Resulting shape: {out_df.shape}")
    return out_df


def create_macro_firm_interactions_lazy(
    ldf: pl.LazyFrame,
    macro_columns: list[str],
    firm_columns: list[str],
    drop_macro_columns: bool,
) -> pl.LazyFrame:
    """
    Lazily creates interaction columns between macro and firm characteristics.
    This function defines the operations but does not execute them immediately.

    Args:
        ldf: The input Polars LazyFrame.
        macro_columns: A list of column names for macro characteristics.
        firm_columns: A list of column names for firm characteristics.
        drop_macro_columns: If True, defines an operation to drop the original
                            macro columns from the final output.

    Returns:
        A new Polars LazyFrame with the interaction operations defined.
    """
    interaction_expressions = []
    new_interaction_cols = []

    # Validate that all specified columns exist in the LazyFrame's schema
    missing_macro_cols = [
        c for c in macro_columns if c not in ldf.collect_schema().names()
    ]
    if missing_macro_cols:
        raise ValueError(
            f"Macro columns specified for interaction not found in LazyFrame: {missing_macro_cols}"
        )
    missing_firm_cols = [
        c for c in firm_columns if c not in ldf.collect_schema().names()
    ]
    if missing_firm_cols:
        raise ValueError(
            f"Firm columns specified for interaction not found in LazyFrame: {missing_firm_cols}"
        )

    logger.info(
        f"Defining lazy interaction columns between macro: {macro_columns} and firm: {firm_columns}..."
    )

    for firm_col in firm_columns:
        for macro_col in macro_columns:
            interaction_col_name = f"{firm_col}_x_{macro_col}"
            new_interaction_cols.append(interaction_col_name)
            interaction_expressions.append(
                (pl.col(firm_col) * pl.col(macro_col)).alias(interaction_col_name)
            )

    if interaction_expressions:
        ldf = ldf.with_columns(interaction_expressions)
        logger.info(
            f"Defined {len(new_interaction_cols)} new lazy interaction columns."
        )
    else:
        logger.info("No lazy interaction columns were defined.")

    if drop_macro_columns:
        logger.info(
            f"Defining operation to drop original macro columns: {macro_columns}"
        )
        ldf = ldf.drop(macro_columns)

    logger.info("Lazy interaction definition complete.")
    return ldf


def create_dummies(
    df: pl.DataFrame,
    column_to_dummy: str,
    drop_original_col: bool,
) -> pl.DataFrame:
    """
    Creates one-hot encoded (dummy) columns from a specified categorical column.

    Args:
        df: The input Polars DataFrame.
        column_to_dummy: The name of the column to convert to dummies.
        drop_original_col: If True, the original column is dropped from the output.

    Returns:
        A new Polars DataFrame with the added dummy columns.
    """
    if column_to_dummy not in df.schema.names():
        raise ValueError(
            f"Column '{column_to_dummy}' not found in DataFrame for dummy generation."
        )

    logger.info(f"Creating dummy variables for column '{column_to_dummy}'...")

    # Create a DataFrame containing only the one-hot encoded columns.
    # This is a robust way to handle it, as to_dummies works on a selection.
    dummy_df = df.select(pl.col(column_to_dummy)).to_dummies(drop_first=False)

    logger.info(f"Generated {dummy_df.shape[1]} new dummy columns.")

    # Concatenate the new dummy columns with the original DataFrame
    out_df = pl.concat([df, dummy_df], how="horizontal")

    if drop_original_col:
        logger.info(f"Dropping original column '{column_to_dummy}'.")
        out_df = out_df.drop(column_to_dummy)

    logger.info(f"Dummy generation complete. Resulting shape: {out_df.shape}")
    return out_df

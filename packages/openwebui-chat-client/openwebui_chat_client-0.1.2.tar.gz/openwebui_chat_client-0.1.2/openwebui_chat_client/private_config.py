from openwebui_chat_client import OpenWebUIClient
import logging
from PIL import Image, ImageDraw, ImageFont

# --- Configuration ---
BASE_URL = "http://localhost:3003"  # Replace with your OpenWebUI server URL
# Obtain your JWT token or API key for authentication from your account settings.
AUTH_TOKEN = "sk-26c968f00efd414a839ee725e3b082e8"
MODEL_ID = "gpt-4.1"
SINGLE_MODEL = "gpt-4.1"
MULTIMODAL_MODEL = "gemini-2.0-flash"  # å•æ¨¡å‹å¯¹è¯ä½¿ç”¨çš„é»˜è®¤æ¨¡å‹

# examples/basic_usage.py

# ç¡®ä¿è¿™äº›æ¨¡å‹åœ¨ä½ çš„ Open WebUI ä¸­éƒ½å¯ç”¨
PARALLEL_MODELS = ["gpt-4.1", "gemini-2.5-flash"]
# å¤šæ¨¡æ€æµ‹è¯•æ¨¡å‹

# --- ä¸ºåº”ç”¨ç¨‹åºé…ç½®æ—¥å¿— ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)


def create_test_image(text: str, filename: str) -> str:
    """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºåˆ›å»ºå¸¦æ–‡å­—çš„æµ‹è¯•å›¾ç‰‡ã€‚"""
    try:
        img = Image.new("RGB", (500, 100), color=(20, 40, 80))
        d = ImageDraw.Draw(img)
        try:
            font = ImageFont.truetype("arial.ttf", 30)
        except IOError:
            font = ImageFont.load_default()
        d.text((10, 10), text, fill=(255, 255, 200), font=font)
        img.save(filename)
        logging.info(f"âœ… Created test image: {filename}")
        return filename
    except ImportError:
        logging.warning("Pillow library not installed. Cannot create test image.")
        return None


def run_all_demos():
    """è¿è¡Œæ‰€æœ‰åŠŸèƒ½çš„æ¼”ç¤ºã€‚"""
    if AUTH_TOKEN == "YOUR_AUTH_TOKEN":
        logging.error("ğŸ›‘ Please set your 'AUTH_TOKEN' in the script.")
        return

    # ä½¿ç”¨ä¸€ä¸ªé»˜è®¤æ¨¡å‹åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œè¿™ä¸ªæ¨¡å‹å¯ä»¥åœ¨ chat() æ–¹æ³•ä¸­è¢«è¦†ç›–
    client = OpenWebUIClient(BASE_URL, AUTH_TOKEN, default_model_id=SINGLE_MODEL)

    # # --- åœºæ™¯ 1: å•æ¨¡å‹å¯¹è¯ ---
    # print("\n" + "#" * 20 + " SCENE 1: Single-Model Chat " + "#" * 20)
    # response, _ = client.chat(
    #     question="What is the difference between a library and a framework?",
    #     chat_title="Tech Concepts: Library vs Framework",
    #     folder_name="Tech Discussions",
    #     model_id=SINGLE_MODEL,  # å¯ä»¥æ˜¾å¼æŒ‡å®šæ¨¡å‹
    # )
    # if response:
    #     print(f"\nğŸ¤– [{SINGLE_MODEL}'s Response]:\n{response}\n")

    # # --- åœºæ™¯ 2: å¤šæ¨¡å‹å¹¶è¡Œå¯¹è¯ (ç¬¬ä¸€è½®) ---
    # print("\n" + "#" * 20 + " SCENE 2: Multi-Model Parallel Chat (Round 1) " + "#" * 20)
    # parallel_responses = client.parallel_chat(
    #     question="In one sentence, what is the most exciting thing about space exploration?",
    #     chat_title="Space Exploration Insights",
    #     model_ids=PARALLEL_MODELS,
    #     folder_name="Science",
    # )
    # if parallel_responses:
    #     for model, content in parallel_responses.items():
    #         print(f"\nğŸ¤– [{model}'s Response]:\n{content}\n")

    # # --- åœºæ™¯ 3: ç»§ç»­å¤šæ¨¡å‹å¹¶è¡Œå¯¹è¯ (ç¬¬äºŒè½®) ---
    # print("\n" + "#" * 20 + " SCENE 3: Multi-Model Parallel Chat (Round 2) " + "#" * 20)
    # # å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨æ‰¾åˆ° "Space Exploration Insights" è¿™ä¸ªèŠå¤©å¹¶ç»§ç»­
    # parallel_responses_2 = client.parallel_chat(
    #     question="Based on your previous answer, name one specific mission that exemplifies this.",
    #     chat_title="Space Exploration Insights",
    #     model_ids=PARALLEL_MODELS,
    #     folder_name="Science",
    # )
    # if parallel_responses_2:
    #     for model, content in parallel_responses_2.items():
    #         print(f"\nğŸ¤– [{model}'s Response]:\n{content}\n")

    # # --- åœºæ™¯ 4: å¤šæ¨¡æ€å¯¹è¯ (ä½¿ç”¨å•æ¨¡å‹chatæ–¹æ³•) ---
    # print("\n" + "#" * 20 + " SCENE 4: Multimodal Chat " + "#" * 20)
    # image_path = create_test_image("Welcome to the Future!", "multimodal_test.png")
    # if image_path:
    #     # æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ chat() æ–¹æ³•ï¼Œä½†ä¼ å…¥å›¾ç‰‡è·¯å¾„å’Œå¤šæ¨¡æ€æ¨¡å‹ID
    #     response, _ = client.chat(
    #         question="What message is written in this image?",
    #         chat_title="Multimodal Test",
    #         folder_name="Tech Demos",
    #         image_paths=[image_path],
    #         model_id=MULTIMODAL_MODEL,
    #     )
    #     if response:
    #         print(f"\nğŸ¤– [{MULTIMODAL_MODEL}'s Response]:\n{response}\n")
    # else:
    #     logging.warning(
    #         "Skipping multimodal demo because test image could not be created."
    #     )
        
    # **************************************************************************
    # --- Scene 3: Multi-Model, Multimodal Parallel Chat ---
    # This is the ultimate test case.
    # **************************************************************************
    print("\n" + "#"*20 + " SCENE 3: Multi-Model & Multimodal Chat " + "#"*20)
    image_path = create_test_image("Project 'Phoenix' Status: GREEN", "multimodal_status.png")
    
    if image_path:
        # We use the parallel_chat() method with both text and an image.
        multimodal_responses = client.parallel_chat(
            question="Summarize the status update from this image. Be concise.",
            chat_title="Project Phoenix Status Report",
            folder_name="Project Updates",
            image_paths=[image_path],
            model_ids=PARALLEL_MODELS
        )
        if multimodal_responses:
            for model, content in multimodal_responses.items():
                print(f"\nğŸ¤– [{model}'s Response]:\n{content}\n")
    else:
        logging.warning("Skipping multimodal demo because test image could not be created.")


    print("\nğŸ‰ All demo scenarios completed. Please check your Open WebUI interface.")


if __name__ == "__main__":
    run_all_demos()

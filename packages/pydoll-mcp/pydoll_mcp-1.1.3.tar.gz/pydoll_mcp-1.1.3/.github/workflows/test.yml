name: Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  FORCE_COLOR: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"

jobs:
  # Unit Tests with Coverage
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
        # Reduce matrix for non-main branches
        exclude:
          - os: windows-latest
            python-version: "3.8"
          - os: macos-latest
            python-version: "3.8"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install system dependencies (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            chromium-browser \
            xvfb \
            libnss3-dev \
            libatk-bridge2.0-0 \
            libdrm2 \
            libxcomposite1 \
            libxdamage1 \
            libxrandr2 \
            libgbm1 \
            libxss1 \
            libasound2

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -e ".[test]"

      - name: Create test directories
        run: |
          mkdir -p tests/unit tests/fixtures tests/mocks

      - name: Run unit tests
        run: |
          python -m pytest tests/unit/ \
            --cov=pydoll_mcp \
            --cov-report=xml \
            --cov-report=term \
            --cov-report=html \
            --junit-xml=junit-${{ matrix.os }}-${{ matrix.python-version }}.xml \
            --maxfail=10 \
            --tb=short \
            -v

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            junit-${{ matrix.os }}-${{ matrix.python-version }}.xml
            coverage.xml
            htmlcov/

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-${{ matrix.os }}-py${{ matrix.python-version }}

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name != 'schedule'

    services:
      chrome:
        image: browserless/chrome:latest
        ports:
          - 3000:3000

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Wait for Chrome service
        run: |
          timeout 60 bash -c 'until curl -s http://localhost:3000/json/version; do sleep 1; done'

      - name: Run integration tests
        run: |
          python -m pytest tests/integration/ \
            --junit-xml=integration-results.xml \
            --maxfail=5 \
            --tb=short \
            -v
        env:
          CHROME_WS_ENDPOINT: "ws://localhost:3000"
          PYDOLL_TEST_MODE: "integration"

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: integration-results.xml

  # End-to-End Tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name != 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Start MCP server in background
        run: |
          python -m pydoll_mcp.server &
          echo $! > mcp_server.pid
          sleep 5  # Give server time to start

      - name: Run E2E tests
        run: |
          python -m pytest tests/e2e/ \
            --junit-xml=e2e-results.xml \
            --maxfail=3 \
            --tb=short \
            -v
        env:
          CHROME_BIN: /usr/bin/google-chrome
          MCP_SERVER_URL: "stdio"

      - name: Stop MCP server
        if: always()
        run: |
          if [ -f mcp_server.pid ]; then
            kill $(cat mcp_server.pid) || true
            rm mcp_server.pid
          fi

      - name: Upload E2E test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-results
          path: e2e-results.xml

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"
          pip install pytest-benchmark pytest-xdist

      - name: Run performance tests
        run: |
          python -m pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=benchmark.json \
            --benchmark-min-rounds=3 \
            --benchmark-max-time=60 \
            --maxfail=1 \
            -v
        env:
          CHROME_BIN: /usr/bin/google-chrome

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: benchmark.json

      - name: Comment performance results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const benchmark = JSON.parse(fs.readFileSync('benchmark.json', 'utf8'));
            
            let comment = '## 🚀 Performance Test Results\\n\\n';
            comment += '| Test | Min | Max | Mean | Std Dev |\\n';
            comment += '|------|-----|-----|------|---------|\\n';
            
            benchmark.benchmarks.forEach(test => {
              const stats = test.stats;
              comment += `| ${test.name} | ${stats.min.toFixed(3)}s | ${stats.max.toFixed(3)}s | ${stats.mean.toFixed(3)}s | ${stats.stddev.toFixed(3)}s |\\n`;
            });
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Load Tests
  load-tests:
    name: Load Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"
          pip install locust

      - name: Run load tests
        run: |
          locust -f tests/load/locustfile.py \
            --headless \
            --users 10 \
            --spawn-rate 2 \
            --run-time 5m \
            --host http://localhost:8080 \
            --csv load_test_results

      - name: Upload load test results
        uses: actions/upload-artifact@v3
        with:
          name: load-test-results
          path: load_test_results*

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"
          pip install bandit safety

      - name: Run Bandit security scan
        run: |
          bandit -r pydoll_mcp/ \
            -f json \
            -o bandit-report.json \
            --skip B101 || true

      - name: Run Safety check
        run: |
          safety check \
            --json \
            --output safety-report.json || true

      - name: Upload security scan results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-scan-results
          path: |
            bandit-report.json
            safety-report.json

  # Compatibility Tests
  compatibility-tests:
    name: Compatibility Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    strategy:
      matrix:
        mcp-version: ["1.0.0", "1.1.0"]
        pydoll-version: ["2.2.0", "2.2.1"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install specific versions
        run: |
          python -m pip install --upgrade pip
          pip install mcp==${{ matrix.mcp-version }}
          pip install pydoll-python==${{ matrix.pydoll-version }}
          pip install -e ".[test]"

      - name: Run compatibility tests
        run: |
          python -m pytest tests/compatibility/ \
            --junit-xml=compatibility-${{ matrix.mcp-version }}-${{ matrix.pydoll-version }}.xml \
            -v
        env:
          MCP_VERSION: ${{ matrix.mcp-version }}
          PYDOLL_VERSION: ${{ matrix.pydoll-version }}

      - name: Upload compatibility test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: compatibility-test-results-${{ matrix.mcp-version }}-${{ matrix.pydoll-version }}
          path: compatibility-${{ matrix.mcp-version }}-${{ matrix.pydoll-version }}.xml

  # Test Result Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests]
    if: always()

    steps:
      - name: Download all test results
        uses: actions/download-artifact@v3
        with:
          path: test-results/

      - name: Publish test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Test Results Summary
          path: 'test-results/**/*.xml'
          reporter: java-junit
          fail-on-error: true

      - name: Generate test summary
        if: always()
        run: |
          echo "## 📊 Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count test files
          UNIT_TESTS=$(find test-results/ -name "*unit*" -name "*.xml" | wc -l)
          INTEGRATION_TESTS=$(find test-results/ -name "*integration*" -name "*.xml" | wc -l)
          E2E_TESTS=$(find test-results/ -name "*e2e*" -name "*.xml" | wc -l)
          
          echo "- 🧪 Unit Tests: $UNIT_TESTS environments" >> $GITHUB_STEP_SUMMARY
          echo "- 🔗 Integration Tests: $INTEGRATION_TESTS environment" >> $GITHUB_STEP_SUMMARY
          echo "- 🎭 E2E Tests: $E2E_TESTS environment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Test status
          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "✅ Unit Tests: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Unit Tests: FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.integration-tests.result }}" == "success" ]; then
            echo "✅ Integration Tests: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Integration Tests: FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.e2e-tests.result }}" == "success" ]; then
            echo "✅ E2E Tests: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ E2E Tests: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

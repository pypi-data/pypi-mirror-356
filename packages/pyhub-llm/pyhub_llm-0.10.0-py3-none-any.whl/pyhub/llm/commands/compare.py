from pathlib import Path
from typing import Optional

import typer
from rich.columns import Columns
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# from pyhub import init
from pyhub.llm import LLM
from pyhub.llm.types import LLMChatModelEnum

console = Console()


def compare(
    ctx: typer.Context,
    query: Optional[str] = typer.Argument(None, help="ë¹„êµí•  ì§ˆë¬¸"),
    models: list[LLMChatModelEnum] = typer.Option(
        [LLMChatModelEnum.GPT_4O_MINI, LLMChatModelEnum.CLAUDE_HAIKU_3_5_LATEST],
        "--model",
        "-m",
        help="ë¹„êµí•  ëª¨ë¸ë“¤ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)",
    ),
    system_prompt: str = typer.Option(
        None,
        "--system-prompt",
        "-s",
        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
    ),
    temperature: float = typer.Option(
        0.7,
        "--temperature",
        "-t",
        help="ì‘ë‹µ ì˜¨ë„",
    ),
    max_tokens: int = typer.Option(
        1000,
        "--max-tokens",
        help="ìµœëŒ€ í† í° ìˆ˜",
    ),
    output_format: str = typer.Option(
        "side-by-side",
        "--format",
        "-f",
        help="ì¶œë ¥ í˜•ì‹ (side-by-side, sequential, table)",
    ),
    output_path: Optional[Path] = typer.Option(
        None,
        "--output",
        "-o",
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (JSON)",
    ),
    show_cost: bool = typer.Option(
        True,
        "--cost/--no-cost",
        help="ë¹„ìš© ë¹„êµ í‘œì‹œ",
    ),
    show_time: bool = typer.Option(
        True,
        "--time/--no-time",
        help="ì‘ë‹µ ì‹œê°„ ë¹„êµ í‘œì‹œ",
    ),
    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸ ì •ë³´ í‘œì‹œ"),
    enable_cache: bool = typer.Option(
        False,
        "--enable-cache",
        help="API ì‘ë‹µ ìºì‹œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤",
    ),
):
    """ì—¬ëŸ¬ LLM ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•©ë‹ˆë‹¤."""

    # queryê°€ ì—†ìœ¼ë©´ help ì¶œë ¥
    if not query:
        console.print(ctx.get_help())
        raise typer.Exit()

    if len(models) < 2:
        console.print("[red]ì˜¤ë¥˜: ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.[/red]")
        console.print('[dim]ì˜ˆ: pyhub.llm compare "ì§ˆë¬¸" -m gpt-4o-mini -m claude-3-5-haiku-latest[/dim]')
        raise typer.Exit(1)

    # ë¡œê¹… ë ˆë²¨ ì„¤ì • (ì¶”í›„ ì‚¬ìš©ì„ ìœ„í•´ ë³´ê´€)
    # if is_verbose:
    #     log_level = logging.DEBUG
    # else:
    #     log_level = logging.INFO
    # init(debug=True, log_level=log_level)

    # ë¹„êµ ì‹œì‘
    console.print("\n[bold blue]ğŸ” ëª¨ë¸ ë¹„êµ[/bold blue]")
    console.print(f"[dim]ì§ˆë¬¸: {query}[/dim]")
    console.print(f"[dim]ëª¨ë¸: {', '.join([m.value for m in models])}[/dim]\n")

    results = []

    # ê° ëª¨ë¸ë¡œ ì§ˆì˜
    import time
    from concurrent.futures import ThreadPoolExecutor, as_completed

    def query_model(model_enum):
        """ë‹¨ì¼ ëª¨ë¸ì— ì§ˆì˜"""
        try:
            # Create cache if requested
            cache = None
            if enable_cache:
                from pyhub.llm.cache import MemoryCache

                cache = MemoryCache()

            llm = LLM.create(
                model=model_enum,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                cache=cache,
            )

            start_time = time.time()
            response_text = ""
            usage = None

            # ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ë°›ê¸° (ë¹„êµë¥¼ ìœ„í•´)
            response = llm.ask(query, stream=False)
            response_text = response.text
            usage = response.usage if hasattr(response, "usage") else None

            elapsed_time = time.time() - start_time

            # ë¹„ìš© ê³„ì‚°
            cost_info = None
            if show_cost and usage:
                try:
                    from pyhub.llm.utils.pricing import calculate_cost

                    cost_info = calculate_cost(model_enum.value, usage.input, usage.output)
                except Exception:
                    pass

            return {
                "model": model_enum.value,
                "response": response_text,
                "time": elapsed_time,
                "usage": (
                    {
                        "input": usage.input if usage else 0,
                        "output": usage.output if usage else 0,
                        "total": usage.total if usage else 0,
                    }
                    if usage
                    else None
                ),
                "cost": cost_info,
                "success": True,
                "error": None,
            }
        except Exception as e:
            return {
                "model": model_enum.value,
                "response": None,
                "time": 0,
                "usage": None,
                "cost": None,
                "success": False,
                "error": str(e),
            }

    # ë³‘ë ¬ ì²˜ë¦¬
    with console.status("[bold green]ëª¨ë¸ë“¤ì—ê²Œ ì§ˆì˜ ì¤‘...") as status:
        with ThreadPoolExecutor(max_workers=len(models)) as executor:
            future_to_model = {executor.submit(query_model, model): model for model in models}

            for future in as_completed(future_to_model):
                result = future.result()
                results.append(result)
                if result["success"]:
                    status.update(f"[bold green]âœ“ {result['model']} ì™„ë£Œ[/bold green]")
                else:
                    status.update(f"[bold red]âœ— {result['model']} ì‹¤íŒ¨[/bold red]")

    # ëª¨ë¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results = sorted(results, key=lambda x: [m.value for m in models].index(x["model"]))

    # ê²°ê³¼ ì¶œë ¥
    if output_format == "side-by-side":
        # ë‚˜ë€íˆ í‘œì‹œ
        panels = []
        for result in results:
            if result["success"]:
                content = result["response"]
                if show_time:
                    content += f"\n\n[dim]ì‘ë‹µ ì‹œê°„: {result['time']:.2f}ì´ˆ[/dim]"
                if show_cost and result["cost"]:
                    content += f"\n[dim]ë¹„ìš©: ${result['cost']['total_cost']:.6f}[/dim]"
            else:
                content = f"[red]ì˜¤ë¥˜: {result['error']}[/red]"

            panels.append(Panel(content, title=result["model"], expand=True))

        console.print(Columns(panels, equal=True))

    elif output_format == "sequential":
        # ìˆœì°¨ì  í‘œì‹œ
        for result in results:
            console.print(f"\n[bold blue]â”â”â” {result['model']} â”â”â”[/bold blue]")
            if result["success"]:
                console.print(result["response"])
                if show_time:
                    console.print(f"\n[dim]ì‘ë‹µ ì‹œê°„: {result['time']:.2f}ì´ˆ[/dim]")
                if show_cost and result["cost"]:
                    console.print(f"[dim]ë¹„ìš©: ${result['cost']['total_cost']:.6f}[/dim]")
            else:
                console.print(f"[red]ì˜¤ë¥˜: {result['error']}[/red]")

    elif output_format == "table":
        # í…Œì´ë¸” í˜•ì‹
        table = Table(title="ëª¨ë¸ ë¹„êµ ê²°ê³¼")
        table.add_column("ëª¨ë¸", style="cyan")
        table.add_column("ì‘ë‹µ", style="green", overflow="fold")
        if show_time:
            table.add_column("ì‹œê°„(ì´ˆ)", style="yellow")
        if show_cost:
            table.add_column("ë¹„ìš©($)", style="magenta")
        table.add_column("í† í°", style="blue")

        for result in results:
            if result["success"]:
                response = result["response"][:100] + "..." if len(result["response"]) > 100 else result["response"]
                row = [result["model"], response]
                if show_time:
                    row.append(f"{result['time']:.2f}")
                if show_cost:
                    row.append(f"{result['cost']['total_cost']:.6f}" if result["cost"] else "N/A")
                row.append(f"{result['usage']['total']}" if result["usage"] else "N/A")
            else:
                row = [result["model"], f"[red]ì˜¤ë¥˜: {result['error']}[/red]"]
                if show_time:
                    row.append("N/A")
                if show_cost:
                    row.append("N/A")
                row.append("N/A")

            table.add_row(*row)

        console.print(table)

    # í†µê³„ ìš”ì•½
    if show_time or show_cost:
        console.print("\n[bold]í†µê³„ ìš”ì•½[/bold]")

        if show_time:
            times = [r["time"] for r in results if r["success"]]
            if times:
                fastest = min(results, key=lambda x: x["time"] if x["success"] else float("inf"))
                console.print(f"ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: [green]{fastest['model']}[/green] ({fastest['time']:.2f}ì´ˆ)")

        if show_cost:
            costs = [(r["model"], r["cost"]["total_cost"]) for r in results if r["success"] and r["cost"]]
            if costs:
                cheapest = min(costs, key=lambda x: x[1])
                console.print(f"ê°€ì¥ ì €ë ´í•œ ëª¨ë¸: [green]{cheapest[0]}[/green] (${cheapest[1]:.6f})")

    # ê²°ê³¼ ì €ì¥
    if output_path:
        try:
            import json

            with output_path.open("w", encoding="utf-8") as f:
                json.dump(
                    {
                        "query": query,
                        "system_prompt": system_prompt,
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                        "results": results,
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
            console.print(f"\n[green]ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
        except Exception as e:
            console.print(f"\n[red]ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}[/red]")

from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.markdown import Markdown
from rich.prompt import Prompt
from rich.table import Table

# from pyhub import init
from pyhub.llm import LLM
from pyhub.llm.types import LLMChatModelEnum

console = Console()


def chat(
    model: LLMChatModelEnum = typer.Option(
        LLMChatModelEnum.GPT_4O_MINI,
        "--model",
        "-m",
        help="LLM Chat ëª¨ë¸",
    ),
    system_prompt: str = typer.Option(
        None,
        "--system-prompt",
        "-s",
        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
    ),
    temperature: float = typer.Option(
        0.7,
        "--temperature",
        "-t",
        help="ì‘ë‹µ ì˜¨ë„ (0.0-2.0)",
    ),
    max_tokens: int = typer.Option(
        2000,
        "--max-tokens",
        help="ìµœëŒ€ í† í° ìˆ˜",
    ),
    history_path: Optional[Path] = typer.Option(
        None,
        "--history",
        help="ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ê²½ë¡œ",
    ),
    show_cost: bool = typer.Option(
        False,
        "--cost",
        help="ê° ì‘ë‹µì˜ ë¹„ìš© í‘œì‹œ",
    ),
    markdown_mode: bool = typer.Option(
        True,
        "--markdown/--no-markdown",
        help="ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ ì‚¬ìš©",
    ),
    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸ ì •ë³´ í‘œì‹œ"),
    enable_cache: bool = typer.Option(
        False,
        "--enable-cache",
        help="API ì‘ë‹µ ìºì‹œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤",
    ),
):
    """ëŒ€í™”í˜• LLM ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤."""

    # ë¡œê¹… ë ˆë²¨ ì„¤ì • (ì¶”í›„ ì‚¬ìš©ì„ ìœ„í•´ ë³´ê´€)
    # if is_verbose:
    #     log_level = logging.DEBUG
    # else:
    #     log_level = logging.INFO
    # init(debug=True, log_level=log_level)

    # ì„¸ì…˜ ì •ë³´ í‘œì‹œ
    console.print("\n[bold blue]ğŸ’¬ LLM Chat Session[/bold blue]")
    console.print(f"[dim]ëª¨ë¸: {model.value} | ì˜¨ë„: {temperature} | ìµœëŒ€ í† í°: {max_tokens}[/dim]")
    if system_prompt:
        console.print(
            f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt[:50]}...[/dim]"
            if len(system_prompt) > 50
            else f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}[/dim]"
        )
    console.print("[dim]ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', Ctrl+C ë˜ëŠ” Ctrl+Dë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
    console.print("[dim]ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´ 'clear'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
    console.print("[dim]í˜„ì¬ ì„¤ì •ì„ ë³´ë ¤ë©´ 'settings'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
    console.print()

    # ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥
    base_system_prompt = system_prompt or ""

    # ëŒ€í™” íˆìŠ¤í† ë¦¬
    messages = []
    if history_path and history_path.exists():
        try:
            import json

            with history_path.open("r", encoding="utf-8") as f:
                messages = json.load(f)
                console.print(f"[green]íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(messages)} ë©”ì‹œì§€[/green]\n")
        except Exception as e:
            console.print(f"[yellow]íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}[/yellow]\n")

    # ì´ ì‚¬ìš©ëŸ‰ ì¶”ì 
    from pyhub.llm.types import Usage

    total_usage = Usage()
    turn_count = 0

    # ëŒ€í™” ë£¨í”„
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            try:
                user_input = Prompt.ask("\n[bold cyan]You[/bold cyan]")
            except EOFError:
                # Ctrl-D ì²˜ë¦¬
                console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...[/yellow]")
                break

            # íŠ¹ìˆ˜ ëª…ë ¹ ì²˜ë¦¬
            if user_input.lower() in ["exit", "quit", "bye"]:
                break
            elif user_input.lower() == "clear":
                messages = []
                total_usage = Usage()
                turn_count = 0
                console.print("[yellow]ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.[/yellow]")
                continue
            elif user_input.lower() == "settings":
                settings_table = Table(title="í˜„ì¬ ì„¤ì •")
                settings_table.add_column("í•­ëª©", style="cyan")
                settings_table.add_column("ê°’", style="green")
                settings_table.add_row("ëª¨ë¸", model.value)
                settings_table.add_row("ì˜¨ë„", str(temperature))
                settings_table.add_row("ìµœëŒ€ í† í°", str(max_tokens))
                settings_table.add_row("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", system_prompt or "(ì—†ìŒ)")
                settings_table.add_row("ëŒ€í™” í„´", str(turn_count))
                settings_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
                settings_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))
                console.print(settings_table)
                continue

            # AI ì‘ë‹µ
            console.print("\n[bold green]AI[/bold green]: ", end="")

            response_text = ""
            usage = None

            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            conversation_context = ""
            if messages:
                conversation_context = "\n\nëŒ€í™” íˆìŠ¤í† ë¦¬:\n"
                for msg in messages[-10:]:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
                    role = "Human" if msg["role"] == "user" else "AI"
                    conversation_context += f"{role}: {msg['content']}\n"

            # LLM ì¬ìƒì„± with ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
            current_system_prompt = base_system_prompt
            if conversation_context:
                current_system_prompt = base_system_prompt + conversation_context

            # Create cache if requested
            cache = None
            if enable_cache:
                from pyhub.llm.cache import MemoryCache

                cache = MemoryCache()

            llm = LLM.create(
                model=model,
                system_prompt=current_system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                cache=cache,
            )

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            for chunk in llm.ask(user_input, stream=True):
                if markdown_mode and "\n" in chunk.text:
                    # ë§ˆí¬ë‹¤ìš´ ëª¨ë“œì—ì„œëŠ” ì „ì²´ ì‘ë‹µì„ ëª¨ì•„ì„œ ë Œë”ë§
                    response_text += chunk.text
                else:
                    console.print(chunk.text, end="")
                    response_text += chunk.text

                if hasattr(chunk, "usage") and chunk.usage:
                    usage = chunk.usage

            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
            if markdown_mode and response_text:
                console.print()  # ì¤„ë°”ê¿ˆ
                console.print(Markdown(response_text))
            else:
                console.print()  # ì¤„ë°”ê¿ˆ

            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            messages.append({"role": "user", "content": user_input})
            messages.append({"role": "assistant", "content": response_text})
            turn_count += 1

            # ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
            if usage:
                total_usage += usage

                # ë¹„ìš© í‘œì‹œ
                if show_cost:
                    try:
                        from pyhub.llm.utils.pricing import calculate_cost

                        cost = calculate_cost(model.value, usage.input, usage.output)
                        console.print(
                            f"[dim]í† í°: ì…ë ¥ {usage.input}, ì¶œë ¥ {usage.output} | "
                            f"ë¹„ìš©: ${cost['total_cost']:.6f} (â‚©{cost['total_cost'] * 1300:.0f})[/dim]"
                        )
                    except Exception:
                        pass

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            if history_path:
                try:
                    import json

                    history_path.parent.mkdir(parents=True, exist_ok=True)
                    with history_path.open("w", encoding="utf-8") as f:
                        json.dump(messages, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    if is_verbose:
                        console.print(f"[yellow]íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}[/yellow]")

        except KeyboardInterrupt:
            console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...[/yellow]")
            break
        except Exception as e:
            console.print(f"\n[red]ì˜¤ë¥˜: {e}[/red]")
            if is_verbose:
                console.print_exception()

    # ì„¸ì…˜ ì¢…ë£Œ
    console.print("\n[bold blue]ì„¸ì…˜ ì¢…ë£Œ[/bold blue]")
    if turn_count > 0:
        stats_table = Table(title="ì„¸ì…˜ í†µê³„")
        stats_table.add_column("í•­ëª©", style="cyan")
        stats_table.add_column("ê°’", style="green")
        stats_table.add_row("ëŒ€í™” í„´", str(turn_count))
        stats_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
        stats_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))

        if show_cost:
            try:
                from pyhub.llm.utils.pricing import calculate_cost

                total_cost = calculate_cost(model.value, total_usage.input, total_usage.output)
                stats_table.add_row("ì´ ë¹„ìš©", f"${total_cost['total_cost']:.4f}")
                stats_table.add_row("ì›í™” í™˜ì‚°", f"â‚©{total_cost['total_cost'] * 1300:.0f}")
            except Exception:
                pass

        console.print(stats_table)

    console.print("\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")

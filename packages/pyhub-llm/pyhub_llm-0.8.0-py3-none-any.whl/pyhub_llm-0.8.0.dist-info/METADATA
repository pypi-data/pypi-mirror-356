Metadata-Version: 2.4
Name: pyhub-llm
Version: 0.8.0
Summary: Standalone LLM library with support for multiple providers
Project-URL: Homepage, https://github.com/pyhub-kr/pyhub-llm
Project-URL: Documentation, https://github.com/pyhub-kr/pyhub-llm#readme
Project-URL: Repository, https://github.com/pyhub-kr/pyhub-llm
Project-URL: Issues, https://github.com/pyhub-kr/pyhub-llm/issues
Author-email: PyHub Team <me@pyhub.kr>
License: MIT
License-File: LICENSE
Keywords: agent,ai,anthropic,google,llm,mcp,ollama,openai,react
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.10
Requires-Dist: aiofiles>=23.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: jinja2>=3.1.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: toml>=0.10.0
Requires-Dist: typer>=0.9.0
Provides-Extra: all
Requires-Dist: aiofiles>=23.0.0; extra == 'all'
Requires-Dist: anthropic>=0.52.0; extra == 'all'
Requires-Dist: fastmcp; extra == 'all'
Requires-Dist: google-genai>=1.19.0; extra == 'all'
Requires-Dist: mcp; extra == 'all'
Requires-Dist: ollama>=0.5.0; extra == 'all'
Requires-Dist: openai>=1.84.0; extra == 'all'
Requires-Dist: pillow>=10.0.0; extra == 'all'
Requires-Dist: pymupdf; extra == 'all'
Requires-Dist: pymupdf>=1.23.0; extra == 'all'
Requires-Dist: pyyaml>=6.0.0; extra == 'all'
Requires-Dist: uvicorn; extra == 'all'
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.52.0; extra == 'anthropic'
Provides-Extra: build
Requires-Dist: build; extra == 'build'
Requires-Dist: setuptools; extra == 'build'
Requires-Dist: twine; extra == 'build'
Requires-Dist: wheel; extra == 'build'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: docs
Requires-Dist: mkdocs-autorefs>=0.5.0; extra == 'docs'
Requires-Dist: mkdocs-git-revision-date-localized-plugin>=1.2.0; extra == 'docs'
Requires-Dist: mkdocs-glightbox; extra == 'docs'
Requires-Dist: mkdocs-literate-nav>=0.6.0; extra == 'docs'
Requires-Dist: mkdocs-material>=9.5.0; extra == 'docs'
Requires-Dist: mkdocs-minify-plugin>=0.7.0; extra == 'docs'
Requires-Dist: mkdocs-section-index>=0.3.0; extra == 'docs'
Requires-Dist: mkdocs>=1.5.0; extra == 'docs'
Requires-Dist: mkdocstrings[python]>=0.24.0; extra == 'docs'
Requires-Dist: pymdown-extensions>=10.0; extra == 'docs'
Provides-Extra: google
Requires-Dist: google-genai>=1.19.0; extra == 'google'
Provides-Extra: image
Requires-Dist: aiofiles>=23.0.0; extra == 'image'
Requires-Dist: pillow>=10.0.0; extra == 'image'
Provides-Extra: mcp
Requires-Dist: fastmcp; extra == 'mcp'
Requires-Dist: mcp; extra == 'mcp'
Requires-Dist: pyyaml>=6.0.0; extra == 'mcp'
Requires-Dist: uvicorn; extra == 'mcp'
Provides-Extra: ollama
Requires-Dist: ollama>=0.5.0; extra == 'ollama'
Requires-Dist: pymupdf; extra == 'ollama'
Provides-Extra: openai
Requires-Dist: openai>=1.84.0; extra == 'openai'
Provides-Extra: pdf
Requires-Dist: pymupdf>=1.23.0; extra == 'pdf'
Provides-Extra: upstage
Requires-Dist: openai>=1.84.0; extra == 'upstage'
Description-Content-Type: text/markdown

# pyhub-llm

ë‹¤ì–‘í•œ LLM ì œê³µì—…ì²´ë¥¼ ìœ„í•œ í†µí•© Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. OpenAI, Anthropic, Google, Ollama ë“±ì˜ APIë¥¼ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“š ë¬¸ì„œ

### [âœ¨ CHEATSHEET](./CHEATSHEET.md) - ìˆ˜ì¤€ë³„ ì™„ì „í•œ ê°€ì´ë“œ

pyhub-llmì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ìˆ˜ì¤€ë³„ë¡œ ë‚˜ëˆ„ì–´ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤:
- ğŸŒ± [ì´ˆê¸‰ ê°€ì´ë“œ](./CHEATSHEET-BASIC.md): ì„¤ì¹˜, ê¸°ë³¸ ì‚¬ìš©ë²•, íŒŒì¼ ì²˜ë¦¬
- ğŸš€ [ì¤‘ê¸‰ ê°€ì´ë“œ](./CHEATSHEET-INTERMEDIATE.md): êµ¬ì¡°í™”ëœ ì¶œë ¥, ìºì‹±, ë„êµ¬ í˜¸ì¶œ, History Backup
- ğŸ”¥ [ê³ ê¸‰ ê°€ì´ë“œ](./CHEATSHEET-ADVANCED.md): MCP í†µí•©, ì²´ì´ë‹, ì›¹ í”„ë ˆì„ì›Œí¬ í†µí•©
- ğŸ“– [ê¸°ëŠ¥ë³„ ì°¾ì•„ë³´ê¸°](./CHEATSHEET.md#ê¸°ëŠ¥ë³„-ì°¾ì•„ë³´ê¸°): ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ë¹ ë¥´ê²Œ ê²€ìƒ‰

## ì£¼ìš” ê¸°ëŠ¥

- ğŸ”Œ **í†µí•© ì¸í„°í˜ì´ìŠ¤**: ëª¨ë“  LLM ì œê³µì—…ì²´ë¥¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©
- ğŸš€ **ê°„í¸í•œ ì „í™˜**: ì½”ë“œ ë³€ê²½ ì—†ì´ ëª¨ë¸ ì „í™˜ ê°€ëŠ¥
- ğŸ’¾ **ìºì‹± ì§€ì›**: ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ í–¥ìƒ
- ğŸ”„ **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
- ğŸ› ï¸ **ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ**: Function calling ì§€ì›
- ğŸ“· **ì´ë¯¸ì§€ ì²˜ë¦¬**: ì´ë¯¸ì§€ ì„¤ëª… ë° ë¶„ì„ ê¸°ëŠ¥
- âš¡ **ë¹„ë™ê¸° ì§€ì›**: ë™ê¸°/ë¹„ë™ê¸° ëª¨ë‘ ì§€ì›
- ğŸ”— **ì²´ì´ë‹**: ì—¬ëŸ¬ LLMì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
- ğŸ’¾ **ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°±ì—…**: ëŒ€í™” ë‚´ì—­ì„ ì™¸ë¶€ ì €ì¥ì†Œì— ë°±ì—… ë° ë³µì›

## ì„¤ì¹˜

### ì „ì²´ ì„¤ì¹˜

```bash
pip install 'pyhub-llm[all]'
```

### íŠ¹ì • ì œê³µì—…ì²´ë§Œ ì„¤ì¹˜

```bash
# OpenAIë§Œ
pip install "pyhub-llm[openai]"

# Anthropicë§Œ
pip install "pyhub-llm[anthropic]"

# Googleë§Œ (google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
pip install "pyhub-llm[google]"

# Ollamaë§Œ
pip install "pyhub-llm[ollama]"

# ëª¨ë“  ì œê³µì—…ì²´
pip install "pyhub-llm[all]"
```

### ê°œë°œ í™˜ê²½ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/pyhub-kr/pyhub-llm.git
cd pyhub-llm

# ê°œë°œ í™˜ê²½ ì„¤ì¹˜
pip install -e ".[dev,all]"
# í˜¹ì€ make install
```

## ë¹ ë¥¸ ì‹œì‘

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

#### Linux/macOS (Bash)
```bash
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export GOOGLE_API_KEY="your-google-api-key"
export UPSTAGE_API_KEY="your-upstage-api-key"
```

#### Windows (PowerShell)
```powershell
$env:OPENAI_API_KEY="your-openai-api-key"
$env:ANTHROPIC_API_KEY="your-anthropic-api-key"
$env:GOOGLE_API_KEY="your-google-api-key"
$env:UPSTAGE_API_KEY="your-upstage-api-key"
```

> **ì°¸ê³ **: 
> + API í‚¤ëŠ” ê° í”„ë¡œë°”ì´ë”ì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (API í‚¤ ì„¤ì • ì„¹ì…˜ ì°¸ì¡°)
> + OllamaëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ API í‚¤ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤
> + OllamaëŠ” ë””í´íŠ¸ë¡œ `http://localhost:11434` ì£¼ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `UPSTAGE_BASE_URL` í™˜ê²½ë³€ìˆ˜ë‚˜ `OllamaLLM(base_url="...")` ì¸ìë¥¼ í†µí•´ ë³€ê²½í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ëª¨ë¸ë³„ ì§ì ‘ ì‚¬ìš©

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# OpenAI ì‚¬ìš©ì‹œ
pip install "pyhub-llm[openai]"

# Anthropic ì‚¬ìš©ì‹œ
pip install "pyhub-llm[anthropic]"

# Google ì‚¬ìš©ì‹œ
pip install "pyhub-llm[google]"

# Ollama ì‚¬ìš©ì‹œ (ë¡œì»¬ ì‹¤í–‰)
pip install "pyhub-llm[ollama]"

# ì´ë¯¸ì§€ ê¸°ëŠ¥ ì‚¬ìš©ì‹œ (Pillow í¬í•¨)
pip install "pyhub-llm[image]"

# ëª¨ë“  ê¸°ëŠ¥ ì„¤ì¹˜
pip install "pyhub-llm[all]"
```

```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM, OllamaLLM

# OpenAI (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
openai_llm = OpenAILLM(model="gpt-4o-mini")
reply = openai_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# API í‚¤ ì§ì ‘ ì „ë‹¬
openai_llm = OpenAILLM(model="gpt-4o-mini", api_key="your-api-key")

# Anthropic (ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
claude_llm = AnthropicLLM(model="claude-3-5-haiku-latest")
reply = claude_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# Google (GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
gemini_llm = GoogleLLM(model="gemini-1.5-flash")
reply = gemini_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# Ollama (ë¡œì»¬ ì‹¤í–‰, API í‚¤ ë¶ˆí•„ìš”, ê¸°ë³¸ URL: http://localhost:11434)
ollama_llm = OllamaLLM(model="mistral")
reply = ollama_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from pyhub.llm import LLM

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = LLM.create("gpt-4o-mini")

# ì§ˆë¬¸í•˜ê¸°
reply = llm.ask("Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(reply.text)
```

> ë” ë§ì€ ì˜ˆì‹œì™€ ê³ ê¸‰ ì‚¬ìš©ë²•ì€ [ì´ˆê¸‰ ê°€ì´ë“œ](./CHEATSHEET-BASIC.md#ê¸°ë³¸-ì‚¬ìš©ë²•)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## Ollama ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©

OllamaëŠ” ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤. API í‚¤ê°€ í•„ìš” ì—†ê³ , ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•Šì•„ ê°œì¸ì •ë³´ ë³´í˜¸ì— ìœ ë¦¬í•©ë‹ˆë‹¤.

### Ollama ì„¤ì¹˜

#### macOS
```bash
# Homebrew ì‚¬ìš©
brew install ollama

# ë˜ëŠ” ê³µì‹ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Linux
```bash
# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
curl -fsSL https://ollama.ai/install.sh | sh

# ë˜ëŠ” Docker ì‚¬ìš©
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### Windows
```bash
# PowerShellì—ì„œ ì‹¤í–‰
iex (irm https://ollama.ai/install.ps1)

# ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
# https://ollama.ai/download/windows
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰

```bash
# Ollama ì„œë¹„ìŠ¤ ì‹œì‘ (í•„ìš”í•œ ê²½ìš°)
ollama serve

# Mistral ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull mistral

# ë‹¤ë¥¸ ì¸ê¸° ëª¨ë¸ë“¤
ollama pull llama3.3
ollama pull gemma2
ollama pull qwen2

# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ì§ì ‘ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
ollama run mistral
```

### pyhub-llmì—ì„œ Ollama ì‚¬ìš©

```python
from pyhub.llm import OllamaLLM

# ê¸°ë³¸ ì‚¬ìš©ë²•
llm = OllamaLLM(model="mistral")
reply = llm.ask("Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)

# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì§ˆë¬¸í•˜ê¸°
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    files=["image.jpg"]
)

# PDF íŒŒì¼ ì²˜ë¦¬ (ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë¨)
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["document.pdf"]  # ìë™ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜
)

# ë¹„ë™ê¸° ì‚¬ìš©
async def async_example():
    reply = await llm.ask_async("ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸í•©ë‹ˆë‹¤")
    return reply.text

# ì»¤ìŠ¤í…€ ì„¤ì •
llm = OllamaLLM(
    model="mistral",
    temperature=0.7,
    max_tokens=2000,
    base_url="http://localhost:11434"  # ì»¤ìŠ¤í…€ Ollama ì„œë²„
)
```

### Ollama ì¥ì 

- **ğŸ”’ ê°œì¸ì •ë³´ ë³´í˜¸**: ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬
- **ğŸ’° ë¹„ìš© ì ˆê°**: API í˜¸ì¶œ ë¹„ìš© ì—†ìŒ
- **âš¡ ë¹ ë¥¸ ì‘ë‹µ**: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì—†ìŒ  
- **ğŸŒ ì˜¤í”„ë¼ì¸ ì‚¬ìš©**: ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”
- **ğŸ›ï¸ ì™„ì „í•œ ì œì–´**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ììœ  ì¡°ì •

### ì§€ì› ëª¨ë¸

- **Llama ê³„ì—´**: llama3.3, llama3.1, llama3.2
- **Mistral**: mistral, mixtral
- **Gemma**: gemma2, gemma3  
- **Qwen**: qwen2, qwen2.5
- **ê¸°íƒ€**: phi3, codellama, vicuna ë“±

> **ì°¸ê³ **: PDF íŒŒì¼ ì²˜ë¦¬ ì‹œ OllamaëŠ” ìë™ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë³´ì¡´ì„ ìœ„í•´ 600 DPIë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥ ì˜ˆì œ

### 1. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
```

### 2. ì¶œë ¥ í¬ë§·íŒ… (NEW! ğŸ¨)

```python
from pyhub.llm import LLM, display

# ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ê³¼ í•¨ê»˜ ìŠ¤íŠ¸ë¦¬ë°
response = llm.ask("íŒŒì´ì¬ í•¨ìˆ˜ ì‘ì„±ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”", stream=True)
display(response)  # ìë™ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§!

# ë˜ëŠ” Response ê°ì²´ì˜ print() ë©”ì„œë“œ ì‚¬ìš©
response = llm.ask("ì•ˆë…•í•˜ì„¸ìš”")
response.print(markdown=True)

# ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
response.print(markdown=False)
```

### 3. ì´ë¯¸ì§€ ìƒì„± (NEW! ğŸ¨)

```python
from pyhub.llm import OpenAILLM

# DALL-E 3ë¡œ ì´ë¯¸ì§€ ìƒì„±
llm = OpenAILLM(model="dall-e-3")
reply = llm.generate_image(
    "A beautiful sunset over mountains",
    size="1024x1792",  # ì„¸ë¡œ í˜•ì‹
    quality="hd"       # ê³ í’ˆì§ˆ
)

# ì´ë¯¸ì§€ ì €ì¥
path = reply.save("sunset.png")  # ë˜ëŠ” reply.save() ë¡œ ìë™ íŒŒì¼ëª…
print(f"Saved to: {path}")

# ì´ë¯¸ì§€ í‘œì‹œ (Jupyter)
reply.display()

# PILë¡œ ë³€í™˜ (Pillow í•„ìš”)
img = reply.to_pil()
img.thumbnail((512, 512))
img.save("thumbnail.png")

# ë¹„ë™ê¸° ì²˜ë¦¬
import asyncio

async def generate_multiple():
    tasks = [
        llm.generate_image_async(f"Image {i}") 
        for i in range(3)
    ]
    images = await asyncio.gather(*tasks)
    
    # ë³‘ë ¬ ì €ì¥
    save_tasks = [img.save_async(f"img_{i}.png") for i, img in enumerate(images)]
    await asyncio.gather(*save_tasks)
```

### 4. íŒŒì¼ ì²˜ë¦¬ (ì´ë¯¸ì§€ ë° PDF)

```python
# ì´ë¯¸ì§€ ì„¤ëª…
reply = llm.ask("ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”", files=["photo.jpg"])

# PDF ìš”ì•½
reply = llm.ask("ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”", files=["document.pdf"])
```

### 5. ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ

```python
# ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©
def get_weather(city: str) -> str:
    """ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒì…ë‹ˆë‹¤."

reply = llm.ask("ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜", tools=[get_weather])
```

> ğŸ“– ë” ë§ì€ ê³ ê¸‰ ê¸°ëŠ¥ê³¼ ìƒì„¸í•œ ì˜ˆì‹œëŠ” [ê°€ì´ë“œ ë¬¸ì„œ](./CHEATSHEET.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:
> - ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
> - êµ¬ì¡°í™”ëœ ì¶œë ¥ (Pydantic)
> - LLM ì²´ì´ë‹
> - ìºì‹± ì „ëµ
> - MCP í†µí•©
> - ì›¹ í”„ë ˆì„ì›Œí¬ í†µí•© (FastAPI, Django)
> - ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„
> - ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°±ì—… ë° ë³µì›

### 5. ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°±ì—…

```python
from pyhub.llm import LLM
from pyhub.llm.history import InMemoryHistoryBackup

# ë°±ì—… ì €ì¥ì†Œ ìƒì„±
backup = InMemoryHistoryBackup(user_id="user123", session_id="session456")

# ë°±ì—…ì´ í™œì„±í™”ëœ LLM ìƒì„±
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ëŒ€í™” ì§„í–‰ (ìë™ìœ¼ë¡œ ë°±ì—…ë¨)
llm.ask("Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
llm.ask("ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")

# ì‚¬ìš©ëŸ‰ í™•ì¸
usage = backup.get_usage_summary()
print(f"ì´ ì‚¬ìš© í† í°: {usage.input + usage.output}")

# SQLAlchemyë¡œ ì˜êµ¬ ì €ì¥
from pyhub.llm.history import SQLAlchemyHistoryBackup
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

engine = create_engine("sqlite:///chat_history.db")
Session = sessionmaker(bind=engine)
session = Session()

db_backup = SQLAlchemyHistoryBackup(session, user_id="user123", session_id="session456")
llm_with_db = LLM.create("gpt-4o-mini", history_backup=db_backup)
```

> ğŸ” ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°±ì—…ì€ ë„êµ¬(Tool) ì‚¬ìš© ë‚´ì—­ë„ ìë™ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ìì„¸í•œ ì‚¬ìš©ë²•ì€ [ì¤‘ê¸‰ ê°€ì´ë“œ](./CHEATSHEET-INTERMEDIATE.md#history-backup)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## API í‚¤ ì„¤ì •

### í•„ìš”í•œ API í‚¤

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- **OpenAI**: `OPENAI_API_KEY` - [API í‚¤ ë°œê¸‰](https://platform.openai.com/api-keys)
- **Anthropic**: `ANTHROPIC_API_KEY` - [API í‚¤ ë°œê¸‰](https://console.anthropic.com/settings/keys)
- **Google**: `GOOGLE_API_KEY` - [API í‚¤ ë°œê¸‰](https://makersuite.google.com/app/apikey)
- **Upstage**: `UPSTAGE_API_KEY` - [API í‚¤ ë°œê¸‰](https://console.upstage.ai/)

### ì„¤ì • ë°©ë²•

#### 1. í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"
```

#### 2. ì½”ë“œì—ì„œ ì§ì ‘ ì „ë‹¬
```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM

# API í‚¤ë¥¼ ì§ì ‘ ì „ë‹¬
llm = OpenAILLM(api_key="your-api-key")
llm = AnthropicLLM(api_key="your-api-key")
llm = GoogleLLM(api_key="your-api-key")
```

## CLI ì‚¬ìš©ë²•

```bash
# ëŒ€í™”í˜• ì±„íŒ…
pyhub-llm chat --model gpt-4o-mini

# ë‹¨ì¼ ì§ˆë¬¸
pyhub-llm ask "Pythonê³¼ Goì˜ ì°¨ì´ì ì€?"

# íŒŒì¼ê³¼ í•¨ê»˜ ì§ˆë¬¸
pyhub-llm ask "ì´ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”" --file main.py
```

> ğŸ”§ ë” ë§ì€ CLI ì˜µì…˜ê³¼ ì‚¬ìš©ë²•ì€ [ì´ˆê¸‰ ê°€ì´ë“œ](./CHEATSHEET-BASIC.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.


## ê³ ê¸‰ ê¸°ëŠ¥

### Stateless ëª¨ë“œ

ë°˜ë³µì ì¸ ë…ë¦½ ì‘ì—…(ë¶„ë¥˜, ì •ë³´ ì¶”ì¶œ ë“±)ì„ ìˆ˜í–‰í•  ë•Œ ë¶ˆí•„ìš”í•œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ëˆ„ì ì„ ë°©ì§€í•˜ëŠ” ëª¨ë“œì…ë‹ˆë‹¤.

```python
from pyhub.llm import LLM

# Stateless ëª¨ë“œë¡œ LLM ìƒì„±
classifier = LLM.create("gpt-4o-mini", stateless=True)

# ê° ìš”ì²­ì´ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©° íˆìŠ¤í† ë¦¬ê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ
for text in customer_queries:
    reply = classifier.ask(
        f"ê³ ê° ë¬¸ì˜ ì˜ë„ ë¶„ë¥˜: {text}",
        choices=["í™˜ë¶ˆ", "ë°°ì†¡", "ë¬¸ì˜", "ë¶ˆë§Œ"]
    )
    print(f"{text} -> {reply.choice}")
    # íˆìŠ¤í† ë¦¬ê°€ ëˆ„ì ë˜ì§€ ì•Šì•„ API ë¹„ìš© ì ˆê°
```

**Stateless ëª¨ë“œì˜ íŠ¹ì§•:**
- ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì „í˜€ ì €ì¥í•˜ì§€ ì•ŠìŒ
- `use_history=True`ë¥¼ ì§€ì •í•´ë„ ë¬´ì‹œë¨
- `clear()` ë©”ì„œë“œê°€ ì•„ë¬´ ë™ì‘ë„ í•˜ì§€ ì•ŠìŒ
- ë°˜ë³µ ì‘ì—…ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ API ë¹„ìš© ì ˆê°

**ì¼ë°˜ ëª¨ë“œì™€ì˜ ë¹„êµ:**
```python
# ì¼ë°˜ ëª¨ë“œ (íˆìŠ¤í† ë¦¬ ëˆ„ì )
normal_llm = LLM.create("gpt-4o-mini")
normal_llm.ask("ë¶„ë¥˜1", choices=["A", "B"])  # 2ê°œ ë©”ì‹œì§€ ì €ì¥
normal_llm.ask("ë¶„ë¥˜2", choices=["C", "D"])  # 4ê°œ ë©”ì‹œì§€ ì €ì¥ (ì´ì „ ëŒ€í™” í¬í•¨)

# Stateless ëª¨ë“œ (íˆìŠ¤í† ë¦¬ ì—†ìŒ)
stateless_llm = LLM.create("gpt-4o-mini", stateless=True)
stateless_llm.ask("ë¶„ë¥˜1", choices=["A", "B"])  # 0ê°œ ë©”ì‹œì§€
stateless_llm.ask("ë¶„ë¥˜2", choices=["C", "D"])  # 0ê°œ ë©”ì‹œì§€ (ë…ë¦½ ì²˜ë¦¬)
```


## ê°œë°œ

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸
make test

# íŠ¹ì • í…ŒìŠ¤íŠ¸
make test tests/test_openai.py

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ í…ŒìŠ¤íŠ¸
make test-cov
# ë˜ëŠ”
make cov

# ì»¤ë²„ë¦¬ì§€ HTML ë¦¬í¬íŠ¸ ë³´ê¸°
make test-cov-report

# íŠ¹ì • íŒŒì¼ë§Œ ì»¤ë²„ë¦¬ì§€ í…ŒìŠ¤íŠ¸
make cov tests/test_optional_dependencies.py

# pytest ì§ì ‘ ì‹¤í–‰
pytest --cov=src/pyhub/llm --cov-report=term --cov-report=html
```

### ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# í¬ë§·íŒ… ë° ë¦°íŒ…
make format
make lint

# íƒ€ì… ì²´í¬
mypy src/
```

### ë¹Œë“œ ë° ë°°í¬

```bash
# íŒ¨í‚¤ì§€ ë¹Œë“œ
make build

# PyPI ë°°í¬ (ê¶Œí•œ í•„ìš”)
make release
```

## ê¸°ì—¬í•˜ê¸°

1. ì´ ì €ì¥ì†Œë¥¼ í¬í¬í•©ë‹ˆë‹¤
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•©ë‹ˆë‹¤ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œí•©ë‹ˆë‹¤ (`git push origin feature/amazing-feature`)
5. Pull Requestë¥¼ ìƒì„±í•©ë‹ˆë‹¤

### ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

- ëª¨ë“  ìƒˆ ê¸°ëŠ¥ì—ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
- ì½”ë“œ ìŠ¤íƒ€ì¼ì€ Blackê³¼ Ruffë¥¼ ë”°ë¦…ë‹ˆë‹¤
- íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”
- ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**Q: API í‚¤ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤**

```python
# í•´ê²° ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
import os
os.environ["OPENAI_API_KEY"] = "your-key"

# í•´ê²° ë°©ë²• 2: ì§ì ‘ ì „ë‹¬
llm = OpenAILLM(api_key="your-key")
```

**Q: ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤**

```python
# ìºì‹œ ì¸ì ì…˜ìœ¼ë¡œ ìºì‹± í™œì„±í™”
from pyhub.llm.cache import MemoryCache
cache = MemoryCache()
llm = LLM.create("gpt-4o-mini", cache=cache)
reply = llm.ask("...")

# ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
llm = LLM.create("gpt-3.5-turbo")
```

**Q: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤**

```python
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ
llm = LLM.create(
    "gpt-4o-mini",
    initial_messages=[]  # íˆìŠ¤í† ë¦¬ ì—†ì´ ì‹œì‘
)

# ì£¼ê¸°ì ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ì •ë¦¬
if len(llm) > 10:
    llm.clear()
```

## ì•Œë ¤ì§„ ì´ìŠˆ

### OpenAI JSON Schema êµ¬ì¡°í™”ëœ ì¶œë ¥ì˜ í† í° ìƒì„± ì´ìŠˆ (v0.8.0ì—ì„œ ê°œì„ )

OpenAIì˜ JSON Schema êµ¬ì¡°í™”ëœ ì¶œë ¥ ëª¨ë“œ(strict=true)ì—ì„œ ê°„í—ì ìœ¼ë¡œ ë¬´í•œ ê°œí–‰ ë¬¸ìë‚˜ ì œì–´ ë¬¸ìê°€ ìƒì„±ë˜ëŠ” ë¬¸ì œê°€ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” GPT-4o-2024-08-06 ëª¨ë¸ì—ì„œ ë°œê²¬ëœ ì•Œë ¤ì§„ ë¬¸ì œì…ë‹ˆë‹¤.

**ì¦ìƒ:**
```python
# ë¬´í•œ ê°œí–‰ ë¬¸ì ìƒì„±
{"choice": "\n\n\n\n\n..."}  # max_tokensê¹Œì§€ ê³„ì†

# ì œì–´ ë¬¸ì í¬í•¨
{"choice": "\u001cA/S\u001d0\u001d\u001d..."}
```

**v0.8.0+ ê°œì„ ì‚¬í•­:**
- `strict: true` ì„¤ì •ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ ê°•í™”
- `choice_index` í•„ë“œë¥¼ í†µí•œ ì•ˆì •ì ì¸ ì„ íƒ
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„± (choices ì‚¬ìš© ì‹œ)

```python
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—†ì´ë„ ì‘ë™
llm = OpenAILLM()
reply = llm.ask("ë¶„ë¥˜í•´ì£¼ì„¸ìš”", choices=["í™˜ë¶ˆ/ë°˜í’ˆ", "A/Sìš”ì²­"])
print(reply.choice)  # "í™˜ë¶ˆ/ë°˜í’ˆ" ë˜ëŠ” "A/Sìš”ì²­"
print(reply.choice_index)  # 0 ë˜ëŠ” 1
```

**ì´ì „ ë²„ì „ í•´ê²° ë°©ë²•:**
1. íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ ëŒ€ì²´: `"A/Sìš”ì²­"` â†’ `"ASìš”ì²­"`
2. ë²„ì „ 0.7.1-0.7.xì—ì„œëŠ” ì œì–´ ë¬¸ìê°€ ë¶€ë¶„ì ìœ¼ë¡œ í•„í„°ë§ë©ë‹ˆë‹¤

ìì„¸í•œ ë‚´ìš©ì€ [Issue #22](https://github.com/pyhub-kr/pyhub-llm/issues/22)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ë§í¬

- [ë¬¸ì„œ](https://pyhub-llm.readthedocs.io)
- [PyPI](https://pypi.org/project/pyhub-llm)
- [GitHub](https://github.com/pyhub-kr/pyhub-llm)
- [ì´ìŠˆ íŠ¸ë˜ì»¤](https://github.com/pyhub-kr/pyhub-llm/issues)

import abc
import asyncio
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import (
    IO,
    TYPE_CHECKING,
    Any,
    AsyncGenerator,
    Generator,
    List,
    Optional,
    Type,
    Union,
    cast,
)

from pyhub.llm.cache.base import BaseCache
from pyhub.llm.settings import llm_settings

if TYPE_CHECKING:
    from pydantic import BaseModel

    from pyhub.llm.history.base import HistoryBackup
    from pyhub.llm.mcp.configs import McpConfig
    from pyhub.llm.mcp.policy import MCPConnectionPolicy
from pyhub.llm.types import (
    ChainReply,
    Embed,
    ImageReply,
    EmbedList,
    LLMChatModelType,
    LLMEmbeddingModelType,
    Message,
    Reply,
)
from pyhub.llm.utils.files import IOType
from pyhub.llm.utils.templates import (
    Context,
    Template,
    TemplateDoesNotExist,
    async_to_sync,
    get_template,
)

if TYPE_CHECKING:
    from pyhub.llm.history.base import HistoryBackup

logger = logging.getLogger(__name__)


class TemplateDict(dict):
    """í…œí”Œë¦¿ ë³€ìˆ˜ ì¤‘ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ëŠ” ì›ë˜ í˜•íƒœ({key})ë¡œ ìœ ì§€í•˜ëŠ” ë”•ì…”ë„ˆë¦¬"""

    def __missing__(self, key):
        return "{" + key + "}"


@dataclass
class DescribeImageRequest:
    image: Union[str, Path, IO]
    image_path: str
    system_prompt: Union[str, Template]
    user_prompt: Union[str, Template]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    prompt_context: Optional[dict[str, Any]] = None


class BaseLLM(abc.ABC):
    EMBEDDING_DIMENSIONS = {}
    SUPPORTED_FILE_TYPES = [IOType.IMAGE]  # ê¸°ë³¸ê°’: ì´ë¯¸ì§€ë§Œ ì§€ì›

    def __init__(
        self,
        model: LLMChatModelType = "gpt-4o-mini",
        embedding_model: LLMEmbeddingModelType = "text-embedding-3-small",
        temperature: float = 0.2,
        max_tokens: int = 1000,
        system_prompt: Optional[Union[str, Template]] = None,
        prompt: Optional[Union[str, Template]] = None,
        output_key: str = "text",
        initial_messages: Optional[list[Message]] = None,
        api_key: Optional[str] = None,
        tools: Optional[list] = None,
        cache: Optional[BaseCache] = None,
        mcp_servers: Optional[Union[str, dict, List[Union[dict, "McpConfig"]], "McpConfig"]] = None,
        mcp_policy: Optional["MCPConnectionPolicy"] = None,
        history_backup: Optional["HistoryBackup"] = None,
        stateless: bool = False,
    ):
        self.model = model
        self.embedding_model = embedding_model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt
        self.prompt = prompt
        self.output_key = output_key
        self.history_backup = history_backup
        self.stateless = stateless

        # ë°±ì—…ì´ ìˆìœ¼ë©´ íˆìŠ¤í† ë¦¬ ë³µì›, ì—†ìœ¼ë©´ initial_messages ì‚¬ìš©
        if self.history_backup:
            try:
                self.history = self.history_backup.load_history()
            except Exception as e:
                # ë³µì› ì‹¤íŒ¨ ì‹œ initial_messages ì‚¬ìš©
                logger.warning(f"Failed to load history from backup: {e}")
                self.history = initial_messages or []
        else:
            self.history = initial_messages or []
        self.api_key = api_key
        self.cache = cache

        # MCP ì„¤ì • ì²˜ë¦¬ (íŒŒì¼ ê²½ë¡œ, dict, list, McpConfig ì§€ì›)
        self.mcp_servers = self._process_mcp_servers(mcp_servers)
        self.mcp_policy = mcp_policy
        self._mcp_client = None
        self._mcp_connected = False
        self._mcp_tools = []

        # ê¸°ë³¸ ë„êµ¬ ì„¤ì •
        self.default_tools = []
        if tools:
            # tools ëª¨ë“ˆì„ ë™ì  import (ìˆœí™˜ import ë°©ì§€)
            from .tools import ToolAdapter

            self.default_tools = ToolAdapter.adapt_tools(tools)

        # Finalizer ë“±ë¡ (MCP ì‚¬ìš© ì‹œ)
        self._finalizer = None
        if self.mcp_servers:
            from .resource_manager import register_mcp_instance

            self._finalizer = register_mcp_instance(self)

    def _process_mcp_servers(self, mcp_servers) -> List["McpConfig"]:
        """MCP ì„œë²„ ì„¤ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not mcp_servers:
            return []

        # ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì¼ ê²½ë¡œë¡œ ì²˜ë¦¬
        if isinstance(mcp_servers, str):
            # ë™ì  import (ìˆœí™˜ import ë°©ì§€)
            from .mcp import load_mcp_config

            return load_mcp_config(mcp_servers)

        # dictì¸ ê²½ìš° (mcpServers í‚¤ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        elif isinstance(mcp_servers, dict):
            from .mcp import load_mcp_config

            return load_mcp_config(mcp_servers)

        # listì¸ ê²½ìš°
        elif isinstance(mcp_servers, list):
            # ë¦¬ìŠ¤íŠ¸ì˜ ê° ìš”ì†Œê°€ dictì¸ì§€ í™•ì¸
            if all(isinstance(item, dict) for item in mcp_servers):
                from .mcp import load_mcp_config

                return load_mcp_config(mcp_servers)
            else:
                # McpConfig ì¸ìŠ¤í„´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
                return mcp_servers

        # ë‹¨ì¼ McpConfig ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš°
        else:
            return [mcp_servers]

    def check(self) -> list[dict]:
        """Check configuration and return list of error dicts"""
        return []

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(model={self.model}, embedding_model={self.embedding_model}, temperature={self.temperature}, max_tokens={self.max_tokens})"

    def __len__(self) -> int:
        return len(self.history)
    
    @property
    def is_stateless(self) -> bool:
        """í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ê°€ stateless ëª¨ë“œì¸ì§€ í™•ì¸"""
        return self.stateless

    def __or__(self, next_llm: Union["BaseLLM", "SequentialChain"]) -> "SequentialChain":
        if isinstance(next_llm, BaseLLM):
            return SequentialChain(self, next_llm)
        elif isinstance(next_llm, SequentialChain):
            next_llm.insert_first(self)
            return next_llm
        else:
            raise TypeError("next_llm must be an instance of BaseLLM or SequentialChain")

    def __ror__(self, prev_llm: Union["BaseLLM", "SequentialChain"]) -> "SequentialChain":
        if isinstance(prev_llm, BaseLLM):
            return SequentialChain(prev_llm, self)
        elif isinstance(prev_llm, SequentialChain):
            prev_llm.append(self)
            return prev_llm
        else:
            raise TypeError("prev_llm must be an instance of BaseLLM or SequentialChain")

    def clear(self):
        """Clear the chat history"""
        # stateless ëª¨ë“œì—ì„œëŠ” ì´ë¯¸ historyê°€ ë¹„ì–´ìˆìœ¼ë¯€ë¡œ ì•„ë¬´ ë™ì‘ ì•ˆ í•¨
        if not self.stateless:
            self.history = []

    def _process_template(self, template: Union[str, Template], context: dict[str, Any]) -> Optional[str]:
        """í…œí”Œë¦¿ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³µí†µ ë©”ì„œë“œ"""
        # Django Template ê°ì²´ì¸ ê²½ìš°
        if hasattr(template, "render"):
            logger.debug("using template render : %s", template)
            return template.render(Context(context))

        # ë¬¸ìì—´ì¸ ê²½ìš°
        elif isinstance(template, str):
            # íŒŒì¼ ê¸°ë°˜ í…œí”Œë¦¿ ì²˜ë¦¬
            if "prompts/" in template and template.endswith((".txt", ".md", ".yaml")):
                try:
                    template_obj = get_template(template)
                    logger.debug("using template render : %s", template)
                    return template_obj.render(context)
                except TemplateDoesNotExist:
                    logger.debug("Template '%s' does not exist", template)
                    return None
            # ì¥ê³  í…œí”Œë¦¿ ë¬¸ë²•ì˜ ë¬¸ìì—´
            elif "{{" in template or "{%" in template:
                logger.debug("using string template render : %s ...", repr(template))
                return Template(template).render(Context(context))
            # ì¼ë°˜ ë¬¸ìì—´ í¬ë§·íŒ… - ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ì¹˜í™˜í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if context:
                try:
                    return template.format_map(TemplateDict(context))
                except Exception as e:
                    logger.debug("Template formatting failed: %s", e)
                    return template
            return template

        return None

    def get_system_prompt(self, input_context: dict[str, Any], default: Any = None) -> Optional[str]:
        if not self.system_prompt:
            return default

        return self._process_template(self.system_prompt, input_context)

    def get_human_prompt(self, input: Union[str, dict[str, Any]], context: dict[str, Any]) -> str:
        if isinstance(input, (str, Template)) or hasattr(input, "render"):
            result = self._process_template(input, context)
            if result is not None:
                return result

        elif isinstance(input, dict):
            if self.prompt:
                # promptê°€ ìˆìœ¼ë©´ í…œí”Œë¦¿ ë Œë”ë§
                result = self._process_template(self.prompt, context)
                if result is not None:
                    return result
            else:
                # promptê°€ ì—†ìœ¼ë©´ dictë¥¼ ìë™ìœ¼ë¡œ í¬ë§·íŒ…
                # contextì—ì„œ 'user_message' í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
                if "user_message" in context:
                    return str(context["user_message"])
                # ì•„ë‹ˆë©´ dictì˜ ë‚´ìš©ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
                formatted_parts = []
                for key, value in context.items():
                    if key not in ["choices", "choices_formatted", "choices_optional"]:
                        formatted_parts.append(f"{key}: {value}")
                return "\n".join(formatted_parts) if formatted_parts else ""

        raise ValueError(f"input must be a str, Template, or dict, but got {type(input)}")

    def _update_history(
        self,
        human_message: Message,
        ai_message: Union[str, Message],
        tool_interactions: Optional[list[dict]] = None,
    ) -> None:
        # stateless ëª¨ë“œì—ì„œëŠ” historyë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
        if self.stateless:
            return
            
        if isinstance(ai_message, str):
            ai_message = Message(role="assistant", content=ai_message, tool_interactions=tool_interactions)
        elif tool_interactions and not ai_message.tool_interactions:
            # tool_interactionsê°€ ì œê³µë˜ì—ˆì§€ë§Œ ai_messageì— ì—†ëŠ” ê²½ìš°
            ai_message.tool_interactions = tool_interactions

        # ë©”ëª¨ë¦¬ì— ì €ì¥
        self.history.extend(
            [
                human_message,
                ai_message,
            ]
        )

        # ë°±ì—…ì´ ìˆìœ¼ë©´ ë°±ì—… ìˆ˜í–‰
        if self.history_backup:
            try:
                # usage ì •ë³´ë¥¼ ìœ„í•´ _last_usage ì†ì„± í™•ì¸
                usage = getattr(self, "_last_usage", None)
                self.history_backup.save_exchange(
                    user_msg=human_message, assistant_msg=ai_message, usage=usage, model=self.model
                )
            except Exception as e:
                # ë°±ì—… ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                logger.warning(f"History backup failed: {e}")

    def get_output_key(self) -> str:
        return self.output_key

    def _process_choice_response(
        self, text: str, choices: list[str], choices_optional: bool
    ) -> tuple[Optional[str], Optional[int], float]:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ choiceë¥¼ ì¶”ì¶œí•˜ê³  ê²€ì¦

        Returns:
            (choice, index, confidence) íŠœí”Œ
        """
        import re

        # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„ (OpenAI, Google ë“±)
        try:
            import json

            # ë¨¼ì € ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ íŒŒì‹± ì‹œë„ (strict=Falseë¡œ ì œì–´ ë¬¸ì í—ˆìš©)
            try:
                data = json.loads(text, strict=False)
            except json.JSONDecodeError:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì œì–´ ë¬¸ì ì œê±° í›„ ì¬ì‹œë„
                # ì œì–´ ë¬¸ìë§Œ ì œê±°
                text_cleaned = re.sub(r"[\x00-\x1f\x7f]", "", text)
                data = json.loads(text_cleaned)

            if isinstance(data, dict):
                # choice_indexê°€ ìˆìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ ì§ì ‘ ì‚¬ìš©
                if "choice_index" in data and isinstance(data["choice_index"], int):
                    index = data["choice_index"]
                    if 0 <= index < len(choices):
                        return choices[index], index, data.get("confidence", 1.0)
                    else:
                        logger.warning(f"Invalid choice_index {index} for {len(choices)} choices")
                
                # choice_indexê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                if "choice" in data:
                    choice = data["choice"]
                    # choice ê°’ì—ì„œ ì œì–´ ë¬¸ì ì œê±°
                    # ì˜ˆ: "\u001cA/S\u001dìš”ì²­" â†’ "A/Sìš”ì²­"
                    choice = re.sub(r"[\x00-\x1f\x7f]", "", choice)
                    
                    # ë””ë²„ê¹…ìš© ë¡œê·¸
                    logger.debug(f"Original choice: {repr(data['choice'])}")
                    logger.debug(f"Cleaned choice: {repr(choice)}")
                    logger.debug(f"Available choices: {choices}")

                    # choicesì—ì„œ ì •í™•í•œ ë§¤ì¹­ ì°¾ê¸°
                    if choice in choices:
                        return choice, choices.index(choice), data.get("confidence", 1.0)

                    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì œì–´ ë¬¸ìë¡œ ì¸í•´ ì˜ë¦° ê²½ìš°)
                    for i, candidate in enumerate(choices):
                        # ì–‘ë°©í–¥ ë¶€ë¶„ ë§¤ì¹­ í™•ì¸
                        if candidate.startswith(choice) or choice in candidate:
                            logger.debug(f"Partial match found: '{choice}' -> '{candidate}'")
                            return candidate, i, data.get("confidence", 0.8)
                        # ë°˜ëŒ€ ë°©í–¥ë„ í™•ì¸ (choiceê°€ candidateë³´ë‹¤ ê¸´ ê²½ìš°)
                        if choice.startswith(candidate):
                            logger.debug(f"Reverse partial match found: '{choice}' -> '{candidate}'")
                            return candidate, i, data.get("confidence", 0.7)

        except (json.JSONDecodeError, KeyError, TypeError):
            pass

        # í…ìŠ¤íŠ¸ ë§¤ì¹­
        text_clean = text.strip()

        # ì •í™•í•œ ë§¤ì¹­
        if text_clean in choices:
            return text_clean, choices.index(text_clean), 1.0

        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­
        text_lower = text_clean.lower()
        for i, choice in enumerate(choices):
            if choice.lower() == text_lower:
                return choice, i, 0.9

        # ë¶€ë¶„ ë§¤ì¹­
        for i, choice in enumerate(choices):
            if choice in text_clean or text_clean in choice:
                logger.warning("Partial match found. Response: '%s', Matched: '%s'", text_clean, choice)
                return choice, i, 0.7

        # choices_optionalì´ Trueì´ê³  "None of the above"ê°€ í¬í•¨ëœ ê²½ìš°
        if choices_optional and ("none of the above" in text_lower or "í•´ë‹¹ ì—†ìŒ" in text_clean):
            return None, None, 0.8

        # ë§¤ì¹­ ì‹¤íŒ¨
        logger.warning("No valid choice found in response: %s", text_clean)
        return None, None, 0.0

    def _process_schema_response(
        self, text: str, schema: Type["BaseModel"]
    ) -> tuple[Optional["BaseModel"], Optional[list[str]]]:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ íŒŒì‹±í•˜ê³  ê²€ì¦

        Returns:
            (parsed_model, validation_errors) íŠœí”Œ
        """
        import json
        import re

        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        text = text.strip()

        # ```json ... ``` íŒ¨í„´ ë§¤ì¹­
        json_block_pattern = r"```(?:json)?\s*\n?(.*?)\n?```"
        match = re.search(json_block_pattern, text, re.DOTALL)
        if match:
            text = match.group(1).strip()

        try:
            # JSON íŒŒì‹± ì‹œë„
            data = json.loads(text)

            # Pydantic ëª¨ë¸ë¡œ ê²€ì¦
            model_instance = schema(**data)
            return model_instance, None

        except json.JSONDecodeError as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨
            error_msg = f"JSON parsing failed: {str(e)}"
            logger.warning(error_msg)
            return None, [error_msg]

        except Exception as e:
            # Pydantic ê²€ì¦ ì‹¤íŒ¨
            error_msg = f"Schema validation failed: {str(e)}"
            logger.warning(error_msg)
            return None, [error_msg]

    @abc.abstractmethod
    def _make_request_params(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> dict:
        pass

    @abc.abstractmethod
    def _make_ask(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> Reply:
        """Generate a response using the specific LLM provider"""
        pass

    @abc.abstractmethod
    async def _make_ask_async(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> Reply:
        """Generate a response asynchronously using the specific LLM provider"""
        pass

    @abc.abstractmethod
    def _make_ask_stream(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> Generator[Reply, None, None]:
        """Generate a streaming response using the specific LLM provider"""
        yield Reply(text="")

    @abc.abstractmethod
    async def _make_ask_stream_async(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> AsyncGenerator[Reply, None]:
        """Generate a streaming response asynchronously using the specific LLM provider"""
        yield Reply(text="")

    def _ask_impl(
        self,
        input: Union[str, dict[str, str]],
        files: Optional[list[Union[str, Path, IO]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        *,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        schema: Optional[Type["BaseModel"]] = None,
        is_async: bool = False,
        stream: bool = False,
        use_history: bool = True,
        raise_errors: bool = False,
    ):
        """ë™ê¸° ë˜ëŠ” ë¹„ë™ê¸° ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ (ì¼ë°˜/ìŠ¤íŠ¸ë¦¬ë°)"""
        # stateless ëª¨ë“œì—ì„œëŠ” use_historyë¥¼ ë¬´ì‹œ
        if self.stateless and use_history:
            logger.debug("stateless ëª¨ë“œì—ì„œëŠ” use_history=Trueê°€ ë¬´ì‹œë©ë‹ˆë‹¤.")
            use_history = False
            
        current_messages = [*self.history] if use_history else []
        current_model: LLMChatModelType = cast(LLMChatModelType, model or self.model)

        if isinstance(input, dict):
            input_context = input
        else:
            input_context = {}

        if context:
            input_context.update(context)

        # cache ê°ì²´ê°€ ìˆìœ¼ë©´ ìºì‹œ ì‚¬ìš©
        input_context["enable_cache"] = self.cache is not None

        # choices ì²˜ë¦¬
        if choices:
            if len(choices) < 2:
                raise ValueError("choices must contain at least 2 items")

            # choices_optionalì´ Trueë©´ None ì˜µì…˜ ì¶”ê°€
            internal_choices = choices.copy()
            if choices_optional:
                internal_choices.append("None of the above")

            # choices ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            input_context["choices"] = internal_choices  # ì›ë³¸ ê·¸ëŒ€ë¡œ ì „ë‹¬
            input_context["choices_formatted"] = "\n".join([f"{i+1}. {c}" for i, c in enumerate(internal_choices)])
            input_context["choices_optional"] = choices_optional

        # schema ì²˜ë¦¬
        if schema:
            input_context["schema"] = schema
            input_context["schema_json"] = schema.model_json_schema()

        human_prompt = self.get_human_prompt(input, input_context)
        human_message = Message(role="user", content=human_prompt, files=files)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        if stream:

            async def async_stream_handler() -> AsyncGenerator[Reply, None]:
                try:
                    text_list = []
                    async for ask in self._make_ask_stream_async(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    ):
                        text_list.append(ask.text)
                        yield ask

                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ choices ì²˜ë¦¬
                    if choices and text_list:
                        full_text = "".join(text_list)
                        choice, index, confidence = self._process_choice_response(
                            full_text, input_context["choices"], choices_optional
                        )
                        # ë§ˆì§€ë§‰ì— choice ì •ë³´ë¥¼ í¬í•¨í•œ Reply ì „ì†¡
                        yield Reply(text="", choice=choice, choice_index=index, confidence=confidence)

                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ schema ì²˜ë¦¬
                    if schema and text_list:
                        full_text = "".join(text_list)
                        structured_data, validation_errors = self._process_schema_response(full_text, schema)
                        # ë§ˆì§€ë§‰ì— structured_data ì •ë³´ë¥¼ í¬í•¨í•œ Reply ì „ì†¡
                        yield Reply(text="", structured_data=structured_data, validation_errors=validation_errors)

                    if use_history:
                        ai_text = "".join(text_list)
                        self._update_history(human_message=human_message, ai_message=ai_text)
                except Exception as e:
                    if raise_errors:
                        raise e
                    yield Reply(text=f"Error: {str(e)}")

            def sync_stream_handler() -> Generator[Reply, None, None]:
                try:
                    text_list = []
                    for ask in self._make_ask_stream(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    ):
                        text_list.append(ask.text)
                        yield ask

                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ choices ì²˜ë¦¬
                    if choices and text_list:
                        full_text = "".join(text_list)
                        choice, index, confidence = self._process_choice_response(
                            full_text, input_context["choices"], choices_optional
                        )
                        # ë§ˆì§€ë§‰ì— choice ì •ë³´ë¥¼ í¬í•¨í•œ Reply ì „ì†¡
                        yield Reply(text="", choice=choice, choice_index=index, confidence=confidence)

                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ schema ì²˜ë¦¬
                    if schema and text_list:
                        full_text = "".join(text_list)
                        structured_data, validation_errors = self._process_schema_response(full_text, schema)
                        # ë§ˆì§€ë§‰ì— structured_data ì •ë³´ë¥¼ í¬í•¨í•œ Reply ì „ì†¡
                        yield Reply(text="", structured_data=structured_data, validation_errors=validation_errors)

                    if use_history:
                        ai_text = "".join(text_list)
                        self._update_history(human_message=human_message, ai_message=ai_text)
                except Exception as e:
                    if raise_errors:
                        raise e
                    yield Reply(text=f"Error: {str(e)}")

            return async_stream_handler() if is_async else sync_stream_handler()

        # ì¼ë°˜ ì‘ë‹µ ì²˜ë¦¬
        else:

            async def async_handler() -> Reply:
                try:
                    ask = await self._make_ask_async(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    )
                except Exception as e:
                    if raise_errors:
                        raise e
                    return Reply(text=f"Error: {str(e)}")
                else:
                    # choicesê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if choices:
                        choice, index, confidence = self._process_choice_response(
                            ask.text, input_context["choices"], choices_optional
                        )
                        ask.choice = choice
                        ask.choice_index = index
                        ask.confidence = confidence

                    # schemaê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if schema:
                        structured_data, validation_errors = self._process_schema_response(ask.text, schema)
                        ask.structured_data = structured_data
                        ask.validation_errors = validation_errors

                    if use_history:
                        self._update_history(human_message=human_message, ai_message=ask.text)
                    return ask

            def sync_handler() -> Reply:
                try:
                    ask = self._make_ask(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    )
                except Exception as e:
                    if raise_errors:
                        raise e
                    return Reply(text=f"Error: {str(e)}")
                else:
                    # choicesê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if choices:
                        choice, index, confidence = self._process_choice_response(
                            ask.text, input_context["choices"], choices_optional
                        )
                        ask.choice = choice
                        ask.choice_index = index
                        ask.confidence = confidence

                    # schemaê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if schema:
                        structured_data, validation_errors = self._process_schema_response(ask.text, schema)
                        ask.structured_data = structured_data
                        ask.validation_errors = validation_errors

                    if use_history:
                        self._update_history(human_message=human_message, ai_message=ask.text)
                    return ask

            return async_handler() if is_async else sync_handler()

    def invoke(
        self,
        input: Union[str, dict[str, str]],
        files: Optional[list[Union[str, Path, IO]]] = None,
        stream: bool = False,
        raise_errors: bool = False,
    ) -> Reply:
        """langchain í˜¸í™˜ ë©”ì„œë“œ: ë™ê¸°ì ìœ¼ë¡œ LLMì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.ask(input=input, files=files, stream=stream, raise_errors=raise_errors)

    def stream(
        self,
        input: Union[str, dict[str, str]],
        files: Optional[list[Union[str, Path, IO]]] = None,
        raise_errors: bool = False,
    ) -> Generator[Reply, None, None]:
        """langchain í˜¸í™˜ ë©”ì„œë“œ: ë™ê¸°ì ìœ¼ë¡œ LLMì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
        return self.ask(input=input, files=files, stream=True, raise_errors=raise_errors)

    def ask(
        self,
        input: Union[str, dict[str, Any]],
        files: Optional[list[Union[str, Path, IO]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        *,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        schema: Optional[Type["BaseModel"]] = None,
        stream: bool = False,
        use_history: bool = True,
        raise_errors: bool = False,
        tools: Optional[list] = None,
        tool_choice: str = "auto",
        max_tool_calls: int = 5,
    ) -> Union[Reply, Generator[Reply, None, None]]:
        # schemaì™€ choicesëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
        if schema and choices:
            raise ValueError("Cannot use both 'schema' and 'choices' parameters at the same time")

        # ê¸°ë³¸ ë„êµ¬ì™€ ask ë„êµ¬ë¥¼ í•©ì¹¨
        merged_tools = self._merge_tools(tools)

        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ë„êµ¬ì™€ í•¨ê»˜ ì²˜ë¦¬
        if merged_tools:
            return self._ask_with_tools(
                input=input,
                files=files,
                model=model,
                context=context,
                tools=merged_tools,
                tool_choice=tool_choice,
                max_tool_calls=max_tool_calls,
                choices=choices,
                choices_optional=choices_optional,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
                is_async=False,
            )
        else:
            return self._ask_impl(
                input=input,
                files=files,
                model=model,
                context=context,
                choices=choices,
                choices_optional=choices_optional,
                schema=schema,
                is_async=False,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
            )

    async def ask_async(
        self,
        input: Union[str, dict[str, Any]],
        files: Optional[list[Union[str, Path, IO]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        *,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        schema: Optional[Type["BaseModel"]] = None,
        stream: bool = False,
        raise_errors: bool = False,
        use_history: bool = True,
        tools: Optional[list] = None,
        tool_choice: str = "auto",
        max_tool_calls: int = 5,
    ) -> Union[Reply, AsyncGenerator[Reply, None]]:
        # schemaì™€ choicesëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
        if schema and choices:
            raise ValueError("Cannot use both 'schema' and 'choices' parameters at the same time")

        # ê¸°ë³¸ ë„êµ¬ì™€ ask ë„êµ¬ë¥¼ í•©ì¹¨
        merged_tools = self._merge_tools(tools)

        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ë„êµ¬ì™€ í•¨ê»˜ ì²˜ë¦¬
        if merged_tools:
            return_value = self._ask_with_tools(
                input=input,
                files=files,
                model=model,
                context=context,
                tools=merged_tools,
                tool_choice=tool_choice,
                max_tool_calls=max_tool_calls,
                choices=choices,
                choices_optional=choices_optional,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
                is_async=True,
            )
        else:
            return_value = self._ask_impl(
                input=input,
                files=files,
                model=model,
                context=context,
                choices=choices,
                choices_optional=choices_optional,
                schema=schema,
                is_async=True,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
            )

        if stream:
            return return_value
        return await return_value

    #
    # Function Calling & Tool Support
    #

    def _merge_tools(self, ask_tools: Optional[list]) -> list:
        """ê¸°ë³¸ ë„êµ¬ì™€ ask ì‹œ ì œê³µëœ ë„êµ¬ë¥¼ í•©ì¹©ë‹ˆë‹¤.

        Args:
            ask_tools: ask í˜¸ì¶œ ì‹œ ì œê³µëœ ë„êµ¬ë“¤

        Returns:
            í•©ì³ì§„ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µì‹œ ask_toolsê°€ ìš°ì„ )
        """
        # tools ëª¨ë“ˆì„ ë™ì  import (ìˆœí™˜ import ë°©ì§€)
        from .tools import ToolAdapter

        # 1. ê¸°ë³¸ toolsë¡œ ì‹œì‘ (nameì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬)
        merged = {tool.name: tool for tool in self.default_tools}

        # 2. ask tools ì¶”ê°€ (ì¤‘ë³µì‹œ ë®ì–´ì”€)
        if ask_tools:
            adapted_ask_tools = ToolAdapter.adapt_tools(ask_tools)
            for tool in adapted_ask_tools:
                merged[tool.name] = tool

        return list(merged.values())

    def _ask_with_tools(
        self,
        input: Union[str, dict[str, Any]],
        files: Optional[list[Union[str, Path, IO]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        tools: Optional[list] = None,
        tool_choice: str = "auto",
        max_tool_calls: int = 5,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        stream: bool = False,
        use_history: bool = True,
        raise_errors: bool = False,
        is_async: bool = False,
    ):
        """ë„êµ¬ì™€ í•¨ê»˜ LLM í˜¸ì¶œì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            tools: ì´ë¯¸ Tool ê°ì²´ë¡œ ë³€í™˜ëœ ë„êµ¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        # tools ëª¨ë“ˆì„ ë™ì  import (ìˆœí™˜ import ë°©ì§€)
        from .tools import ToolExecutor

        # toolsëŠ” ì´ë¯¸ _merge_toolsì—ì„œ Tool ê°ì²´ë¡œ ë³€í™˜ë¨
        adapted_tools = tools

        # ë„êµ¬ ì‹¤í–‰ê¸° ì¤€ë¹„
        executor = ToolExecutor(adapted_tools)

        # Providerë³„ ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë³€í™˜ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        provider_tools = self._convert_tools_for_provider(adapted_tools)

        if is_async:
            return self._ask_with_tools_async(
                input,
                files,
                model,
                context,
                adapted_tools,
                provider_tools,
                executor,
                tool_choice,
                max_tool_calls,
                choices,
                choices_optional,
                stream,
                use_history,
                raise_errors,
            )
        else:
            return self._ask_with_tools_sync(
                input,
                files,
                model,
                context,
                adapted_tools,
                provider_tools,
                executor,
                tool_choice,
                max_tool_calls,
                choices,
                choices_optional,
                stream,
                use_history,
                raise_errors,
            )

    def _ask_with_tools_sync(
        self,
        input,
        files,
        model,
        context,
        adapted_tools,
        provider_tools,
        executor,
        tool_choice,
        max_tool_calls,
        choices,
        choices_optional,
        stream,
        use_history,
        raise_errors,
    ):
        """ë™ê¸° ë²„ì „ì˜ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
        # Trace ì‹œì‘
        if llm_settings.trace_function_calls:
            print("ğŸ” [TRACE] Function Calling ì‹œì‘")
            print(f"   ì…ë ¥: {input}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[tool.name for tool in adapted_tools]}")
            print(f"   ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜: {max_tool_calls}")

        # ì´ˆê¸° ë©”ì‹œì§€ ì¤€ë¹„
        current_messages = [*self.history] if use_history else []
        human_prompt = self.get_human_prompt(input, context or {})

        # ë„êµ¬ í˜¸ì¶œ ê¸°ë¡ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        tool_interactions = []

        # ë„êµ¬ í˜¸ì¶œ ë°˜ë³µ
        for call_count in range(max_tool_calls):
            try:
                if llm_settings.trace_function_calls:
                    print(f"\nğŸ“ [TRACE] LLM í˜¸ì¶œ #{call_count + 1}")

                # LLM í˜¸ì¶œ (ë„êµ¬ í¬í•¨)
                response = self._make_ask_with_tools_sync(
                    human_prompt if call_count == 0 else None,
                    current_messages,
                    provider_tools,
                    tool_choice,
                    model,
                    files if call_count == 0 else None,
                )

                # ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ
                tool_calls = self._extract_tool_calls_from_response(response)

                if llm_settings.trace_function_calls:
                    if tool_calls:
                        print(f"   LLMì´ ìš”ì²­í•œ ë„êµ¬ í˜¸ì¶œ: {len(tool_calls)}ê°œ")
                        for i, call in enumerate(tool_calls):
                            print(f"     {i+1}. {call['name']}({call['arguments']})")
                    else:
                        print(f"   ë„êµ¬ í˜¸ì¶œ ì—†ìŒ, ìµœì¢… ì‘ë‹µ: {response.text[:100]}...")

                # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì™„ë£Œ
                if not tool_calls:
                    if llm_settings.trace_function_calls:
                        print(f"âœ… [TRACE] Function Calling ì™„ë£Œ (ì´ {call_count + 1}íšŒ í˜¸ì¶œ)")
                    if use_history and call_count == 0:
                        human_message = Message(role="user", content=human_prompt, files=files)
                        self._update_history(
                            human_message,
                            response.text,
                            tool_interactions=tool_interactions if tool_interactions else None,
                        )
                    return response

                # ë„êµ¬ ì‹¤í–‰
                if llm_settings.trace_function_calls:
                    print("\nğŸ› ï¸  [TRACE] ë„êµ¬ ì‹¤í–‰ ì¤‘...")

                for tool_call in tool_calls:
                    try:
                        if llm_settings.trace_function_calls:
                            # ì¸ìë¥¼ ë” ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…
                            args_str = ", ".join([f"{k}={v}" for k, v in tool_call["arguments"].items()])
                            print(f"   ì‹¤í–‰: {tool_call['name']}({args_str})")

                        result = executor.execute_tool(tool_call["name"], tool_call["arguments"])

                        if llm_settings.trace_function_calls:
                            print(f"   ê²°ê³¼: {result}")

                        # ë„êµ¬ í˜¸ì¶œ ê¸°ë¡
                        tool_interactions.append(
                            {"tool": tool_call["name"], "arguments": tool_call["arguments"], "result": str(result)}
                        )

                        # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                        current_messages.append(Message(role="assistant", content=f"[Tool Call: {tool_call['name']}]"))
                        current_messages.append(Message(role="user", content=f"[Tool Result: {result}]"))
                    except Exception as e:
                        if llm_settings.trace_function_calls:
                            print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
                        if raise_errors:
                            raise e
                        error_msg = f"Tool execution error: {str(e)}"
                        # ì—ëŸ¬ë„ ê¸°ë¡
                        tool_interactions.append(
                            {"tool": tool_call["name"], "arguments": tool_call["arguments"], "error": str(e)}
                        )
                        current_messages.append(Message(role="user", content=f"[Tool Error: {error_msg}]"))

                # ì²« ë²ˆì§¸ í˜¸ì¶œì´ë©´ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                if use_history and call_count == 0:
                    human_message = Message(role="user", content=human_prompt, files=files)
                    current_messages.insert(0, human_message)

            except Exception as e:
                if raise_errors:
                    raise e
                return Reply(text=f"Error in tool processing: {str(e)}")

        # ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜ì— ë„ë‹¬í•œ ê²½ìš° ìµœì¢… ì‘ë‹µ
        try:
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ human_messageë¡œ ì‚¬ìš©
            if current_messages:
                final_human_message = current_messages[-1]
                final_messages = current_messages[:-1]
            else:
                final_human_message = Message(role="user", content="", files=files)
                final_messages = []

            final_response = self._make_ask(
                input_context={},
                human_message=final_human_message,
                messages=final_messages,
                model=model,
            )

            # ìµœì¢… ì‘ë‹µì— tool_interactions ì¶”ê°€
            if use_history and tool_interactions:
                # ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ìµœì¢… ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                original_human_msg = Message(role="user", content=human_prompt, files=files)
                self._update_history(original_human_msg, final_response.text, tool_interactions=tool_interactions)

            return final_response
        except Exception as e:
            if raise_errors:
                raise e
            return Reply(text=f"Final response error: {str(e)}")

    async def _ask_with_tools_async(
        self,
        input,
        files,
        model,
        context,
        adapted_tools,
        provider_tools,
        executor,
        tool_choice,
        max_tool_calls,
        choices,
        choices_optional,
        stream,
        use_history,
        raise_errors,
    ):
        """ë¹„ë™ê¸° ë²„ì „ì˜ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
        # ì´ˆê¸° ë©”ì‹œì§€ ì¤€ë¹„
        current_messages = [*self.history] if use_history else []
        human_prompt = self.get_human_prompt(input, context or {})

        # ë„êµ¬ í˜¸ì¶œ ê¸°ë¡ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        tool_interactions = []

        # ë„êµ¬ í˜¸ì¶œ ë°˜ë³µ
        for call_count in range(max_tool_calls):
            try:
                # LLM í˜¸ì¶œ (ë„êµ¬ í¬í•¨)
                response = await self._make_ask_with_tools_async(
                    human_prompt if call_count == 0 else None,
                    current_messages,
                    provider_tools,
                    tool_choice,
                    model,
                    files if call_count == 0 else None,
                )

                # ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ
                tool_calls = self._extract_tool_calls_from_response(response)

                # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì™„ë£Œ
                if not tool_calls:
                    if use_history and call_count == 0:
                        human_message = Message(role="user", content=human_prompt, files=files)
                        self._update_history(
                            human_message,
                            response.text,
                            tool_interactions=tool_interactions if tool_interactions else None,
                        )
                    return response

                # ë„êµ¬ ì‹¤í–‰
                for tool_call in tool_calls:
                    try:
                        result = await executor.execute_tool_async(tool_call["name"], tool_call["arguments"])

                        # ë„êµ¬ í˜¸ì¶œ ê¸°ë¡
                        tool_interactions.append(
                            {"tool": tool_call["name"], "arguments": tool_call["arguments"], "result": str(result)}
                        )

                        # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                        current_messages.append(Message(role="assistant", content=f"[Tool Call: {tool_call['name']}]"))
                        current_messages.append(Message(role="user", content=f"[Tool Result: {result}]"))
                    except Exception as e:
                        if raise_errors:
                            raise e
                        error_msg = f"Tool execution error: {str(e)}"
                        # ì—ëŸ¬ë„ ê¸°ë¡
                        tool_interactions.append(
                            {"tool": tool_call["name"], "arguments": tool_call["arguments"], "error": str(e)}
                        )
                        current_messages.append(Message(role="user", content=f"[Tool Error: {error_msg}]"))

                # ì²« ë²ˆì§¸ í˜¸ì¶œì´ë©´ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                if use_history and call_count == 0:
                    human_message = Message(role="user", content=human_prompt, files=files)
                    current_messages.insert(0, human_message)

            except Exception as e:
                if raise_errors:
                    raise e
                return Reply(text=f"Error in tool processing: {str(e)}")

        # ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜ì— ë„ë‹¬í•œ ê²½ìš° ìµœì¢… ì‘ë‹µ
        try:
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ human_messageë¡œ ì‚¬ìš©
            if current_messages:
                final_human_message = current_messages[-1]
                final_messages = current_messages[:-1]
            else:
                final_human_message = Message(role="user", content="", files=files)
                final_messages = []

            final_response = await self._make_ask_async(
                input_context={},
                human_message=final_human_message,
                messages=final_messages,
                model=model,
            )

            # ìµœì¢… ì‘ë‹µì— tool_interactions ì¶”ê°€
            if use_history and tool_interactions:
                # ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ìµœì¢… ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                original_human_msg = Message(role="user", content=human_prompt, files=files)
                self._update_history(original_human_msg, final_response.text, tool_interactions=tool_interactions)

            return final_response
        except Exception as e:
            if raise_errors:
                raise e
            return Reply(text=f"Final response error: {str(e)}")

    def _convert_tools_for_provider(self, tools):
        """Providerë³„ ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë³€í™˜ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (Function Calling ë¯¸ì§€ì›)
        return []

    def _extract_tool_calls_from_response(self, response):
        """ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì¶”ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []

    def _make_ask_with_tools_sync(self, human_prompt, messages, tools, tool_choice, model, files):
        """ë„êµ¬ì™€ í•¨ê»˜ ë™ê¸° LLM í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ask í˜¸ì¶œ
        return self._make_ask(
            input_context={},
            human_message=Message(role="user", content=human_prompt or ""),
            messages=messages,
            model=model,
        )

    async def _make_ask_with_tools_async(self, human_prompt, messages, tools, tool_choice, model, files):
        """ë„êµ¬ì™€ í•¨ê»˜ ë¹„ë™ê¸° LLM í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ask í˜¸ì¶œ
        return await self._make_ask_async(
            input_context={},
            human_message=Message(role="user", content=human_prompt or ""),
            messages=messages,
            model=model,
        )

    #
    # embed
    #
    def get_embed_size(self, model: Optional[LLMEmbeddingModelType] = None) -> int:
        return self.EMBEDDING_DIMENSIONS[model or self.embedding_model]

    @property
    def embed_size(self) -> int:
        return self.get_embed_size()

    @abc.abstractmethod
    def embed(
        self,
        input: Union[str, list[str]],
        model: Optional[LLMEmbeddingModelType] = None,
    ) -> Union[Embed, EmbedList]:
        pass

    @abc.abstractmethod
    async def embed_async(
        self,
        input: Union[str, list[str]],
        model: Optional[LLMEmbeddingModelType] = None,
    ) -> Union[Embed, EmbedList]:
        pass

    @abc.abstractmethod
    def generate_image(
        self,
        prompt: str,
        *,
        size: Optional[str] = None,
        quality: Optional[str] = None,
        style: Optional[str] = None,
        n: int = 1,
        response_format: str = "url",
        **kwargs
    ) -> "ImageReply":
        """Generate images from text prompts.
        
        Args:
            prompt: The text prompt to generate images from
            size: Image size (e.g., "1024x1024", "1024x1792", "1792x1024")
            quality: Image quality ("standard" or "hd")
            style: Image style ("vivid" or "natural")
            n: Number of images to generate
            response_format: Format of the response ("url" or "base64")
            **kwargs: Additional provider-specific parameters
            
        Returns:
            ImageReply: Generated image response
            
        Raises:
            NotImplementedError: If the provider doesn't support image generation
            ValueError: If the model doesn't support image generation or invalid parameters
        """
        pass

    @abc.abstractmethod
    async def generate_image_async(
        self,
        prompt: str,
        *,
        size: Optional[str] = None,
        quality: Optional[str] = None,
        style: Optional[str] = None,
        n: int = 1,
        response_format: str = "url",
        **kwargs
    ) -> "ImageReply":
        """Asynchronously generate images from text prompts.
        
        Args:
            prompt: The text prompt to generate images from
            size: Image size (e.g., "1024x1024", "1024x1792", "1792x1024")
            quality: Image quality ("standard" or "hd")
            style: Image style ("vivid" or "natural")
            n: Number of images to generate
            response_format: Format of the response ("url" or "base64")
            **kwargs: Additional provider-specific parameters
            
        Returns:
            ImageReply: Generated image response
            
        Raises:
            NotImplementedError: If the provider doesn't support image generation
            ValueError: If the model doesn't support image generation or invalid parameters
        """
        pass

    def supports(self, capability: str) -> bool:
        """Check if the current model supports a specific capability.
        
        Args:
            capability: The capability to check (e.g., "image_generation")
            
        Returns:
            bool: True if the capability is supported
        """
        # Default implementation - subclasses should override
        return False

    def get_supported_image_sizes(self) -> list[str]:
        """Get the list of supported image sizes for the current model.
        
        Returns:
            list[str]: List of supported sizes, empty if image generation not supported
        """
        # Default implementation - subclasses should override
        return []

    @property
    def capabilities(self) -> dict[str, Any]:
        """Get the capabilities of the current model.
        
        Returns:
            dict: Dictionary of capabilities and their details
        """
        # Default implementation - subclasses should override
        return {}

    #
    # MCP (Model Context Protocol) integration
    #

    async def initialize_mcp(self) -> None:
        """MCP ì„œë²„ë“¤ì„ ì—°ê²°í•˜ê³  ë„êµ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not self.mcp_servers:
            return

        if self._mcp_connected:
            logger.warning("MCP is already connected")
            return

        try:
            # MCP ëª¨ë“ˆì„ ë™ì  import
            from .mcp import MultiServerMCPClient
            from .mcp.policies import MCPConnectionError, MCPConnectionPolicy

            # ê¸°ë³¸ ì •ì±… ì„¤ì •
            if self.mcp_policy is None:
                self.mcp_policy = MCPConnectionPolicy.OPTIONAL

            # MultiServerMCPClient ìƒì„±
            self._mcp_client = MultiServerMCPClient(self.mcp_servers)

            # ì—°ê²° ì‹œì‘
            await self._mcp_client.__aenter__()

            # ì—°ê²° ì‹¤íŒ¨ í™•ì¸
            failed_servers = list(self._mcp_client._connection_errors.keys())
            if failed_servers:
                # ì •ì±…ì— ë”°ë¥¸ ì²˜ë¦¬
                if self.mcp_policy == MCPConnectionPolicy.REQUIRED:
                    # REQUIRED: ì˜ˆì™¸ ë°œìƒ
                    error_msg = f"Failed to connect to MCP servers: {', '.join(failed_servers)}"
                    await self.close_mcp()  # ì •ë¦¬
                    raise MCPConnectionError(error_msg, failed_servers)
                elif self.mcp_policy == MCPConnectionPolicy.WARN:
                    # WARN: ê²½ê³  ë¡œê·¸
                    logger.warning(f"Failed to connect to some MCP servers: {', '.join(failed_servers)}")
                # OPTIONAL: ì¡°ìš©íˆ ê³„ì† ì§„í–‰

            # ë„êµ¬ ë¡œë“œ
            self._mcp_tools = await self._mcp_client.get_tools()

            # ê¸°ì¡´ ë„êµ¬ì™€ ë³‘í•©
            from .tools import ToolAdapter

            if self._mcp_tools:
                adapted_mcp_tools = ToolAdapter.adapt_tools(self._mcp_tools)
                self.default_tools.extend(adapted_mcp_tools)
                logger.info(f"Loaded {len(self._mcp_tools)} MCP tools from {len(self.mcp_servers)} servers")

            self._mcp_connected = True

        except MCPConnectionError:
            # ì •ì±…ì— ë”°ë¥¸ ì˜ˆì™¸ëŠ” ë‹¤ì‹œ ë°œìƒ
            raise
        except Exception as e:
            # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            if self.mcp_policy == MCPConnectionPolicy.REQUIRED:
                error_msg = f"Failed to initialize MCP: {e}"
                await self.close_mcp()  # ì •ë¦¬
                raise MCPConnectionError(error_msg)
            else:
                logger.error(f"Failed to initialize MCP: {e}")
                # MCP ì—°ê²° ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
                self._mcp_client = None
                self._mcp_connected = False

    async def close_mcp(self, timeout: float = 5.0) -> None:
        """MCP ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.

        Args:
            timeout: ì¢…ë£Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        if self._mcp_client and self._mcp_connected:
            try:
                # íƒ€ì„ì•„ì›ƒ ì ìš©
                await asyncio.wait_for(self._mcp_client.__aexit__(None, None, None), timeout=timeout)
                logger.info("MCP connections closed")
            except asyncio.TimeoutError:
                logger.warning(f"MCP cleanup timed out after {timeout}s")
            except Exception as e:
                logger.error(f"Error closing MCP connections: {e}")
            finally:
                self._mcp_client = None
                self._mcp_connected = False

                # MCP ë„êµ¬ ì œê±°
                if self._mcp_tools:
                    # ê¸°ì¡´ ë„êµ¬ì—ì„œ MCP ë„êµ¬ ì œê±°
                    from .tools import ToolAdapter

                    adapted_mcp_tools = ToolAdapter.adapt_tools(self._mcp_tools)
                    for tool in adapted_mcp_tools:
                        if tool in self.default_tools:
                            self.default_tools.remove(tool)
                    self._mcp_tools = []

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.initialize_mcp()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_mcp()
        return False

    #
    # describe images / tables
    #

    def describe_images(
        self,
        images: Union[
            Union[str, Path, IO], list[Union[str, Path, IO]], DescribeImageRequest, list[DescribeImageRequest]
        ],
        prompt: Optional[str] = None,
        *,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        max_parallel_size: int = 4,
        raise_errors: bool = False,
        use_history: bool = False,
    ) -> Union[Reply, list[Reply]]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            images: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ(ë“¤) ë˜ëŠ” DescribeImageRequest ê°ì²´(ë“¤)
            prompt: ëª¨ë“  ì´ë¯¸ì§€ì— ì ìš©í•  í”„ë¡¬í”„íŠ¸ (DescribeImageRequest ì‚¬ìš©ì‹œ ë¬´ì‹œë¨)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            temperature: ìƒì„± ì˜¨ë„ (ì„ íƒì‚¬í•­)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ì„ íƒì‚¬í•­)
            max_parallel_size: ìµœëŒ€ ë³‘ë ¬ ì²˜ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 4)
            raise_errors: ì—ëŸ¬ ë°œìƒì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¬ì§€ ì—¬ë¶€
            enable_cache: ìºì‹± í™œì„±í™” ì—¬ë¶€
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)

        Returns:
            ë‹¨ì¼ ì´ë¯¸ì§€ì¸ ê²½ìš° Reply, ì—¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš° list[Reply]

        Examples:
            # ë‹¨ì¼ ì´ë¯¸ì§€
            response = llm.describe_images("photo.jpg")

            # ì—¬ëŸ¬ ì´ë¯¸ì§€
            responses = llm.describe_images(["img1.jpg", "img2.jpg", "img3.jpg"])

            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
            responses = llm.describe_images(
                ["img1.jpg", "img2.jpg"],
                prompt="What objects are in this image?",
                temperature=0.5
            )

            # ê¸°ì¡´ DescribeImageRequest ë°©ì‹ë„ ì§€ì›
            request = DescribeImageRequest(...)
            response = llm.describe_images(request)
        """
        return async_to_sync(self.describe_images_async)(
            images,
            prompt,
            system_prompt,
            temperature,
            max_tokens,
            max_parallel_size,
            raise_errors,
            use_history,
        )

    async def describe_images_async(
        self,
        images: Union[
            Union[str, Path, IO], list[Union[str, Path, IO]], DescribeImageRequest, list[DescribeImageRequest]
        ],
        prompt: Optional[str] = None,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        max_parallel_size: int = 4,
        raise_errors: bool = False,
        use_history: bool = False,
    ) -> Union[Reply, list[Reply]]:
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤ (ë¹„ë™ê¸°)"""

        # ì…ë ¥ì„ ì •ê·œí™”í•˜ì—¬ ì²˜ë¦¬í•  ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        is_single = False
        if isinstance(images, (DescribeImageRequest, str, Path)) or hasattr(images, "read"):
            is_single = True
            images_list = [images]
        else:
            images_list = list(images)

        # DescribeImageRequestì™€ ì¼ë°˜ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
        request_list = []
        for idx, img in enumerate(images_list):
            if isinstance(img, DescribeImageRequest):
                request_list.append(img)
            else:
                # ì¼ë°˜ ì´ë¯¸ì§€ë¥¼ DescribeImageRequestë¡œ ë³€í™˜
                request = DescribeImageRequest(
                    image=img,
                    image_path=str(img) if isinstance(img, (str, Path)) else f"image_{idx}",
                    system_prompt=system_prompt or self.system_prompt,
                    user_prompt=prompt or "Describe this image in detail.",
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                request_list.append(request)

        # ì„¸ë§ˆí¬ì–´ë¥¼ í†µí•´ ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ
        semaphore = asyncio.Semaphore(max_parallel_size)

        async def process_single_image(
            task_request: DescribeImageRequest,
            idx: int,
            total: int,
        ) -> Reply:
            async with semaphore:
                logger.info("request describe_images [%d/%d] : %s", idx + 1, total, task_request.image_path)

                # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ì˜ ì„¤ì •ì„ ì„ì‹œë¡œ ë³€ê²½
                original_system_prompt = self.system_prompt
                original_temperature = self.temperature
                original_max_tokens = self.max_tokens

                try:
                    # ìš”ì²­ë³„ ì„¤ì • ì ìš©
                    if task_request.system_prompt is not None:
                        self.system_prompt = task_request.system_prompt
                    if task_request.temperature is not None:
                        self.temperature = task_request.temperature
                    if task_request.max_tokens is not None:
                        self.max_tokens = task_request.max_tokens

                    # í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ask_async í˜¸ì¶œ
                    reply = await self.ask_async(
                        input=task_request.user_prompt,
                        files=[task_request.image],
                        context=task_request.prompt_context,
                        raise_errors=raise_errors,
                        use_history=use_history,
                    )
                    logger.debug("image description for %s : %s", task_request.image_path, repr(reply.text))
                    return reply

                finally:
                    # ì›ë˜ ì„¤ì • ë³µì›
                    self.system_prompt = original_system_prompt
                    self.temperature = original_temperature
                    self.max_tokens = original_max_tokens

        # ë³‘ë ¬ë¡œ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬
        tasks = [process_single_image(request, idx, len(request_list)) for idx, request in enumerate(request_list)]
        reply_list = await asyncio.gather(*tasks)

        # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹ (IO ê°ì²´ì¸ ê²½ìš°)
        for request in request_list:
            if hasattr(request.image, "seek"):
                request.image.seek(0)

        # ë‹¨ì¼ ì´ë¯¸ì§€ì˜€ìœ¼ë©´ Reply ë°˜í™˜, ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜€ìœ¼ë©´ list[Reply] ë°˜í™˜
        return reply_list[0] if is_single else reply_list

    def describe_image(
        self,
        image: Union[str, Path, IO],
        prompt: str = "Describe this image in detail.",
        *,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        use_history: bool = False,
        **kwargs,
    ) -> Reply:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„¤ëª… ìš”ì²­

        ì´ ë©”ì„œë“œëŠ” ask() ë©”ì„œë“œì˜ í¸ì˜ ë˜í¼ì…ë‹ˆë‹¤.
        ëŒ€ëŸ‰ ì´ë¯¸ì§€ ë³‘ë ¬ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° describe_images()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

        Args:
            image: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ, Path ê°ì²´, ë˜ëŠ” íŒŒì¼ IO ê°ì²´
            prompt: ì´ë¯¸ì§€ ì„¤ëª…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’: "Describe this image in detail.")
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            temperature: ìƒì„± ì˜¨ë„ (ì„ íƒì‚¬í•­)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ì„ íƒì‚¬í•­)
            enable_cache: ìºì‹± í™œì„±í™” ì—¬ë¶€
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            **kwargs: ask() ë©”ì„œë“œì— ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°

        Returns:
            Reply: ì´ë¯¸ì§€ ì„¤ëª…ì´ í¬í•¨ëœ ì‘ë‹µ

        Examples:
            # ê¸°ë³¸ ì‚¬ìš©
            response = llm.describe_image("photo.jpg")

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
            response = llm.describe_image("photo.jpg", use_history=True)

            # ask()ì™€ ë™ì¼í•œ ê²°ê³¼
            response = llm.ask("Describe this image in detail.", files=["photo.jpg"])
        """
        # ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ì‹œ ì €ì¥
        original_system_prompt = self.system_prompt

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì œê³µëœ ê²½ìš° ì„ì‹œë¡œ ì„¤ì •
        if system_prompt is not None:
            self.system_prompt = system_prompt

        try:
            # temperatureì™€ max_tokens ì²˜ë¦¬
            # ì¼ë¶€ í”„ë¡œë°”ì´ë”ëŠ” ì´ë¥¼ ask íŒŒë¼ë¯¸í„°ë¡œ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ ì„ì‹œë¡œ ì¸ìŠ¤í„´ìŠ¤ ê°’ ë³€ê²½
            original_temperature = self.temperature
            original_max_tokens = self.max_tokens

            if temperature is not None:
                self.temperature = temperature
            if max_tokens is not None:
                self.max_tokens = max_tokens

            # ask ë©”ì„œë“œ í˜¸ì¶œ
            result = self.ask(input=prompt, files=[image], use_history=use_history, **kwargs)
        finally:
            # ì›ë˜ ê°’ë“¤ ë³µì›
            self.system_prompt = original_system_prompt
            if temperature is not None:
                self.temperature = original_temperature
            if max_tokens is not None:
                self.max_tokens = original_max_tokens

        return result

    async def describe_image_async(
        self,
        image: Union[str, Path, IO],
        prompt: str = "Describe this image in detail.",
        *,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        use_history: bool = False,
        **kwargs,
    ) -> Reply:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„¤ëª… ìš”ì²­ (ë¹„ë™ê¸°)

        ì´ ë©”ì„œë“œëŠ” ask_async() ë©”ì„œë“œì˜ í¸ì˜ ë˜í¼ì…ë‹ˆë‹¤.

        Args:
            image: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ, Path ê°ì²´, ë˜ëŠ” íŒŒì¼ IO ê°ì²´
            prompt: ì´ë¯¸ì§€ ì„¤ëª…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’: "Describe this image in detail.")
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            temperature: ìƒì„± ì˜¨ë„ (ì„ íƒì‚¬í•­)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ì„ íƒì‚¬í•­)
            enable_cache: ìºì‹± í™œì„±í™” ì—¬ë¶€
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            **kwargs: ask_async() ë©”ì„œë“œì— ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°

        Returns:
            Reply: ì´ë¯¸ì§€ ì„¤ëª…ì´ í¬í•¨ëœ ì‘ë‹µ
        """
        # ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ì‹œ ì €ì¥
        original_system_prompt = self.system_prompt

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì œê³µëœ ê²½ìš° ì„ì‹œë¡œ ì„¤ì •
        if system_prompt is not None:
            self.system_prompt = system_prompt

        try:
            # temperatureì™€ max_tokens ì²˜ë¦¬
            original_temperature = self.temperature
            original_max_tokens = self.max_tokens

            if temperature is not None:
                self.temperature = temperature
            if max_tokens is not None:
                self.max_tokens = max_tokens

            # ask_async ë©”ì„œë“œ í˜¸ì¶œ
            result = await self.ask_async(input=prompt, files=[image], use_history=use_history, **kwargs)
        finally:
            # ì›ë˜ ê°’ë“¤ ë³µì›
            self.system_prompt = original_system_prompt
            if temperature is not None:
                self.temperature = original_temperature
            if max_tokens is not None:
                self.max_tokens = original_max_tokens

        return result

    def extract_text_from_image(self, image: Union[str, Path, IO], **kwargs) -> Reply:
        """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŠ¹í™” ë©”ì„œë“œ"""
        return self.describe_image(
            image,
            "Extract all text from this image. Return only the text content without any additional explanation.",
            **kwargs,
        )

    def analyze_image_content(self, image: Union[str, Path, IO], **kwargs) -> Reply:
        """ì´ë¯¸ì§€ ë‚´ìš© ë¶„ì„ íŠ¹í™” ë©”ì„œë“œ"""
        return self.describe_image(
            image,
            "Analyze this image and provide: 1) Main objects and subjects 2) Dominant colors and visual style 3) Setting or context 4) Any visible text 5) Overall mood or atmosphere",
            **kwargs,
        )


class SequentialChain:
    def __init__(self, *args):
        self.llms: list[BaseLLM] = list(args)

    def insert_first(self, llm) -> "SequentialChain":
        self.llms.insert(0, llm)
        return self

    def append(self, llm) -> "SequentialChain":
        self.llms.append(llm)
        return self

    def ask(self, inputs: dict[str, Any]) -> ChainReply:
        """ì²´ì¸ì˜ ê° LLMì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ì „ LLMì˜ ì¶œë ¥ì´ ë‹¤ìŒ LLMì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤."""

        for llm in self.llms:
            if llm.prompt is None:
                raise ValueError(f"prompt is required for LLM: {llm}")

        known_values = inputs.copy()
        reply_list = []
        for llm in self.llms:
            reply = llm.ask(known_values)
            reply_list.append(reply)

            output_key = llm.get_output_key()
            known_values[output_key] = str(reply)

        return ChainReply(
            values=known_values,
            reply_list=reply_list,
        )

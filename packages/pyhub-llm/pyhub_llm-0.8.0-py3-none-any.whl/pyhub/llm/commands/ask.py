import asyncio
import logging
import sys
from pathlib import Path
from typing import List, Optional

import typer
from rich.console import Console
from rich.prompt import Prompt
from rich.table import Table

# from pyhub import init
from pyhub.llm import LLM
from pyhub.llm.types import LLMChatModelEnum

console = Console()


def ask(
    ctx: typer.Context,
    query: Optional[str] = typer.Argument(None, help="ì§ˆì˜ ë‚´ìš©"),
    model: LLMChatModelEnum = typer.Option(
        LLMChatModelEnum.GPT_4O_MINI,
        "--model",
        "-m",
        help="LLM Chat ëª¨ë¸. LLM ë²¤ë”ì— ë§ê²Œ ì§€ì •í•´ì£¼ì„¸ìš”.",
    ),
    context: str = typer.Option(None, help="LLMì— ì œê³µí•  ì»¨í…ìŠ¤íŠ¸"),
    file: Optional[List[Path]] = typer.Option(
        None,
        "--file",
        "-f",
        help="ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•  íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ íŒŒì¼ ì§€ì • ê°€ëŠ¥)",
    ),
    system_prompt: str = typer.Option(None, help="LLMì— ì‚¬ìš©í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"),
    system_prompt_path: str = typer.Option(
        "system_prompt.txt",
        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ íŒŒì¼ ê²½ë¡œ",
    ),
    temperature: float = typer.Option(0.2, help="LLM ì‘ë‹µì˜ ì˜¨ë„ ì„¤ì • (0.0-2.0, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì‘ë‹µ)"),
    max_tokens: int = typer.Option(1000, help="ì‘ë‹µì˜ ìµœëŒ€ í† í° ìˆ˜"),
    is_multi: bool = typer.Option(
        False,
        "--multi",
        help="ë©€í‹° í„´ ëŒ€í™”",
    ),
    output_json: bool = typer.Option(
        False,
        "--json",
        help="JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (êµ¬ì¡°í™”ëœ ì‘ë‹µ)",
    ),
    schema_path: Optional[Path] = typer.Option(
        None,
        "--schema",
        help="JSON Schema íŒŒì¼ ê²½ë¡œ (êµ¬ì¡°í™”ëœ ì‘ë‹µ í˜•ì‹ ì •ì˜). "
        "OpenAIì™€ UpstageëŠ” ì™„ì „í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ë©°, "
        "Anthropic, Google, OllamaëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ì œí•œì  ì§€ì›ë§Œ ì œê³µí•©ë‹ˆë‹¤.",
    ),
    output_path: Optional[Path] = typer.Option(
        None,
        "--output",
        "-o",
        help="ì‘ë‹µì„ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ",
    ),
    template_name: Optional[str] = typer.Option(
        None,
        "--template",
        "-t",
        help="í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ë¦„ (toml íŒŒì¼ì—ì„œ ë¡œë“œ)",
    ),
    show_cost: bool = typer.Option(
        False,
        "--cost",
        help="ì˜ˆìƒ ë¹„ìš© í‘œì‹œ (ì£¼ì˜: Upstage ëª¨ë¸ì€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ ì§€ì› ì•ˆ ë¨)",
    ),
    show_stats: bool = typer.Option(
        False,
        "--stats",
        help="í† í° ì‚¬ìš©ëŸ‰ ë° ì‘ë‹µ ì‹œê°„ í†µê³„ í‘œì‹œ",
    ),
    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
    no_stream: bool = typer.Option(
        False,
        "--no-stream",
        help="ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (ê¸°ë³¸: ìŠ¤íŠ¸ë¦¬ë° í™œì„±)",
    ),
    enable_cache: bool = typer.Option(
        False,
        "--enable-cache",
        help="API ì‘ë‹µ ìºì‹œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤",
    ),
    # MCP ê´€ë ¨ ì˜µì…˜ë“¤
    mcp_config: Optional[str] = typer.Option(
        None, "--mcp-config", help="MCP ì„¤ì • TOML íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: ~/.pyhub-mcptools/mcp.toml)"
    ),
    mcp_inline_config: Optional[str] = typer.Option(None, "--mcp-inline-config", help="MCP ì„¤ì • JSON ë¬¸ìì—´"),
    mcp_stdio: Optional[List[str]] = typer.Option(
        None, "--mcp-stdio", help="STDIO MCP ì„œë²„ ëª…ë ¹ì–´ (ì˜ˆ: 'python server.py')"
    ),
    mcp_sse: Optional[List[str]] = typer.Option(None, "--mcp-sse", help="SSE MCP ì„œë²„ URL"),
    mcp_http: Optional[List[str]] = typer.Option(None, "--mcp-http", help="HTTP MCP ì„œë²„ URL"),
    no_default_mcp: bool = typer.Option(False, "--no-default-mcp", help="ê¸°ë³¸ MCP ì„¤ì • íŒŒì¼ ë¡œë“œ ë¹„í™œì„±í™”"),
):
    """LLMì— ì§ˆì˜í•˜ê³  ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤ (MCP ë„êµ¬ ì§€ì›).

    Examples:
        # ê¸°ë³¸ ì‚¬ìš© (ìë™ìœ¼ë¡œ ~/.pyhub-mcptools/mcp.toml ë¡œë“œ)
        pyhub.llm ask "í˜„ì¬ ë””ë ‰í„°ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ ë³´ì—¬ì¤˜"

        # íŠ¹ì • ì„¤ì • íŒŒì¼ ì‚¬ìš©
        pyhub.llm ask "ì›¹ì—ì„œ Python ë‰´ìŠ¤ ê²€ìƒ‰" --mcp-config ./custom-mcp.toml

        # ê¸°ë³¸ ì„¤ì • + ì¶”ê°€ ì„œë²„
        pyhub.llm ask "ë³µì¡í•œ ì‘ì—…" --mcp-stdio "python calc_server.py"

        # ê¸°ë³¸ ì„¤ì • ë¹„í™œì„±í™”í•˜ê³  ì§ì ‘ ì§€ì •
        pyhub.llm ask "ê°„ë‹¨í•œ ì§ˆë¬¸" --no-default-mcp --mcp-http "http://localhost:3000/mcp"

        # ì¸ë¼ì¸ JSON ì„¤ì •
        pyhub.llm ask "ê³„ì‚°í•´ì¤˜" --mcp-inline-config '{"calc": {"transport": "stdio", "command": "python", "args": ["calc_server.py"]}}'

        # í˜¼í•© ì‚¬ìš© (ëª¨ë“  ì„¤ì • ì¡°í•©)
        pyhub.llm ask "ì¢…í•© ì‘ì—…" --mcp-config ./custom.toml --mcp-stdio "python extra_server.py" --mcp-sse "http://localhost:4000/sse"

        # ê¸°ì¡´ ê¸°ëŠ¥ë“¤
        pyhub.llm ask "What is Python?" --no-stream --cost
        pyhub.llm ask "Explain AI" --output response.txt --json
    """

    if query is None:
        if sys.stdin.isatty():
            # stdinì´ í„°ë¯¸ë„ì¸ ê²½ìš° (íŒŒì´í”„ë¼ì¸ì´ ì•„ë‹Œ ê²½ìš°) - help ì¶œë ¥
            console.print(ctx.get_help())
            raise typer.Exit()
        else:
            # stdinì—ì„œ ì…ë ¥ì„ ë°›ëŠ” ê²½ìš°
            console.print("[red]ì˜¤ë¥˜: ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
            console.print('[dim]ì‚¬ìš©ë²•: pyhub.llm ask "ì§ˆë¬¸"[/dim]')
            console.print('[dim]ë˜ëŠ”: echo "ì»¨í…ìŠ¤íŠ¸" | pyhub.llm ask "ì§ˆë¬¸"[/dim]')
            raise typer.Exit(1)

    # Use stdin as context if available and no context argument was provided
    if context is None and not sys.stdin.isatty():
        context = sys.stdin.read().strip()

    # Handle file options
    if file:
        file_contexts = []
        for file_path in file:
            if not file_path.exists():
                console.print(f"[red]ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}[/red]")
                raise typer.Exit(1)
            try:
                with file_path.open("r", encoding="utf-8") as f:
                    content = f.read()
                    file_contexts.append(f"# {file_path.name}\n\n{content}")
            except Exception as e:
                console.print(f"[red]ì˜¤ë¥˜: íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}[/red]")
                raise typer.Exit(1)

        # Combine file contents with existing context
        file_context = "\n\n---\n\n".join(file_contexts)
        if context:
            context = f"{context}\n\n---\n\n{file_context}"
        else:
            context = file_context

    # Handle system prompt options
    if system_prompt_path:
        try:
            with open(system_prompt_path, "r") as f:
                system_prompt = f.read().strip()
        except IOError:
            pass

    if context:
        system_prompt = ((system_prompt or "") + "\n\n" + f"<context>{context}</context>").strip()

    # if system_prompt:
    #     console.print(f"# System prompt\n\n{system_prompt}\n\n----\n\n", style="blue")

    if is_verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.WARNING  # Only show warnings and errors when not verbose
    # init(debug=is_verbose, log_level=log_level)

    # í…œí”Œë¦¿ ë¡œë“œ
    if template_name:
        try:
            import toml

            # from pyhub.config import Config
            # toml_path = Config.get_default_toml_path()
            toml_path = Path.home() / ".pyhub.toml"  # Temporary fix
            if toml_path.exists():
                with toml_path.open("r", encoding="utf-8") as f:
                    config = toml.load(f)
                    templates = config.get("prompt_templates", {}).get(template_name, {})
                    if templates:
                        system_prompt = templates.get("system", system_prompt)
                        if "user" in templates and "{query}" in templates["user"]:
                            query = templates["user"].format(query=query)
                    else:
                        console.print(f"[yellow]ê²½ê³ : í…œí”Œë¦¿ '{template_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
        except Exception as e:
            console.print(f"[yellow]í…œí”Œë¦¿ ë¡œë“œ ì˜¤ë¥˜: {e}[/yellow]")

    # JSON Schema ë¡œë“œ
    choices = None
    if schema_path and schema_path.exists():
        try:
            import json

            with schema_path.open("r", encoding="utf-8") as f:
                choices = json.load(f)
        except Exception as e:
            console.print(f"[red]JSON Schema íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}[/red]")
            raise typer.Exit(1)

    # MCP ì„¤ì • ë¡œë“œ ìš°ì„ ìˆœìœ„:
    # 1. ê¸°ë³¸ ì„¤ì • íŒŒì¼ (~/.pyhub-mcptools/mcp.toml)
    # 2. í™˜ê²½ë³€ìˆ˜ (PYHUB_MCP_*)
    # 3. --mcp-config ì§€ì • íŒŒì¼
    # 4. --mcp-inline-config JSON
    # 5. CLI ê°œë³„ ì˜µì…˜ë“¤ (--mcp-stdio, --mcp-sse, --mcp-http)

    mcp_configs = []

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ ì„¤ì • ë¹„í™œì„±í™” í™•ì¸
    from pyhub.llm.mcp import MCPConfigLoader

    env_disable_default = MCPConfigLoader.is_default_config_disabled()

    # 1. ê¸°ë³¸ ì„¤ì • íŒŒì¼ ë¡œë“œ
    if not no_default_mcp and not env_disable_default:
        try:
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • íŒŒì¼ ê²½ë¡œ í™•ì¸
            env_config_path = MCPConfigLoader.get_environment_config_path()
            if env_config_path:
                default_config = MCPConfigLoader.load_from_file(env_config_path)
                if is_verbose:
                    console.print(f"[dim]í™˜ê²½ë³€ìˆ˜ MCP ì„¤ì • íŒŒì¼ ë¡œë“œ: {env_config_path}[/dim]")
            else:
                default_config = MCPConfigLoader.load_from_default_file()
                if is_verbose and default_config:
                    console.print(f"[dim]ê¸°ë³¸ MCP ì„¤ì • ë¡œë“œ: {len(default_config)} ì„œë²„[/dim]")

            if default_config:
                mcp_configs.append(default_config)
        except Exception as e:
            if is_verbose:
                console.print(f"[yellow]ê¸°ë³¸ MCP ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}[/yellow]")

    # 2. í™˜ê²½ë³€ìˆ˜ ì„¤ì • ë¡œë“œ
    try:
        env_config = MCPConfigLoader.load_from_environment()
        if env_config:
            mcp_configs.append(env_config)
            if is_verbose:
                console.print(f"[dim]í™˜ê²½ë³€ìˆ˜ MCP ì„¤ì • ë¡œë“œ: {len(env_config)} ì„œë²„[/dim]")
    except Exception as e:
        if is_verbose:
            console.print(f"[yellow]í™˜ê²½ë³€ìˆ˜ MCP ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}[/yellow]")

    # 3. ì§€ì •ëœ ì„¤ì • íŒŒì¼
    if mcp_config:
        try:
            file_config = MCPConfigLoader.load_from_file(mcp_config)
            mcp_configs.append(file_config)
            if is_verbose:
                console.print(f"[dim]ì„¤ì • íŒŒì¼ ë¡œë“œ: {mcp_config}[/dim]")
        except Exception as e:
            console.print(f"[red]MCP ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}[/red]")
            raise typer.Exit(1)

    # 4. ì¸ë¼ì¸ JSON ì„¤ì •
    if mcp_inline_config:
        try:
            inline_config = MCPConfigLoader.load_from_json(mcp_inline_config)
            mcp_configs.append(inline_config)
        except Exception as e:
            console.print(f"[red]MCP ì¸ë¼ì¸ ì„¤ì • íŒŒì‹± ì‹¤íŒ¨: {e}[/red]")
            raise typer.Exit(1)

    # 5. CLI ê°œë³„ ì˜µì…˜ë“¤
    if mcp_stdio or mcp_sse or mcp_http:
        try:
            cli_config = MCPConfigLoader.load_from_cli_args(mcp_stdio=mcp_stdio, mcp_sse=mcp_sse, mcp_http=mcp_http)
            if cli_config:
                mcp_configs.append(cli_config)
        except Exception as e:
            console.print(f"[red]MCP CLI ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}[/red]")
            raise typer.Exit(1)

    # ì„¤ì • ë³‘í•© (ë‚˜ì¤‘ ì„¤ì •ì´ ìš°ì„ )
    merged_mcp_config = {}
    if mcp_configs:
        try:
            merged_mcp_config = MCPConfigLoader.merge_configs(*mcp_configs)
        except Exception as e:
            console.print(f"[red]MCP ì„¤ì • ë³‘í•© ì‹¤íŒ¨: {e}[/red]")
            raise typer.Exit(1)

    if is_verbose:
        table = Table()
        table.add_column("ì„¤ì •", style="cyan")
        table.add_column("ê°’", style="green")
        table.add_row("model", model)
        table.add_row("context", context)
        table.add_row("system prompt", system_prompt)
        table.add_row("user prompt", query)
        table.add_row("temperature", str(temperature))
        table.add_row("max_tokens", str(max_tokens))
        table.add_row("ë©€í‹° í„´ ì—¬ë¶€", "O" if is_multi else "X")
        table.add_row("JSON ì¶œë ¥", "O" if output_json else "X")
        if schema_path:
            table.add_row("JSON Schema", str(schema_path))
        console.print(table)

    # Create cache if requested
    cache = None
    if enable_cache:
        from pyhub.llm.cache import MemoryCache

        cache = MemoryCache()

    llm = LLM.create(
        model=model,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens,
        cache=cache,
    )
    if is_verbose:
        console.print(f"Using llm {llm.model}")

    # MCP ë„êµ¬ ë¡œë“œ
    all_tools = []
    if merged_mcp_config:
        if is_verbose:
            console.print(f"[yellow]Loading tools from {len(merged_mcp_config)} MCP servers...[/yellow]")

        try:

            async def load_mcp_tools():
                from pyhub.llm.mcp.multi_client import MultiServerMCPClient

                client = MultiServerMCPClient(merged_mcp_config)
                async with client:
                    return await client.get_all_tools()

            mcp_tools = asyncio.run(load_mcp_tools())
            console.print(f"[green]âœ… Loaded {len(mcp_tools)} tools from MCP servers[/green]")

            if is_verbose:
                for tool in mcp_tools:
                    server = tool.get("_mcp_server", "unknown")
                    console.print(f"[dim]  - {tool['name']} (from {server})[/dim]")

            # MCP ë„êµ¬ë¥¼ Function Calling í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if mcp_tools:
                from pyhub.llm.tools import convert_mcp_tools_for_function_calling

                fc_tools = convert_mcp_tools_for_function_calling(mcp_tools, merged_mcp_config)
                all_tools.extend(fc_tools)

        except Exception as e:
            console.print(f"[red]MCP ë„êµ¬ ë¡œë“œ ì‹¤íŒ¨: {e}[/red]")
            if is_verbose:
                import traceback

                console.print(f"[red]{traceback.format_exc()}[/red]")
            raise typer.Exit(1)

    # ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë¡œê·¸
    if is_verbose and all_tools:
        console.print(f"[dim]ì´ {len(all_tools)} ê°œ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥[/dim]")

    import time

    start_time = time.time()

    if not is_multi:
        response_text = ""
        usage = None

        if is_verbose:
            console.print(f"[dim]ë””ë²„ê·¸: ëª¨ë¸={model.value}, show_cost={show_cost}, show_stats={show_stats}[/dim]")

        if output_json or choices:
            # êµ¬ì¡°í™”ëœ ì‘ë‹µ
            if all_tools:
                response = llm.ask(query, choices=choices, tools=all_tools)
            else:
                response = llm.ask(query, choices=choices)
            usage = response.usage if hasattr(response, "usage") else None
            if output_json:
                import json
                from dataclasses import asdict, is_dataclass

                # dataclassë¥¼ dictë¡œ ë³€í™˜
                if is_dataclass(response):
                    response_dict = asdict(response)
                    # Usageì˜ total propertyë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€
                    if "usage" in response_dict and response_dict["usage"]:
                        response_dict["usage"]["total"] = response.usage.total
                elif hasattr(response, "model_dump"):
                    response_dict = response.model_dump()
                else:
                    response_dict = response

                # ë¹„ìš© ì •ë³´ ì¶”ê°€ (ìš”ì²­ëœ ê²½ìš°)
                if show_cost and usage:
                    try:
                        from pyhub.llm.utils.pricing import calculate_cost

                        cost = calculate_cost(model.value, usage.input, usage.output)
                        response_dict["cost"] = {
                            "input_cost": cost["input_cost"],
                            "output_cost": cost["output_cost"],
                            "total_cost": cost["total_cost"],
                            "total_cost_krw": cost["total_cost"] * 1300,
                        }
                    except Exception as e:
                        if is_verbose:
                            console.print(f"[yellow]ë¹„ìš© ê³„ì‚° ì‹¤íŒ¨: {e}[/yellow]")

                response_text = json.dumps(response_dict, ensure_ascii=False, indent=2)
                console.print(response_text)
            else:
                response_text = str(response)
                console.print(response_text)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
            if no_stream:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                if is_verbose:
                    console.print("[dim]ë””ë²„ê·¸: ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš©[/dim]")
                if all_tools:
                    response = llm.ask(query, stream=False, tools=all_tools)
                else:
                    response = llm.ask(query, stream=False)
                response_text = response.text
                usage = response.usage
                console.print(response_text)
            else:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ê¸°ë³¸)
                if is_verbose:
                    console.print("[dim]ë””ë²„ê·¸: ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...[/dim]")
                if all_tools:
                    chunks = llm.ask(query, stream=True, tools=all_tools)
                else:
                    chunks = llm.ask(query, stream=True)
                for chunk in chunks:
                    if chunk.text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶œë ¥
                        console.print(chunk.text, end="")
                        response_text += chunk.text
                    if hasattr(chunk, "usage") and chunk.usage:
                        if is_verbose:
                            console.print(
                                f"\n[dim]ë””ë²„ê·¸: Usage ë°œê²¬ - input: {chunk.usage.input}, output: {chunk.usage.output}[/dim]"
                            )
                        usage = chunk.usage
                console.print()

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        elapsed_time = time.time() - start_time

        # íŒŒì¼ë¡œ ì €ì¥
        if output_path:
            try:
                with output_path.open("w", encoding="utf-8") as f:
                    f.write(response_text)
                console.print(f"[green]ì‘ë‹µì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
            except Exception as e:
                console.print(f"[red]íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}[/red]")

        # í†µê³„ í‘œì‹œ
        if show_stats and usage:
            stats_table = Table(title="í†µê³„")
            stats_table.add_column("í•­ëª©", style="cyan")
            stats_table.add_column("ê°’", style="green")
            stats_table.add_row("ì…ë ¥ í† í°", str(usage.input))
            stats_table.add_row("ì¶œë ¥ í† í°", str(usage.output))
            stats_table.add_row("ì´ í† í°", str(usage.total))
            stats_table.add_row("ì‘ë‹µ ì‹œê°„", f"{elapsed_time:.2f}ì´ˆ")
            console.print(stats_table)

        # ë¹„ìš© í‘œì‹œ
        if show_cost:
            if is_verbose:
                console.print(f"[dim]ë””ë²„ê·¸: show_cost={show_cost}, usage={usage}[/dim]")
            if usage:
                try:
                    from pyhub.llm.utils.pricing import calculate_cost

                    cost = calculate_cost(model, usage.input, usage.output)
                    cost_table = Table(title="ì˜ˆìƒ ë¹„ìš©")
                    cost_table.add_column("í•­ëª©", style="cyan")
                    cost_table.add_column("ê°’", style="green")
                    cost_table.add_row("ì…ë ¥ ë¹„ìš©", f"$ {cost['input_cost']:.6f}")
                    cost_table.add_row("ì¶œë ¥ ë¹„ìš©", f"$ {cost['output_cost']:.6f}")
                    cost_table.add_row("ì´ ë¹„ìš©", f"$ {cost['total_cost']:.6f}")
                    cost_table.add_row("ì›í™” í™˜ì‚°", f"â‚© {cost['total_cost'] * 1300:.6f}")
                    console.print(cost_table)
                except Exception as e:
                    console.print(f"[red]ë¹„ìš© ê³„ì‚° ì˜¤ë¥˜: {e}[/red]")
                    if is_verbose:
                        import traceback

                        console.print(f"[yellow]{traceback.format_exc()}[/yellow]")
            else:
                # ëª¨ë¸ë³„ ìƒì„¸í•œ ì•ˆë‚´ ë©”ì‹œì§€
                if "upstage" in model.value.lower() or "solar" in model.value.lower():
                    console.print(
                        "[yellow]ê²½ê³ : Upstage ëª¨ë¸ì€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.[/yellow]"
                    )
                    console.print(
                        "[dim]ğŸ’¡ íŒ: ë¹„ìš© ì •ë³´ë¥¼ ë³´ë ¤ë©´ --no-stream ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”.[/dim]"
                    )
                else:
                    console.print(
                        "[yellow]ê²½ê³ : í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìºì‹œëœ ì‘ë‹µì´ê±°ë‚˜ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©ëŸ‰ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)[/yellow]"
                    )

    else:
        console.print("Human:", query)

        while query:
            console.print("AI:", end=" ")
            if no_stream:
                if all_tools:
                    response = llm.ask(query, stream=False, tools=all_tools)
                else:
                    response = llm.ask(query, stream=False)
                console.print(response.text)
            else:
                if all_tools:
                    chunks = llm.ask(query, stream=True, tools=all_tools)
                else:
                    chunks = llm.ask(query, stream=True)
                for chunk in chunks:
                    console.print(chunk.text, end="")
                console.print()

            query = Prompt.ask("Human")

Metadata-Version: 2.4
Name: safeagent
Version: 0.1.0
Summary: A minimal and flexible framework for orchestrating LLM workflows.
Author: Viktor Veselov
License: MIT License
        
        Copyright (c) 2025 Viktor Veselov
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: jinja2
Requires-Dist: py-rbac
Requires-Dist: faiss-cpu
Requires-Dist: neo4j
Requires-Dist: redis
Requires-Dist: numpy
Requires-Dist: cryptography
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Dynamic: license-file

<div>
<br>
</div>

[![Version](https://img.shields.io/badge/version-0.1.0-blue)](https://pypi.org/project/minillm/)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](docs/html/index.html)

**MiniLLM** is a lightweight framework for building long-running language model workflows with durable memory and governance logging. Gemini provides the default model for generation and embeddings.

# MiniLLM

**MiniLLM** is a lightweight framework for building long‑running language model workflows with durable memory and governance logging. Gemini is the default provider for both generation and embeddings.

`minillm` is a minimal, flexible framework for orchestrating LLM workflows. It provides pluggable components for retrieval, memory, and orchestration with built in governance logging. The default configuration now targets Google's **Gemini** models for both generation and embeddings.


## Get started

Install MiniLLM:

```bash
pip install -e .[test]
```

Create a simple question-answering agent:
=======

Create a simple question-answering agent:
=======
Create a simple question‑answering agent:


```python
from minillm.pipeline import main as run_pipeline

run_pipeline()
```

Comprehensive documentation, including a Quickstart guide and API reference, is available in the [docs folder](docs/html/index.html).

## Core benefits
=======

For more information, see the [Quickstart](docs/html/quickstart.html). To learn how to build more advanced workflows or customize components, browse the [full documentation](docs/html/index.html).

## Core benefits

- Durable execution with a minimal orchestrator
- Governance logging built into every component
- Comprehensive memory with summary support
- Retrieval via FAISS or Neo4j graphs using Gemini embeddings

## MiniLLM ecosystem

MiniLLM integrates with other tools like Neo4j, Redis, and Jinja2 templates. Gemini models are configured by default through the `GEMINI_API_KEY` environment variable, but you can swap in another provider by adjusting your `Config`.

## Additional resources

- [Quickstart](docs/html/quickstart.html)
- [API Reference](docs/html/reference.html)
=======
See the [Quickstart](docs/html/quickstart.html) for details, or browse the [full HTML docs](docs/html/index.html).

## Core benefits

- **Durable execution** with a minimal orchestrator
- **Governance logging** built into every component
- **Comprehensive memory** using Redis or in‑memory storage
- **Plug‑and‑play retrieval** via FAISS or Neo4j graphs

## Ecosystem

MiniLLM integrates easily with other tools such as Neo4j, Redis, and Jinja2 templates. Gemini models provide the default LLM and embedding services, but you can swap in other providers through configuration.


## Optional Docker usage

A `Dockerfile` is included for containerized execution:

- Durable execution with a minimal orchestrator
- Governance logging built into every component
- Comprehensive memory with summary support
- Retrieval via FAISS or Neo4j graphs using Gemini embeddings

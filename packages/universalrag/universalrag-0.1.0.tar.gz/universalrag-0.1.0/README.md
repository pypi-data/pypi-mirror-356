# ğŸ“¦ UniversalRAG

**UniversalRAG** is a plug-and-play, multimodal Retrieval-Augmented Generation (RAG) framework that lets you query data from ** PDFs, Videos, Images, Audio, Documents**, and more using powerful LLMs like **OpenAI**, **Groq**, **HuggingFace**.




## Why UniversalRAG?

UniversalRAG is designed to simplify the entire Retrieval-Augmented Generation (RAG) pipeline for developers.

- âŒ No need to write separate code for different document types
- âŒ No manual chunking, vectorizing, or retrieval code
- âŒ No need to understand the low-level details of embeddings or vector DBs

Just pass in:
- a PDF, YouTube video, audio file, or website URL
- ask your question
- get an accurate answer backed by retrieval

UniversalRAG saves developers hours of boilerplate work and enables them to focus on building real-world GenAI apps faster.

Itâ€™s your one-stop solution for multimodal RAG.

---

## ğŸš€ Features

- âœ… Supports 6+ input formats: PDF, DOCX, Image, Audio, Video
- âœ… Embedding-based retrieval system
- âœ… Built-in support for 3 types of models:
  - `groq` (LLaMA 3.1 8B Instant)
  - `huggingface` (HuggingFaceH4/zephyr-7b-beta)
  - `openai` (GPT-3.5 Turbo)
- âœ… Clean interface: Just import and ask!
- âœ… Easily extendable
- âœ… Supports `.env`-based API key management

---

## ğŸ“¦ Installation

```bash
pip install universalrag


ğŸ” Setup API Keys

Create a .env file in your project root and add the keys as per the model(s) youâ€™re using:
OPENAI_API_KEY=your_openai_api_key
GROQ_API_KEY=your_groq_api_key
HUGGINGFACEHUB_API_TOKEN=your_huggingface_token


â¸»

ğŸ§ª Example Usage

âš ï¸NOTE:Load the any of the pdf,url,video,audio,doc,image at the place where you are running this input file
âš ï¸NOTE:copy the path of the specific (pdf,url,video,audio,doc,image) as well in the {input_path}
âš ï¸NOTE:The User gets to Choose which model he/she wants to communicate with.
âš ï¸NOTE:{groq,openai,huggingface,}-inputs for the variable {model_name}

#CODE:
ğŸ—‚ï¸ Input file (choose one)
from universalrag.pipeline import RAGPipeline
input_path = r"anything.pdf"         # PDF                   
# input_path = "lecture.mp4"         # ğŸ¥ Video
# input_path = "https://example.com" # ğŸŒ URL
# input_path = "meeting.wav"         # ğŸ§ Audio
# input_path = "notes.docx"          # ğŸ“ƒ Word Doc
# input_path = "image.jpg"           # ğŸ–¼ï¸ Image
# ğŸ¤– Initialize pipeline with desired model
rag = RAGPipeline(input_path, model_name="groq")  
# â“ Ask a question
question = "Summarize the content."
answer = rag.ask(question)
print("\nğŸ¤– Answer:")
print(answer)


âš ï¸ Notes
	â€¢	You must install model dependencies (e.g., openai, groq, transformers, langchain) as per your use case.
	â€¢	You need API keys for Groq, OpenAI, or Hugging Face models.
	â€¢	Some formats (e.g. audio/video) require ffmpeg to be installed.



ğŸ“ƒ License

MIT License Â© 2025 Vigyat Singh

â¸»

â¤ï¸ Contribute

Feel free to open issues, suggest improvements, or create pull requests!

â¸»

ğŸŒ Contact

For queries or collaborations, reach out to:
	â€¢	GitHub: @vigyat13
	â€¢	Email: vigyatsingh@2004.com
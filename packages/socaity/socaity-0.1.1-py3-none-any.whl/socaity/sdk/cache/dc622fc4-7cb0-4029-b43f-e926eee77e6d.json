{"id": "dc622fc4-7cb0-4029-b43f-e926eee77e6d", "display_name": "openai/gpt-4o", "description": null, "short_desc": null, "endpoints": [{"id": "predict_predictions_post", "display_name": "predict_predictions_post", "description": "Run a single prediction on the model", "short_desc": "Predict", "path": "/predictions", "parameters": [{"name": "prefer", "type": "string", "required": false, "default": null, "location": "header", "param_schema": {"type": "string", "title": "Prefer"}, "description": null}, {"name": "top_p", "type": "number", "required": false, "default": 1, "location": "body", "param_schema": {"type": "number", "title": "Top P", "default": 1, "maximum": 1, "minimum": 0, "x-order": 6, "description": "Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)"}, "description": "Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)"}, {"name": "prompt", "type": "string", "required": false, "default": null, "location": "body", "param_schema": {"type": "string", "title": "Prompt", "x-order": 0, "description": "The prompt to send to the model. Do not use if using messages."}, "description": "The prompt to send to the model. Do not use if using messages."}, {"name": "messages", "type": ["array", "object"], "required": false, "default": [], "location": "body", "param_schema": {"type": "array", "items": {"type": "object"}, "title": "Messages", "default": [], "x-order": 2, "description": "A JSON string representing a list of messages. For example: [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]. If provided, prompt and system_prompt are ignored."}, "description": "A JSON string representing a list of messages. For example: [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]. If provided, prompt and system_prompt are ignored."}, {"name": "image_input", "type": ["array", "file", "string"], "required": false, "default": [], "location": "body", "param_schema": {"type": "array", "items": {"type": "string", "format": "uri"}, "title": "Image Input", "default": [], "x-order": 3, "description": "List of images to send to the model"}, "description": "List of images to send to the model"}, {"name": "temperature", "type": "number", "required": false, "default": 1, "location": "body", "param_schema": {"type": "number", "title": "Temperature", "default": 1, "maximum": 2, "minimum": 0, "x-order": 4, "description": "Sampling temperature between 0 and 2"}, "description": "Sampling temperature between 0 and 2"}, {"name": "system_prompt", "type": "string", "required": false, "default": null, "location": "body", "param_schema": {"type": "string", "title": "System Prompt", "x-order": 1, "description": "System prompt to set the assistant's behavior"}, "description": "System prompt to set the assistant's behavior"}, {"name": "presence_penalty", "type": "number", "required": false, "default": 0, "location": "body", "param_schema": {"type": "number", "title": "Presence Penalty", "default": 0, "maximum": 2, "minimum": -2, "x-order": 8, "description": "Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."}, "description": "Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."}, {"name": "frequency_penalty", "type": "number", "required": false, "default": 0, "location": "body", "param_schema": {"type": "number", "title": "Frequency Penalty", "default": 0, "maximum": 2, "minimum": -2, "x-order": 7, "description": "Frequency penalty parameter - positive values penalize the repetition of tokens."}, "description": "Frequency penalty parameter - positive values penalize the repetition of tokens."}, {"name": "max_completion_tokens", "type": "integer", "required": false, "default": 4096, "location": "body", "param_schema": {"type": "integer", "title": "Max Completion Tokens", "default": 4096, "x-order": 5, "description": "Maximum number of completion tokens to generate"}, "description": "Maximum number of completion tokens to generate"}], "responses": {"200": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/PredictionResponse"}}}, "description": "Successful Response"}, "422": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/HTTPValidationError"}}}, "description": "Validation Error"}}, "timeout_s": null}], "specification": "socaity", "used_models": [], "category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "family_id": "1d0fe916-5283-485d-a31f-167379bcf7c3", "service_address": {"url": "https://api.socaity.ai/v1/openai/gpt-4o"}, "created_at": "2025-06-18T13:00:01.974811+00:00", "version": "ee9bd0ed2f06f4950132caa1e46b64d209ba6e06"}
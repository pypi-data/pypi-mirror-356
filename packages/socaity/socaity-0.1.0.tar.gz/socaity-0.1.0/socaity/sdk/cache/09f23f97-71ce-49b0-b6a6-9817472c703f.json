{"id": "09f23f97-71ce-49b0-b6a6-9817472c703f", "display_name": "openai/gpt-4o-mini", "description": null, "short_desc": null, "endpoints": [{"id": "predict_predictions_post", "display_name": "predict_predictions_post", "description": "Run a single prediction on the model", "short_desc": "Predict", "path": "/predictions", "parameters": [{"name": "prefer", "type": "string", "required": false, "default": null, "location": "header", "param_schema": {"type": "string", "title": "Prefer"}, "description": null}, {"name": "top_p", "type": "number", "required": false, "default": 1, "location": "body", "param_schema": {"type": "number", "title": "Top P", "default": 1, "maximum": 1, "minimum": 0, "x-order": 5, "description": "Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)"}, "description": "Nucleus sampling parameter - the model considers the results of the tokens with top_p probability mass. (0.1 means only the tokens comprising the top 10% probability mass are considered.)"}, {"name": "prompt", "type": "string", "required": false, "default": null, "location": "body", "param_schema": {"type": "string", "title": "Prompt", "x-order": 0, "description": "The prompt to send to the model. Do not use if using messages."}, "description": "The prompt to send to the model. Do not use if using messages."}, {"name": "image_input", "type": ["array", "file", "string"], "required": false, "default": [], "location": "body", "param_schema": {"type": "array", "items": {"type": "string", "format": "uri"}, "title": "Image Input", "default": [], "x-order": 2, "description": "List of images to send to the model"}, "description": "List of images to send to the model"}, {"name": "temperature", "type": "number", "required": false, "default": 1, "location": "body", "param_schema": {"type": "number", "title": "Temperature", "default": 1, "maximum": 2, "minimum": 0, "x-order": 3, "description": "Sampling temperature between 0 and 2"}, "description": "Sampling temperature between 0 and 2"}, {"name": "system_prompt", "type": "string", "required": false, "default": null, "location": "body", "param_schema": {"type": "string", "title": "System Prompt", "x-order": 1, "description": "System prompt to set the assistant's behavior"}, "description": "System prompt to set the assistant's behavior"}, {"name": "presence_penalty", "type": "number", "required": false, "default": 0, "location": "body", "param_schema": {"type": "number", "title": "Presence Penalty", "default": 0, "maximum": 2, "minimum": -2, "x-order": 7, "description": "Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."}, "description": "Presence penalty parameter - positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."}, {"name": "frequency_penalty", "type": "number", "required": false, "default": 0, "location": "body", "param_schema": {"type": "number", "title": "Frequency Penalty", "default": 0, "maximum": 2, "minimum": -2, "x-order": 6, "description": "Frequency penalty parameter - positive values penalize the repetition of tokens."}, "description": "Frequency penalty parameter - positive values penalize the repetition of tokens."}, {"name": "max_completion_tokens", "type": "integer", "required": false, "default": 4096, "location": "body", "param_schema": {"type": "integer", "title": "Max Completion Tokens", "default": 4096, "x-order": 4, "description": "Maximum number of completion tokens to generate"}, "description": "Maximum number of completion tokens to generate"}], "responses": {"200": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/PredictionResponse"}}}, "description": "Successful Response"}, "422": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/HTTPValidationError"}}}, "description": "Validation Error"}}, "timeout_s": null}], "specification": "socaity", "used_models": [], "category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "family_id": "806e8292-3edc-464b-b67e-8d0f5323840a", "service_address": {"url": "http://localhost:8001/v1/openai/gpt-4o-mini"}, "created_at": "2025-06-17T13:41:26.617827+00:00", "version": "99d0094c6f788d3f2eb17301e4a1de06c719f552"}
{"id": "18956f9d-4f1c-42e3-9456-e44253ae19e2", "display_name": "meta/meta-llama-3-70b-instruct", "description": null, "short_desc": null, "endpoints": [{"id": "ready_ready_get", "display_name": "ready_ready_get", "description": null, "short_desc": "Ready", "path": "/ready", "parameters": [], "responses": {"200": {"content": {"application/json": {"schema": {"title": "Response Ready Ready Get"}}}, "description": "Successful Response"}}, "timeout_s": null}, {"id": "predict_predictions_post", "display_name": "predict_predictions_post", "description": "Run a single prediction on the model", "short_desc": "Predict", "path": "/predictions", "parameters": [{"name": "prefer", "type": "string", "required": false, "default": null, "location": "header", "param_schema": {"type": "string", "title": "Prefer"}, "description": null}, {"name": "top_k", "type": "integer", "required": false, "default": 50, "location": "body", "param_schema": {"type": "integer", "title": "Top K", "default": 50, "x-order": 5, "description": "The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering)."}, "description": "The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering)."}, {"name": "top_p", "type": "number", "required": false, "default": 0.9, "location": "body", "param_schema": {"type": "number", "title": "Top P", "default": 0.9, "x-order": 4, "description": "A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)."}, "description": "A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)."}, {"name": "prompt", "type": "string", "required": false, "default": "", "location": "body", "param_schema": {"type": "string", "title": "Prompt", "default": "", "x-order": 0, "description": "Prompt"}, "description": "Prompt"}, {"name": "max_tokens", "type": "integer", "required": false, "default": 512, "location": "body", "param_schema": {"type": "integer", "title": "Max Tokens", "default": 512, "x-order": 2, "description": "The maximum number of tokens the model should generate as output."}, "description": "The maximum number of tokens the model should generate as output."}, {"name": "min_tokens", "type": "integer", "required": false, "default": 0, "location": "body", "param_schema": {"type": "integer", "title": "Min Tokens", "default": 0, "x-order": 1, "description": "The minimum number of tokens the model should generate as output."}, "description": "The minimum number of tokens the model should generate as output."}, {"name": "temperature", "type": "number", "required": false, "default": 0.6, "location": "body", "param_schema": {"type": "number", "title": "Temperature", "default": 0.6, "x-order": 3, "description": "The value used to modulate the next token probabilities."}, "description": "The value used to modulate the next token probabilities."}, {"name": "prompt_template", "type": "string", "required": false, "default": "{prompt}", "location": "body", "param_schema": {"type": "string", "title": "Prompt Template", "default": "{prompt}", "x-order": 8, "description": "Prompt template. The string `{prompt}` will be substituted for the input prompt. If you want to generate dialog output, use this template as a starting point and construct the prompt string manually, leaving `prompt_template={prompt}`."}, "description": "Prompt template. The string `{prompt}` will be substituted for the input prompt. If you want to generate dialog output, use this template as a starting point and construct the prompt string manually, leaving `prompt_template={prompt}`."}, {"name": "presence_penalty", "type": "number", "required": false, "default": 1.15, "location": "body", "param_schema": {"type": "number", "title": "Presence Penalty", "default": 1.15, "x-order": 6, "description": "Presence penalty"}, "description": "Presence penalty"}, {"name": "frequency_penalty", "type": "number", "required": false, "default": 0.2, "location": "body", "param_schema": {"type": "number", "title": "Frequency Penalty", "default": 0.2, "x-order": 7, "description": "Frequency penalty"}, "description": "Frequency penalty"}], "responses": {"200": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/PredictionResponse"}}}, "description": "Successful Response"}, "422": {"content": {"application/json": {"schema": {"$ref": "#/components/schemas/HTTPValidationError"}}}, "description": "Validation Error"}}, "timeout_s": null}], "specification": "socaity", "used_models": [], "category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "family_id": "95accbf3-82ea-41a4-a8e3-abb9199e7d7e", "service_address": {"url": "http://localhost:8001/v1/meta/meta-llama-3-70b-instruct"}, "created_at": "2025-06-17T13:41:26.672925+00:00", "version": "1955e21bc8ae507e8169eaca6bf131a33711b3f9"}
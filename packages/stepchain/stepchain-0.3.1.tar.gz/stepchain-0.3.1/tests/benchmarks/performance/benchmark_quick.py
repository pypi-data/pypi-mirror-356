#!/usr/bin/env python3
"""
Quick Performance Comparison: Direct API vs StepChain
Using shorter tasks to avoid timeouts
"""

import time
import statistics
from openai import OpenAI
from stepchain import setup_stepchain, decompose, execute

# Shorter test cases for quick comparison
QUICK_TESTS = [
    {
        "name": "Simple Task",
        "task": "Write a one-line Python function to add two numbers",
        "complexity": "simple"
    },
    {
        "name": "Moderate Task", 
        "task": "Create a Python function that validates email addresses using regex",
        "complexity": "moderate"
    },
    {
        "name": "Multi-Step Task",
        "task": "Write Python code to: 1) Read a list of numbers, 2) Calculate mean and median, 3) Return results as dict",
        "complexity": "multi-step"
    }
]

def measure_direct_api(client: OpenAI, task: str):
    """Measure direct API performance"""
    start = time.time()
    
    response = client.responses.create(
        model="gpt-4o-mini",
        instructions="You are a Python expert. Complete the task concisely.",
        input=task
    )
    
    # Wait for completion
    while response.status in ["queued", "in_progress"]:
        time.sleep(0.5)
        response = client.responses.retrieve(response.id)
    
    elapsed = time.time() - start
    tokens = response.usage.total_tokens if hasattr(response, 'usage') and response.usage else 0
    
    return {
        "time": elapsed,
        "tokens": tokens,
        "success": response.status == "completed"
    }

def measure_stepchain(task: str):
    """Measure StepChain performance"""
    start = time.time()
    
    # Silence logs for cleaner output
    import logging
    logging.getLogger("stepchain").setLevel(logging.WARNING)
    
    plan = decompose(task)
    results = execute(plan)
    
    elapsed = time.time() - start
    steps = len(plan.steps)
    success = all(r.status == "completed" for r in results)
    
    # Calculate tokens (if available)
    tokens = 0
    for result in results:
        if hasattr(result, 'response') and result.response:
            if hasattr(result.response, 'usage') and result.response.usage:
                tokens += result.response.usage.total_tokens
    
    return {
        "time": elapsed,
        "tokens": tokens,
        "steps": steps,
        "success": success
    }

def main():
    """Run quick benchmark"""
    print("‚ö° Quick Performance Comparison: Direct API vs StepChain\n")
    print("=" * 60)
    
    client = OpenAI()
    setup_stepchain()
    
    results = []
    
    for test in QUICK_TESTS:
        print(f"\nüìù {test['name']} ({test['complexity']})")
        print(f"Task: {test['task'][:60]}...")
        print("-" * 60)
        
        # Run each test 3 times for better accuracy
        direct_times = []
        stepchain_times = []
        
        for i in range(3):
            # Direct API
            direct = measure_direct_api(client, test['task'])
            direct_times.append(direct['time'])
            
            # StepChain
            stepchain = measure_stepchain(test['task'])
            stepchain_times.append(stepchain['time'])
            
            if i == 0:  # Print first run details
                print(f"\nDirect API:")
                print(f"  Time: {direct['time']:.2f}s")
                print(f"  Tokens: {direct['tokens']}")
                print(f"  Success: {direct['success']}")
                
                print(f"\nStepChain:")
                print(f"  Time: {stepchain['time']:.2f}s")
                print(f"  Tokens: {stepchain['tokens']}")
                print(f"  Steps: {stepchain['steps']}")
                print(f"  Success: {stepchain['success']}")
        
        # Calculate averages
        avg_direct = statistics.mean(direct_times)
        avg_stepchain = statistics.mean(stepchain_times)
        overhead = ((avg_stepchain / avg_direct) - 1) * 100
        
        print(f"\nAverage over 3 runs:")
        print(f"  Direct API: {avg_direct:.2f}s")
        print(f"  StepChain: {avg_stepchain:.2f}s")
        print(f"  Overhead: +{overhead:.1f}%")
        
        results.append({
            "test": test['name'],
            "direct_avg": avg_direct,
            "stepchain_avg": avg_stepchain,
            "overhead_pct": overhead
        })
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä SUMMARY")
    print("=" * 60)
    
    print("\n| Task Type | Direct API | StepChain | Overhead |")
    print("|-----------|------------|-----------|----------|")
    
    for r in results:
        print(f"| {r['test']:15} | {r['direct_avg']:6.2f}s | {r['stepchain_avg']:7.2f}s | +{r['overhead_pct']:5.1f}% |")
    
    avg_overhead = statistics.mean([r['overhead_pct'] for r in results])
    print(f"\nAverage overhead: +{avg_overhead:.1f}%")
    
    print("\n‚úÖ Key Takeaways:")
    print("‚Ä¢ StepChain adds 20-40% time overhead on average")
    print("‚Ä¢ Overhead is worth it for complex tasks needing reliability")
    print("‚Ä¢ Direct API is best for simple, single-step operations")
    print("‚Ä¢ StepChain excels at multi-step workflows with dependencies")

if __name__ == "__main__":
    main()